{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "neural_machine_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LQNfwsurIY"
      },
      "source": [
        "Make sure you fill your name and NetID below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATdncJequrIZ"
      },
      "source": [
        "NAME = \"Julian Romero\"\n",
        "NET_ID = \"jvr40\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz_4vmP5urIc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guhvNLHurId"
      },
      "source": [
        "---\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "# Assignment 3: RNN, LSTM, Attention, and Transformers\n",
        "\n",
        "In this assignment, you will implement neural machine (NMT) models using:\n",
        "\n",
        "1. RNNs\n",
        "2. LSTMs and LSTMs with attention\n",
        "3. Transformers.\n",
        "\n",
        "As in the previous assignments, you will see code blocks that look like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "pass\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "x = 3.7\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "Also, please remember:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ3uZsbrurIe"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First let's import some libraries that will be useful in this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-fJK1P8urIe"
      },
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4QEa6turIh"
      },
      "source": [
        "Make sure you are using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhIpDjV0urIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2da84f-efd5-4752-8d06-2481de806c04"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Good to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wshuKb3gurIk"
      },
      "source": [
        "For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8aJWCQ8urIk"
      },
      "source": [
        "def download_if_not_exist(file_name):\n",
        "  \n",
        "  if not os.path.exists(file_name):\n",
        "    import urllib.request\n",
        "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
        "\n",
        "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
        "    \n",
        "  return file_name\n",
        "\n",
        "def read_raw(file_name):\n",
        "  file_name = download_if_not_exist(file_name)\n",
        "  \n",
        "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
        "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
        "  return raw_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDpz4_YurIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50fab00-bb12-4590-aa6f-5dce5982716b"
      },
      "source": [
        "raw_text = read_raw('./data.zip')\n",
        "print(raw_text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n",
            "Wait!\tAttends !\n",
            "Wait!\tAttendez !\n",
            "I see.\tJe comprends.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2kuIckQurIq"
      },
      "source": [
        "Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0xsqG3curIq"
      },
      "source": [
        "def preprocess_raw(text):\n",
        "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "  out = ''\n",
        "  for i, char in enumerate(text.lower()):\n",
        "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
        "      out += ' '\n",
        "    out += char\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSa4X-8aurIt"
      },
      "source": [
        "We further split the source-target pairs into a source list and a target list. We use word-level tokenization here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ck7p7ZUurIt"
      },
      "source": [
        "def split_source_target(text, max_len):\n",
        "  source, target = [], []\n",
        "  for i, line in enumerate(text.split('\\n')):\n",
        "    if i > 5000: # we only use 5000 pairs of translation\n",
        "      break\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "      src_tokens = parts[0].split(' ')\n",
        "      tgt_tokens = parts[1].split(' ')\n",
        "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
        "        source.append(src_tokens)\n",
        "        target.append(tgt_tokens)\n",
        "  return source, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtfqQeLzurIw"
      },
      "source": [
        "def prepare_data(raw_text, max_len=10000):\n",
        "  text = preprocess_raw(raw_text)\n",
        "  source, target = split_source_target(text, max_len)\n",
        "  return source, target\n",
        "\n",
        "source, target = prepare_data(raw_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_nN_gljurIz"
      },
      "source": [
        "Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av3uJroSurIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "b3edcc46-837f-48d4-8e21-d306155d0596"
      },
      "source": [
        "def len_dis(text):\n",
        "  lens = [len(line) for line in text]\n",
        "  len_counter = collections.Counter(lens)\n",
        "\n",
        "  lens = np.array(list(len_counter.keys()))\n",
        "  sort_idx = np.argsort(lens)\n",
        "  lens_sort = lens[sort_idx]\n",
        "  len_counts = np.array(list(len_counter.values()))\n",
        "  len_counts_sort = len_counts[sort_idx]\n",
        "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
        "  return p, lens_sort\n",
        "  \n",
        "src_p, src_lens_sort = len_dis(source)\n",
        "tgt_p, tgt_lens_sort = len_dis(target)\n",
        "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
        "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
        "plt.title('Cumulative Distribution of Sentence Length')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhk7abVnurI2"
      },
      "source": [
        "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qAZn2hourI2"
      },
      "source": [
        "# hyper-param\n",
        "MAX_LEN = 8\n",
        "MIN_FREQ = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S1nTswhurI5"
      },
      "source": [
        "### Build Vocabulary\n",
        "\n",
        "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
        "- Dictionary word2index: Convert word string into index: \n",
        "- Dictionary index2word: Convert index into word string\n",
        "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
        "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary \n",
        "    \n",
        "There are 4 special tokens added in the vocabulary:\n",
        "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
        "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
        "- 'eos': end of sentence. This indicates the end of a sentence\n",
        "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6rN22mburI5"
      },
      "source": [
        "class Vocab():\n",
        "  def __init__(self, name, tokens, min_freq):\n",
        "    self.name = name\n",
        "    self.index2word = {\n",
        "      0: 'pad',\n",
        "      1: 'bos',\n",
        "      2: 'eos',\n",
        "      3: 'unk'\n",
        "    }\n",
        "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "    self.num_word = 4\n",
        "    token_freq = collections.Counter(tokens)\n",
        "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
        "    self._build_vocab(tokens)\n",
        "    \n",
        "  def _build_vocab(self, tokens):\n",
        "    for token in tokens:\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.num_word\n",
        "        self.index2word[self.num_word] = token\n",
        "        self.num_word += 1\n",
        "        \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.word2index.get(tokens, self.word2index['unk'])\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIanbwZ-urI7"
      },
      "source": [
        "### Build Dataset\n",
        "\n",
        "The dataset pipeline involves the following steps:\n",
        "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
        "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
        "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
        "- We use the Pytorch *DataLoader* API to build the dataset generator.\n",
        "\n",
        "For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7CfYZ7CurI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133f4b24-1b81-4196-cb59-ed7d3d51d234"
      },
      "source": [
        "def build_vocab(name, tokens, min_freq):\n",
        "  tokens = [token for line in tokens for token in line]\n",
        "  return Vocab(name, tokens, min_freq)\n",
        "\n",
        "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
        "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
        "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
        "  return vocab_src, vocab_tgt\n",
        "\n",
        "def pad(line, padding_token):\n",
        "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
        "\n",
        "def build_tensor(text, lang, is_source):\n",
        "  lines = [lang[line] for line in text]\n",
        "  if not is_source:\n",
        "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
        "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
        "  valid_len = (array != lang['pad']).sum(1)\n",
        "  return array, valid_len\n",
        "\n",
        "def load_data_nmt(batch_size=2):\n",
        "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
        "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
        "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
        "  train_data = torch.utils.data.TensorDataset(\n",
        "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "  print(train_data[0])\n",
        "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "  return lang_eng, lang_fra, train_iter\n",
        "\n",
        "\n",
        "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
        "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
        "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
        "print('Total number of sentence pairs: {}'.format(len(source)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "Vocabulary size of source language: 433\n",
            "Vocabulary size of target language: 420\n",
            "Total number of sentence pairs: 4990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw1Z0Tr4urI-"
      },
      "source": [
        "## Sequence to Sequence with RNN (baseline)\n",
        "\n",
        "In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
        "\n",
        "Implementation Details:\n",
        "\n",
        "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
        "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
        "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
        "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
        "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j-GzgeLurI_"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBoosPY2urI_"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, sources, valid_len):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
        "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
        "    \"\"\"\n",
        "    word_embedded = self.embedding(sources)\n",
        "    N = word_embedded.shape[0]\n",
        "    \n",
        "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
        "    \n",
        "    o, h = self.enc(word_embedded, h)\n",
        "    \n",
        "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, h, target):\n",
        "    word_embedded = self.embedding(target)\n",
        "    N, T = word_embedded.shape[:2]\n",
        "    \n",
        "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
        "    pred = self.output_emb(o)\n",
        "    return pred, h\n",
        "\n",
        "class NMTRNN(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
        "    super(NMTRNN, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
        "    \n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    h = self.enc(src, src_len)\n",
        "    T = tgt.shape[1]\n",
        "    \n",
        "    pred, _ = self.dec(h, tgt)\n",
        "    loss = 0\n",
        "    for t in range(T-1):\n",
        "      # target sequence should shift by one time-step, because we are predicting the next word\n",
        "      # notice the `ignore_index` parameter is set to 0, which is for the `pad` token\n",
        "      loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
        "\n",
        "    return loss, pred.argmax(dim=-1)\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "      \"\"\"\n",
        "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
        "      token from the previous time step.\n",
        "      \"\"\"\n",
        "      h = self.enc(src, src_len)\n",
        "\n",
        "      inputs = tgt[:, :1]\n",
        "      preds = []\n",
        "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
        "        pred, h = self.dec(h, inputs)\n",
        "        preds.append(pred)\n",
        "        inputs = pred.argmax(dim=-1)\n",
        "        \n",
        "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
        "      return pred\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ovDVbfurJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8053a0-8ca0-44bf-aa44-68722b3307ce"
      },
      "source": [
        "def train_rnn(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
        "\n",
        "rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 0 / 7800\tLoss:\t41.533730\n",
            "pred:\t tensor([146,  53, 193,  40,  43, 392, 152, 371, 330, 330])\n",
            "\n",
            "tgt:\t tensor([ 90, 171,  91,   3,  24,   2,   0,   0,   0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t13.510290\n",
            "pred:\t tensor([14, 11,  3, 11, 11,  3,  3, 11,  2, 11])\n",
            "\n",
            "tgt:\t tensor([  3,  74,   3, 107,  14, 171, 191,  11,   2])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t12.383263\n",
            "pred:\t tensor([ 3,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([ 4,  9,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t10.840661\n",
            "pred:\t tensor([ 48, 164, 164,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 48,  48, 164,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t7.826301\n",
            "pred:\t tensor([ 3,  5,  5,  2, 24,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([  3, 373,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t7.730925\n",
            "pred:\t tensor([3, 5, 2, 5, 3, 3, 3, 3, 3, 3])\n",
            "\n",
            "tgt:\t tensor([23, 24,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t7.771402\n",
            "pred:\t tensor([ 92, 146,   5,   2,  24,   3,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([241, 309,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t7.356518\n",
            "pred:\t tensor([14, 28, 11,  2, 11,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t6.948967\n",
            "pred:\t tensor([171, 342,   3,  11,   2,  11,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([171, 342,  86,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t7.191818\n",
            "pred:\t tensor([176,   3,  24,   2,   5,   3,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([176,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t6.558807\n",
            "pred:\t tensor([15,  3, 37, 11, 11,  2, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([ 14, 370,   3, 410,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t4.740061\n",
            "pred:\t tensor([48,  3, 11,  2, 11,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t5.890306\n",
            "pred:\t tensor([14,  3, 11,  2, 11,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t5.560978\n",
            "pred:\t tensor([14, 79, 28, 41, 11, 11,  2, 11,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14, 79, 33, 41, 34, 11,  2,  0,  0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t4.977003\n",
            "pred:\t tensor([  3, 107,   5,   2,  24,   3,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([  3, 309,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t4.400037\n",
            "pred:\t tensor([55,  3,  5,  2, 24,  3,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([53,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t4.860919\n",
            "pred:\t tensor([ 3,  5,  2, 24,  3,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([12,  5,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t4.377604\n",
            "pred:\t tensor([36, 40,  3,  3,  5,  2, 24,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([ 38,  40, 214,   3,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t3.337041\n",
            "pred:\t tensor([ 14, 201,   3,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 201,  72,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t4.136302\n",
            "pred:\t tensor([55,  3,  5,  2, 24,  3,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t3.952135\n",
            "pred:\t tensor([212,   3,   3,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([212, 303,  72,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t2.971749\n",
            "pred:\t tensor([90, 78, 24,  2, 24,  3,  3, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([90,  3, 24,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t3.885748\n",
            "pred:\t tensor([14, 17,  3, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14,  3, 31, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t3.192772\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t3.611953\n",
            "pred:\t tensor([145,   3,  24,  24,   2,  11,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([145,   3, 339,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t3.390175\n",
            "pred:\t tensor([79,  3, 41, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([79,  3, 41, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t2.959199\n",
            "pred:\t tensor([142, 328,   5,   2,  24,   3,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([142, 328,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t3.109093\n",
            "pred:\t tensor([48,  3,  3, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([48, 49,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t3.249314\n",
            "pred:\t tensor([ 3,  3,  3, 11, 11,  2, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([95, 29,  3, 96, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t3.482628\n",
            "pred:\t tensor([251, 108, 253,  11,   2,  11,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([251,  74, 253,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t2.214852\n",
            "pred:\t tensor([267,   3,  24,   2,  24,   3,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.512619\n",
            "pred:\t tensor([72,  3, 40,  3, 11,  2, 11,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 72,   3,  40, 230,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t3.203490\n",
            "pred:\t tensor([14,  3, 11,  2, 11,  3, 11,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t2.537915\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t2.796291\n",
            "pred:\t tensor([ 38, 202, 203,  14,   3,  11,   2,  11,   3,  11])\n",
            "\n",
            "tgt:\t tensor([ 38, 202, 203,  14, 376,  11,   2,   0,   0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t2.345727\n",
            "pred:\t tensor([168, 203,  24,   2,  24,   3,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([168, 174,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t2.427199\n",
            "pred:\t tensor([14, 28, 11,  2, 11,  3, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t2.392826\n",
            "pred:\t tensor([ 55,   3,   5,   2, 171,   3,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t2.322796\n",
            "pred:\t tensor([38, 40,  3, 11,  2, 11,  3, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t2.277689\n",
            "pred:\t tensor([ 15,   3, 214, 104,  11,   2,  11,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 15,   3, 214, 104,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t2.000644\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t2.218846\n",
            "pred:\t tensor([14, 28,  3,  3,  3, 11,  2, 11,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28, 81,  3,  3, 11,  2,  0,  0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t2.358517\n",
            "pred:\t tensor([ 14, 108, 120, 406, 147, 148,  11,   2,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 108, 120, 406, 147, 148,  11,   2,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t2.290568\n",
            "pred:\t tensor([14,  3, 37,  3, 11,  2, 11,  3, 11,  3])\n",
            "\n",
            "tgt:\t tensor([15,  3, 37,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t2.288733\n",
            "pred:\t tensor([171, 277,  73, 148,  11,   2,  11,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 92, 277,  73, 148,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t2.095607\n",
            "pred:\t tensor([90, 91,  3, 24,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([90, 91,  3, 24,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.801339\n",
            "pred:\t tensor([55, 54,  5,  2, 24,  3,  3,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([53, 54,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t2.330136\n",
            "pred:\t tensor([  3,  24,   2, 171,   3,   3,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([23, 24,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t1.717108\n",
            "pred:\t tensor([ 92, 341,  87,  11,   2,  11,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([ 92, 341,  87,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t2.026826\n",
            "pred:\t tensor([198, 108,   3,  11,   2,  11,   3,  11,   3,  11])\n",
            "\n",
            "tgt:\t tensor([198, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRp6gM67urJE"
      },
      "source": [
        "### RNN Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNntlzSurJF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "ba978f30-49a1-4da8-a5d1-70f916f42b3f"
      },
      "source": [
        "# save the loss curve figure in a file for the report\n",
        "plt.plot(np.arange(len(rnn_loss_list)), rnn_loss_list)\n",
        "plt.title('Loss Curve of Baseline')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Baseline')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha6FNGSiurJH"
      },
      "source": [
        "### Prediction Accuracy\n",
        "\n",
        "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 70%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbQF8sSlurJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9d8c22-8183-4d27-c1da-0b5605e18b50"
      },
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_rnn(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  \n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_rnn(rnn_net, train_iter, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['unk', '.', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['reste', 'à', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', '!', '?', '.', '.']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t [\"j'étais\", 'unk', '.', '.', '.', '.']\n",
            "\n",
            "tgt:\t [\"j'étais\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t [\"c'est\", 'unk', '.', '.', '.', '.']\n",
            "\n",
            "tgt:\t [\"c'est\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', \"l'ai\", 'unk', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['je', \"l'ai\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.7241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rF4_DWFurJJ"
      },
      "source": [
        "## Sequence to Sequence with LSTM and Attention\n",
        "\n",
        "Now let's try to improve our model by using an LSTM and the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJg5FRyaurJK"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
        "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
        "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
        "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
        "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
        "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Implement the LSTM class below. In particular,\n",
        "-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n",
        "- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c484410e121ce9afb7b97ab9a8a87a11",
          "grade": false,
          "grade_id": "cell-e43516618029ca06",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kgbGXvUMurJK"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, device):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.device = device\n",
        "    self.params = self.init_params(input_size, hidden_size)\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      device: torch.device()\n",
        "    \"\"\"\n",
        "  \n",
        "  def init_params(self, input_size, hidden_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      \n",
        "    Outputs:\n",
        "      Weights for proposal: W_xc, W_hc, b_c\n",
        "      Weights for input gate: W_xi, W_hi, b_i\n",
        "      Weights for forget gate: W_xf, W_hf, b_f\n",
        "      Weights for output gate: W_xo, W_ho, b_o\n",
        "    \"\"\"\n",
        "    W_xc, W_hc, b_c = None, None, None\n",
        "    W_xi, W_hi, b_i = None, None, None\n",
        "    W_xf, W_hf, b_f = None, None, None\n",
        "    W_xo, W_ho, b_o = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the weights and biases. The result will be stored in \n",
        "    # `params` below. Weights should be initialized using `torch.randn` multiplied \n",
        "    # with the scale (0.1). Biases should be initialized to 0.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    W_xc = 0.1*torch.randn(input_size, hidden_size).double()\n",
        "    W_hc = 0.1*torch.randn(hidden_size, hidden_size).double()\n",
        "    b_c = 0\n",
        "\n",
        "    W_xi = 0.1*torch.randn(input_size, hidden_size).double()\n",
        "    W_hi = 0.1*torch.randn(hidden_size, hidden_size).double()\n",
        "    b_i = 0\n",
        "    \n",
        "    W_xf = 0.1*torch.randn(input_size, hidden_size).double()\n",
        "    W_hf = 0.1*torch.randn(hidden_size, hidden_size).double()\n",
        "    b_f = 0\n",
        "\n",
        "    W_xo = 0.1*torch.randn(input_size, hidden_size).double()\n",
        "    W_ho = 0.1*torch.randn(hidden_size, hidden_size).double()\n",
        "    b_o = 0\n",
        "    # END OF YOUR CODE\n",
        "    \n",
        "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
        "    return params\n",
        "\n",
        "  \n",
        "  def lstm(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
        "        T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
        "        \n",
        "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n",
        "            (N, hidden_size), is the memory cell of the LSTM.\n",
        "      \n",
        "    Outputs:\n",
        "      o: tensor of size (N, T, hidden_size).\n",
        "      state: the same as input state.\n",
        "    \"\"\"\n",
        "    \n",
        "    src, src_len = X\n",
        "    h, c = state\n",
        "\n",
        "    # make sure always has a T dim\n",
        "    if len(src.shape) == 2:\n",
        "      src = src.unsqueeze(1)\n",
        "\n",
        "    N, T, D_in = src.shape\n",
        "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
        "    \n",
        "    o = []\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of the LSTM.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    W_xi=W_xi.to(\"cuda\")\n",
        "    src=src.to(\"cuda\")\n",
        "    h=h.to(\"cuda\")\n",
        "    W_hf=W_hf.to(\"cuda\")\n",
        "    W_hi=W_hi.to(\"cuda\")\n",
        "    W_xf=W_xf.to(\"cuda\")\n",
        "    W_xo=W_xo.to(\"cuda\")\n",
        "    W_ho=W_ho.to(\"cuda\")\n",
        "    W_xc=W_xc.to(\"cuda\")\n",
        "    W_hc=W_hc.to(\"cuda\")\n",
        "    \n",
        "    for K in range (src.shape[1]):\n",
        "      xt=src[:,K,:]\n",
        "      \n",
        "      I1 = torch.matmul(xt, W_xi).double()\n",
        "      I2 =torch.matmul(h, W_hi).double()\n",
        "      I=torch.sigmoid(I1+I2+b_i).double()\n",
        "      F = torch.sigmoid(torch.matmul(xt, W_xf) + torch.matmul(h, W_hf) + b_f).double()\n",
        "      O = torch.sigmoid(torch.matmul(xt, W_xo)+ torch.matmul(h, W_ho) + b_o).double()\n",
        "      C_tilda = torch.tanh(torch.matmul(xt, W_xc)+ torch.matmul(h, W_hc) + b_c).double()\n",
        "        \n",
        "      c1= (c * F).double()\n",
        "      c2= (I * C_tilda).double()\n",
        "      c=c1+c2\n",
        "      \n",
        "      h = O*(c.tanh())\n",
        "     \n",
        "      o.append(O)\n",
        "    \n",
        "    h[:, src_len[src.shape[1]:]] = 0\n",
        "    c[:, src_len[src.shape[1]:]] = 0\n",
        "    \n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    state = (h, c)\n",
        "    return o, state\n",
        "  \n",
        "  def forward(self, inputs, state):\n",
        "    return self.lstm(inputs, state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGzpDsypurJM"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9296KOEMurJN"
      },
      "source": [
        "Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZo4ZgURurJN"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBHUzHrPurJO"
      },
      "source": [
        "- **Attention**\n",
        "\n",
        "    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
        "    $$\n",
        "    \\begin{align*}\n",
        "    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n",
        "    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n",
        "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
        "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
        "    \\end{align*}\n",
        "    $$\n",
        "    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BvCFaxSurJO"
      },
      "source": [
        "- **Masked Softmax**\n",
        "\n",
        "For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. We provide the implementation of this for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1c6dBcdurJP"
      },
      "source": [
        "def masked_softmax(X, valid_length):\n",
        "  \"\"\"\n",
        "  inputs:\n",
        "    X: 3-D tensor\n",
        "    valid_length: 1-D or 2-D tensor\n",
        "  \"\"\"\n",
        "  mask_value = -1e7 \n",
        "\n",
        "  if len(X.shape) == 2:\n",
        "    X = X.unsqueeze(1)\n",
        "\n",
        "  N, n, m = X.shape\n",
        "\n",
        "  if len(valid_length.shape) == 1:\n",
        "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
        "  else:\n",
        "    valid_length = valid_length.reshape((-1,))\n",
        "\n",
        "  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n",
        "  X.view(-1, m)[mask] = mask_value\n",
        "\n",
        "  Y = torch.softmax(X, dim=-1)\n",
        "\n",
        "  \n",
        "  return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtjiztYurJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb613d1a-5a9a-438c-cabf-3de87b447352"
      },
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4784, 0.5216, 0.0000, 0.0000],\n",
              "         [0.4428, 0.5572, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2810, 0.3079, 0.4111, 0.0000],\n",
              "         [0.3149, 0.3285, 0.3567, 0.0000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exg926gMurJT"
      },
      "source": [
        "- **Scaled Dot Product Attention**\n",
        "    - The scaled dot-product attention uses the score function as: $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n",
        "\n",
        "    <div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
        "\n",
        "Implement the DotProductAttention below. Do not use any loops in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b7a6c1703c6c230a006a6b86326a3b9",
          "grade": false,
          "grade_id": "cell-eac4fccbcd4f068e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "58YjRL-_urJT"
      },
      "source": [
        "class DotProductAttention(nn.Module): \n",
        "  def __init__(self):\n",
        "      super(DotProductAttention, self).__init__()\n",
        "\n",
        "  def forward(self, query, key, value, valid_length=None):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of DotProductAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    dim = query.shape[-1]\n",
        "    score = torch.bmm(query,key.transpose(1,2))/ np.sqrt(dim)\n",
        "    weights = masked_softmax(score, valid_length)\n",
        "\n",
        "    attention = torch.bmm(weights, value)\n",
        "\n",
        "    \n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw8AZVlDurJV"
      },
      "source": [
        "### Correctness Check for DotProductAttention\n",
        "\n",
        "Run the following snippet to check your implementation of DotProductAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvCvoLo4urJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8760a608-8c78-4c93-bab2-19a1d3f8a8b5"
      },
      "source": [
        "att = DotProductAttention()\n",
        "keys = torch.ones((2,10,2),dtype=torch.float)\n",
        "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
        "att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvu_VpDuurJZ"
      },
      "source": [
        "- **MLP Attention**\n",
        "\n",
        "    In MLP attention, we project both query and keys into $R^h$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n",
        "    \n",
        "    $$\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})$$\n",
        "    \n",
        "    where $\\mathbf{v}, \\mathbf{W_q}\\text{, and }\\mathbf{v}$ are learnable parameters.\n",
        "    \n",
        "Implement the MLP attention in matrix form without using any loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9440f3519ad5f8037f192758aecca64a",
          "grade": false,
          "grade_id": "cell-6be727894d4fd817",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "PaliJn1MurJZ"
      },
      "source": [
        "class MLPAttention(nn.Module):  \n",
        "  def __init__(self, d_v, d_k, d_q):\n",
        "    super(MLPAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_k: feature dimension of key\n",
        "      d_v: feature dimension of vector v\n",
        "      d_q: feature dimension of query\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize learnable parameters\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.key = nn.Linear(d_k, d_v, bias=False)\n",
        "    self.value = nn.Linear(d_v,1, bias=False)\n",
        "    self.query = nn.Linear(d_q, d_v,bias=False)\n",
        "    \n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: either (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of MLPAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    query, key= self.query(query),self.key(key)\n",
        "    \n",
        "    feature = torch.tanh(query.unsqueeze(2) + key.unsqueeze(1))\n",
        "    score = self.value(feature).squeeze(-1)\n",
        "    weights = masked_softmax(score,valid_length)\n",
        "    \n",
        "    \n",
        "    Y = torch.bmm(weights, value)\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnI5cSUUurJb"
      },
      "source": [
        "### Correctness Check for MLPAttention\n",
        "\n",
        "Run the following snippet to check your implementation of MLPAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qea1wvhYurJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3f28ce-54a1-4bf2-8dae-18491f96b237"
      },
      "source": [
        "atten = MLPAttention(4, 2, 2)\n",
        "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhSFXV8BurJe"
      },
      "source": [
        "    \n",
        "- **Using Attention in seq2seq Models**\n",
        "\n",
        "    <div>\n",
        "    <img src=\"https://d2l.ai/_images/seq2seq-attention.svg\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: https://d2l.ai/_images/seq2seq-attention.svg\n",
        "\n",
        "    Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n",
        "    \n",
        "    In rough pseudocode, this looks like:\n",
        "    ```\n",
        "    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n",
        "    decoder_input = concatenate([decoder_input, context])\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "491JaqF4urJe"
      },
      "source": [
        "### LSTM Encoder-Decoder\n",
        "\n",
        "\n",
        "Build a seq2seq model with LSTM and attention.\n",
        "\n",
        "- Complete the Encoder forward() function.\n",
        "- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n",
        "- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a88b8130ea1f3b114f520c4ad398172",
          "grade": false,
          "grade_id": "cell-85d8bda82bc92dd8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cFymbtgVurJf"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim, hidden_size, device)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "  \n",
        "  def forward(self, sources, valid_len):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder forward pass\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    word_embedded = self.embedding(sources).double()\n",
        "    N = word_embedded.shape[0]\n",
        "    h = sources.new_zeros(N, self.hidden_size).double()\n",
        "    c = sources.new_zeros(N, self.hidden_size).double()\n",
        "    states, (h, c) = self.enc((word_embedded,valid_len), (h, c))\n",
        "    # END OF YOUR CODE\n",
        "    return states, (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "47443df0b3d409da120d07806259e6f6",
          "grade": false,
          "grade_id": "cell-154ce877082ed913",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Qv37wQrSurJh"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
        "    self.att = DotProductAttention()\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, state, target, valid_len):\n",
        "    loss = 0\n",
        "    preds = []\n",
        "    \n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Decoder forward pass. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code    \n",
        "    outputs,(h,c),src_len = state\n",
        "    \n",
        "    output = torch.stack(outputs)\n",
        "    \n",
        "    output = output.transpose(0,1)\n",
        "    word_embedded = self.embedding(target)\n",
        "\n",
        "    for i in range(0,output.shape[1]-1):\n",
        "      attention=self.att(h.unsqueeze(1),output,output,valid_len)\n",
        "      word_embedded_cat=torch.cat((word_embedded[:,i,:].unsqueeze(1),attention),2)\n",
        "      \n",
        "      o, (h,c) = self.enc((word_embedded_cat,valid_len), (h,c))\n",
        "      o = torch.stack(o,dim=1)\n",
        "      \n",
        "      pred = self.output_emb(o.float()) \n",
        "      logsoft=F.log_softmax(pred[:, 0])\n",
        "      nullloss=F.nll_loss(logsoft, target[:,i+1], ignore_index=0)\n",
        "      loss = loss + nullloss\n",
        "    # END OF YOUR CODE\n",
        "    return loss, pred\n",
        "  \n",
        "  def predict(self, state, target, valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder prediction. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code    \n",
        "    outputs, (h, c), src_len = state\n",
        "    output = torch.stack(outputs,dim=1)\n",
        "    \n",
        "    preds = []\n",
        "    \n",
        "    for t in range(MAX_LEN+1): # plus the 'eos' token\n",
        "      \n",
        "      inputs=target[:,t]\n",
        "      inputs=inputs.unsqueeze(1)\n",
        "      word_embedded = self.embedding(inputs)\n",
        "      \n",
        "      attention=self.att(h.unsqueeze(1),output,output,valid_len)\n",
        "      \n",
        "      word_embedded_cat=torch.cat((word_embedded,attention),dim=2)\n",
        "      \n",
        "      o, (h,c) = self.enc((word_embedded_cat,valid_len), (h,c))\n",
        "      o = torch.stack(o,dim=1)\n",
        "      pred = self.output_emb(o.float()) \n",
        "      preds.append(pred)\n",
        "      \n",
        "        \n",
        "    pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
        "    # END OF YOUR CODE\n",
        "    return pred\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2ktrs8zurJj"
      },
      "source": [
        "class NMTLSTM(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(NMTLSTM, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
        "    \n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return loss, pred\n",
        "  \n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aa0b3f0d28db5e10c6573d2f151cc604",
          "grade": false,
          "grade_id": "cell-bfaaa623c7199b2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3vv1rKNpurJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c14cea-0f39-4284-d837-e16dd6d57a33"
      },
      "source": [
        "def train_lstm(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `lstm_net` variable.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "lr = 0.01\n",
        "# END OF YOUR CODE\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
        "\n",
        "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "iter 0 / 7800\tLoss:\t43.253113\n",
            "pred:\t tensor([[ 1.4058e-01,  4.4424e-03,  4.3847e-01, -1.1602e-01, -1.8764e-01,\n",
            "          2.6865e-01,  4.2830e-01,  5.6273e-04,  1.0354e-01,  1.9857e-01,\n",
            "          8.3292e-03, -4.8909e-01,  5.9808e-01, -1.0118e-01,  3.1807e-01,\n",
            "          3.3070e-01,  9.2634e-02,  1.2230e-01, -2.2646e-01, -1.2463e-01,\n",
            "          6.5368e-01, -1.4253e-03, -1.2820e-01, -6.2969e-01,  2.0439e-01,\n",
            "          1.6816e-01, -1.0133e-01, -3.5931e-01, -3.4865e-02, -4.0409e-02,\n",
            "         -6.8892e-02, -2.9360e-01, -5.1548e-01,  6.5951e-03, -4.6879e-01,\n",
            "          4.3219e-02,  2.4689e-01,  4.4077e-01, -4.4368e-01,  8.8534e-02,\n",
            "         -5.4518e-01, -2.5337e-01,  5.4262e-01, -1.4891e-01, -3.8328e-01,\n",
            "         -5.8254e-02, -7.2107e-01,  1.4928e-01, -3.4785e-02,  1.8246e-01,\n",
            "          7.5358e-01,  2.1795e-01,  4.4682e-01,  2.2339e-01,  4.8403e-01,\n",
            "          7.2138e-01, -3.0837e-01,  2.1715e-02,  3.3424e-01, -4.6040e-02,\n",
            "         -3.6918e-01,  3.4681e-02,  1.8630e-01, -6.7123e-03,  1.5777e-01,\n",
            "          1.0463e-01,  4.1171e-01, -3.4299e-02, -2.9336e-01,  2.0580e-01,\n",
            "         -5.1178e-02,  2.0562e-01, -4.1711e-02,  3.3209e-01,  3.4725e-02,\n",
            "         -5.0686e-01, -2.0098e-01, -5.2687e-01,  5.2298e-01, -5.1301e-01,\n",
            "          1.4046e-01,  3.6308e-01, -3.5726e-01, -5.2068e-01, -4.8960e-01,\n",
            "          3.3903e-01,  3.0053e-01,  6.0363e-01,  3.8897e-01, -4.0661e-01,\n",
            "         -3.7909e-01,  3.8956e-01, -6.5441e-01,  2.6043e-02,  5.9512e-01,\n",
            "          1.5616e-01,  3.5498e-01, -2.8681e-02,  8.2598e-01, -1.4842e-01,\n",
            "         -2.8469e-02, -2.2921e-01, -6.5532e-02, -1.6376e-01, -1.9246e-01,\n",
            "          5.5777e-01, -4.2624e-01,  4.4713e-02,  5.1233e-01, -2.7231e-01,\n",
            "          3.1529e-01,  7.3379e-01,  1.7727e-01, -5.6314e-01,  1.1213e-01,\n",
            "          5.9448e-01,  2.4331e-01, -2.3427e-01, -2.8873e-02,  2.5061e-01,\n",
            "         -5.3523e-01, -2.5181e-01, -3.1249e-01, -4.3298e-01, -1.9582e-03,\n",
            "          2.0100e-01, -1.5367e-01,  4.2328e-01, -5.2254e-02,  2.1061e-03,\n",
            "         -2.3143e-01,  3.9115e-02, -2.0969e-01, -3.7865e-01,  1.8516e-01,\n",
            "         -2.2851e-01, -1.7120e-01,  2.6312e-01,  6.8843e-01, -2.1037e-01,\n",
            "          3.4164e-02,  4.7421e-02, -1.6078e-01,  2.3661e-01,  6.9448e-02,\n",
            "         -5.3460e-01,  1.5480e-01, -3.2286e-01, -2.9575e-01, -3.1302e-01,\n",
            "          3.9977e-01, -3.6678e-01, -3.0967e-01, -3.8245e-01,  6.6576e-02,\n",
            "          2.6389e-01, -8.1849e-01,  5.2293e-02,  2.8581e-01,  4.0434e-01,\n",
            "          7.8370e-01,  1.2283e-01,  1.7683e-01, -2.2294e-01, -2.5254e-01,\n",
            "         -2.5009e-01, -2.0397e-01, -3.0441e-01,  8.5193e-02,  1.4723e-01,\n",
            "         -1.3368e-01,  4.5775e-01,  1.9559e-01, -1.1613e-02, -2.0795e-01,\n",
            "         -1.2631e-01, -2.2735e-01,  1.5898e-01,  3.5337e-01,  1.4721e-01,\n",
            "          1.8135e-01, -8.6331e-02, -6.7106e-02, -4.4598e-01, -6.5218e-01,\n",
            "         -4.2222e-01,  3.7262e-02, -4.1810e-01,  2.6704e-01, -4.3925e-01,\n",
            "         -7.3656e-02, -1.9514e-01, -4.1616e-01,  9.2458e-01, -3.1561e-01,\n",
            "          1.7108e-01, -4.1136e-01,  3.2291e-01, -7.2329e-02, -2.6334e-02,\n",
            "         -1.5533e-01,  2.8926e-01,  1.9789e-01,  2.5759e-01, -4.8276e-01,\n",
            "         -3.9035e-01,  1.3238e-02,  4.1512e-01, -3.9228e-01, -9.5821e-02,\n",
            "          1.4451e-01, -1.1877e-01, -6.3259e-01,  5.0365e-01, -7.8944e-01,\n",
            "         -6.2125e-01, -2.7402e-01, -1.4211e-01,  2.4993e-01, -3.0480e-01,\n",
            "          1.7241e-01,  2.3155e-01, -2.0683e-01,  1.8566e-01, -2.7183e-01,\n",
            "         -3.7465e-01, -3.3973e-01, -5.8200e-01,  1.9411e-01, -1.0155e-01,\n",
            "          6.8519e-01, -3.6072e-01,  4.3922e-01,  5.1125e-01,  2.7965e-02,\n",
            "         -1.2691e-01,  2.7808e-02, -2.6470e-01, -1.1220e-01, -4.3856e-01,\n",
            "          2.0713e-01, -4.2538e-01, -4.1818e-02,  2.9246e-02,  3.6293e-01,\n",
            "         -2.6197e-01, -2.4378e-01, -2.1669e-02,  6.1204e-01,  2.0455e-01,\n",
            "          6.1275e-02, -8.2268e-01, -4.1106e-02,  1.6988e-02, -9.7117e-02,\n",
            "          2.8858e-01, -2.4928e-01,  4.2316e-01,  6.6149e-02, -3.8965e-01,\n",
            "         -3.3041e-02, -2.1425e-01,  3.2774e-01,  1.0008e-01, -1.0359e-01,\n",
            "          7.7110e-02, -4.8267e-01, -6.2583e-01,  1.0821e-01, -1.1980e-02,\n",
            "         -6.4213e-02,  3.4503e-01, -3.5937e-01,  2.8064e-01, -1.3076e-01,\n",
            "         -2.2153e-01, -6.0893e-01,  6.6049e-02,  4.6253e-02,  1.7942e-01,\n",
            "         -2.6975e-01,  3.7804e-01,  3.9216e-01,  2.5730e-01, -6.3560e-01,\n",
            "          5.9354e-02, -3.6780e-01,  5.2223e-01,  2.1914e-01, -5.2701e-02,\n",
            "          4.2469e-01, -3.8749e-01,  7.3971e-01,  1.4443e-01,  1.4928e-01,\n",
            "          3.2912e-01, -5.5778e-01,  1.4447e-01, -6.4220e-02, -7.3284e-02,\n",
            "         -2.7122e-02,  8.1347e-02,  6.7230e-02, -2.2020e-01, -4.1924e-02,\n",
            "          9.2670e-01,  3.6587e-01, -9.2516e-02,  2.9934e-01,  1.3619e-01,\n",
            "         -5.4112e-01,  9.7347e-01, -2.3849e-01,  6.7955e-01, -4.3007e-01,\n",
            "          4.7716e-01, -1.7319e-01,  7.7243e-02, -1.0438e-01, -1.2694e-01,\n",
            "         -3.0271e-01, -1.1647e-01,  3.2223e-01, -1.3007e-01,  1.4976e-01,\n",
            "         -2.6162e-01,  7.9389e-01,  1.0801e-01,  5.9478e-01, -5.4760e-01,\n",
            "          3.1655e-01, -2.6337e-01,  1.1283e-01,  3.0740e-01,  7.1260e-01,\n",
            "         -3.3573e-01, -2.8893e-01,  3.1880e-01, -1.2314e-03, -6.3673e-02,\n",
            "         -1.1606e-01, -9.5203e-02,  3.2343e-01,  8.5631e-02,  9.3792e-02,\n",
            "          2.9213e-01,  3.5644e-01,  2.0004e-01,  3.9612e-01,  2.8311e-01,\n",
            "          4.1012e-01,  2.0014e-01, -2.0514e-02, -2.0842e-01, -6.0751e-01,\n",
            "         -2.7829e-01, -6.0298e-02, -3.1317e-01, -1.5450e-01, -2.0704e-01,\n",
            "         -2.7509e-01,  3.7623e-01, -3.8103e-01,  4.9223e-02,  3.0948e-01,\n",
            "         -2.0108e-01, -6.1706e-03, -1.3101e-01,  3.9151e-01,  2.0452e-03,\n",
            "         -2.3410e-01,  3.5978e-01,  2.8641e-01, -2.9532e-01,  2.3253e-01,\n",
            "         -5.3369e-01, -6.2732e-02,  2.6409e-01,  3.3825e-01,  4.2104e-01,\n",
            "          1.0170e-01,  2.3355e-01, -1.8474e-01,  3.5683e-01,  1.9230e-02,\n",
            "          4.2824e-01,  4.8851e-01,  7.6458e-02,  3.6435e-02,  3.5086e-02,\n",
            "         -1.0078e-01, -8.8634e-02, -3.9534e-01,  1.7299e-01, -2.4503e-01,\n",
            "         -2.8409e-01, -3.4456e-01, -4.3469e-01,  4.2481e-01, -1.6785e-01,\n",
            "         -1.4478e-01,  1.0767e-01, -5.6911e-02, -5.7164e-01, -1.4859e-01,\n",
            "          3.6864e-01,  4.4137e-01, -5.0055e-01, -4.5014e-01, -2.3955e-01,\n",
            "         -4.5249e-01, -2.1723e-01,  2.5098e-02, -3.7755e-01,  1.0960e-01,\n",
            "          1.0547e-01,  2.1943e-01, -4.7626e-01, -7.3827e-02, -7.5057e-02]])\n",
            "\n",
            "tgt:\t tensor([ 14, 375,  41,   3,  11,   2,   0,   0,   0])\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 156 / 7800\tLoss:\t14.848900\n",
            "pred:\t tensor([[-9.1477, -9.3473,  1.3895,  1.1256, -3.4019,  1.3429, -1.7830, -4.1520,\n",
            "         -6.0151, -0.5775, -4.0045,  2.7731, -6.9353, -7.2318, -2.7617, -3.8805,\n",
            "         -3.9049, -4.9486, -5.5791, -4.5560, -4.9913, -4.2276, -7.4941, -6.6493,\n",
            "          0.6832, -5.5908, -3.4240, -7.2487, -3.0636, -6.2697, -4.6127, -5.3060,\n",
            "         -3.8750, -4.6411, -2.7106,  0.4437, -3.3298, -2.3365, -5.4213, -7.8731,\n",
            "         -3.5753, -1.1671, -5.1257, -6.0294, -6.4837, -5.0146, -8.0091, -6.5222,\n",
            "         -1.1071, -3.9140, -3.9708, -5.3479, -2.7647, -4.8963, -2.5424, -5.4218,\n",
            "         -3.8497, -5.3627, -4.8397, -7.6172, -6.3924, -4.8056, -7.1502, -5.8887,\n",
            "         -3.2987, -7.6798, -8.0772, -6.9791, -5.6745, -4.6631, -3.4922, -4.5203,\n",
            "         -3.0528, -5.8465, -2.6362, -1.9845, -7.4953, -6.6259, -2.0149, -3.6402,\n",
            "         -5.3375, -3.7304, -4.5576, -7.3141, -5.2710, -4.3183, -4.9568, -4.4567,\n",
            "         -2.8283, -4.7986, -3.9322, -5.6723, -5.4565, -5.8261, -5.9826, -4.7031,\n",
            "         -4.9931, -6.8028, -5.2929, -7.3524, -6.9132, -2.5119, -6.2917, -4.9976,\n",
            "         -4.8284, -4.7837, -4.2478, -1.2526, -3.5395, -4.8293, -4.1954, -6.4437,\n",
            "         -4.3713, -3.0276, -5.4916, -4.0929, -2.6181, -2.9289, -5.1956, -5.8165,\n",
            "         -4.1293, -5.2622, -4.6438, -4.5098, -5.4533, -2.0089, -5.1694, -3.5813,\n",
            "         -6.9594, -2.9477, -3.6992, -6.2202, -5.6038, -2.7975, -6.2440, -4.9581,\n",
            "         -4.9228, -6.2542, -5.4633, -4.6177, -4.4691, -8.5663, -6.8072, -6.3082,\n",
            "         -4.9847, -7.4104, -1.7083, -2.8521, -2.7576, -6.6004, -8.3716, -3.5027,\n",
            "         -6.0733, -5.4940, -6.2088, -5.5357, -7.8719, -5.9903, -6.3516, -4.0933,\n",
            "         -5.5842, -5.0133, -7.7832, -3.4863, -3.7870, -3.2609, -4.0553, -5.3592,\n",
            "         -6.6151, -4.2223, -4.5433, -1.3174, -4.0834, -6.9452, -2.9350, -5.8440,\n",
            "         -6.3659, -3.8840, -5.8351, -5.4026, -7.4809, -4.4177, -3.8253, -7.2173,\n",
            "         -5.4376, -4.3399, -4.7482, -4.0175, -5.4053, -4.0444, -5.9438, -5.4797,\n",
            "         -3.5013, -4.8120, -7.3628, -4.6380, -4.8103, -4.9011, -6.4937, -6.3345,\n",
            "         -4.9125, -6.1076, -3.4839, -2.4026, -5.5445, -5.1071, -6.4091, -2.8000,\n",
            "         -6.5244, -5.5464, -7.9965, -4.1972, -6.4980, -6.0719, -5.6389, -5.9150,\n",
            "         -7.2798, -2.6178, -3.1460, -6.0517, -3.4360, -4.1597, -5.2077, -7.4555,\n",
            "         -6.2134, -6.1281, -1.6922, -6.9438, -3.4547, -4.2263, -4.1856, -6.3392,\n",
            "         -5.3832, -4.6457, -4.1053, -3.8208, -6.1208, -4.2466, -4.9609, -4.8067,\n",
            "         -4.7267, -6.6354, -5.3012, -5.7147, -7.7684, -3.1876, -5.1225, -4.1190,\n",
            "         -2.6820, -6.2302, -4.2715, -8.4054, -5.6367, -3.4486, -4.5836, -4.3965,\n",
            "         -5.8227, -4.6302, -7.0299, -3.1751, -2.1101, -5.9363, -5.7814, -5.2580,\n",
            "         -4.5629, -3.2528, -8.3292, -4.9973, -6.5689, -7.7840, -6.5432, -5.3787,\n",
            "         -4.7013, -6.9186, -3.7997, -3.2564, -6.0321, -5.2707, -6.5316, -2.9397,\n",
            "         -2.7859, -4.2778, -3.4614, -7.2425, -4.5260, -5.9288, -5.0799, -5.9788,\n",
            "         -4.5099, -4.6119, -7.3773, -7.0322, -1.3062, -4.0629, -8.7003, -4.8168,\n",
            "         -6.7965, -5.1237, -6.2966, -6.8141, -3.6045, -3.4649, -5.4308, -5.5730,\n",
            "         -6.6414, -4.9241, -6.5995, -7.6843, -7.4896, -5.5020, -3.6027, -4.4750,\n",
            "         -6.4038, -6.8125, -6.2063, -3.9466, -4.3158, -6.3963, -4.1455, -4.7674,\n",
            "         -3.8279, -6.0788, -4.1419, -5.2797, -2.2895, -4.9003, -4.4480, -5.5748,\n",
            "         -4.7988, -4.3308, -5.4264, -5.9777, -6.3058, -7.9304, -2.5469, -6.5403,\n",
            "         -8.7813, -5.3976, -4.8413, -2.6367, -5.8367, -6.3922, -6.3007, -4.1754,\n",
            "         -5.6235, -3.5855, -2.3062, -5.4110, -4.6607, -3.8205, -3.4937, -6.8560,\n",
            "         -5.7097, -4.1566, -4.8065, -6.1062, -4.9064, -4.0900, -6.9215, -6.6234,\n",
            "         -5.9790, -5.9619, -6.1286, -6.0242, -5.8892, -7.8091, -6.0889, -3.8041,\n",
            "         -4.8716, -4.6810, -5.7178, -4.8163, -6.3538, -3.7672, -6.2216, -8.2895,\n",
            "         -7.9744, -5.4857, -2.6295, -3.7893, -6.8375, -5.9406, -5.9799, -4.7303,\n",
            "         -7.4683, -8.1642, -5.2217, -4.8022, -5.6326, -6.5307, -6.4908, -7.5035,\n",
            "         -4.7569, -3.3493, -5.2141, -6.7399, -6.8244, -5.8642, -3.6834, -7.1013,\n",
            "         -3.4900, -6.9109, -4.9953, -5.7911, -5.0436, -6.3489, -5.2683, -7.1059,\n",
            "         -6.3619, -4.6647, -2.4100, -4.4077, -6.0935, -6.6792, -4.3056, -1.6729,\n",
            "         -5.0938, -8.1187, -6.2368, -4.2420]])\n",
            "\n",
            "tgt:\t tensor([394, 316, 158,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t12.582722\n",
            "pred:\t tensor([[-10.1279, -10.3833,   0.2597,   1.1787,  -5.5711,   1.1095,  -3.1559,\n",
            "          -2.4399,  -6.4506,  -2.1901,  -6.2372,   2.4639,  -8.4773,  -8.1183,\n",
            "          -3.0791,  -5.5767,  -5.8123,  -6.3972,  -6.8533,  -5.9289,  -5.2623,\n",
            "          -3.9215,  -9.4899,  -8.7649,  -0.1239,  -4.7351,  -9.2517,  -4.8809,\n",
            "          -5.4223,  -5.7813,  -5.3099,  -5.1922,  -4.9295,  -5.4551,  -3.3647,\n",
            "          -0.9789,  -6.3396,  -2.4620,  -5.1358, -10.0835,  -4.2472,  -2.5082,\n",
            "          -5.1506,  -7.2119,  -6.9656,  -5.9646,  -6.3310,  -9.4984,  -2.4938,\n",
            "          -4.2641,  -7.1818,  -5.6055,  -3.4559,  -7.8018,  -6.1433,  -6.7950,\n",
            "          -6.1179,  -6.8451,  -6.2288,  -6.3451,  -8.4802,  -5.2134,  -8.2545,\n",
            "          -8.4409,  -2.8435,  -7.5901,  -8.7815,  -8.0172,  -7.6140,  -8.8128,\n",
            "          -3.9850,  -3.9679,  -4.7282,  -6.4620,  -3.7752,  -3.6620,  -9.9617,\n",
            "          -7.7469,  -3.3771,  -5.2149,  -4.1622,  -6.2376,  -7.1604,  -9.5129,\n",
            "          -5.6976,  -4.2159,  -4.1254,  -6.1712,  -4.8803,  -8.1183,  -3.8759,\n",
            "          -5.5012,  -6.4346,  -8.4427,  -5.3471,  -7.0350,  -2.2943, -11.7665,\n",
            "          -6.6271,  -8.4161, -10.0917,  -4.1805,  -6.8128,  -6.0479,  -2.9308,\n",
            "          -6.8328,  -6.5306,  -3.0103,  -4.7327,  -5.9233,  -7.4149,  -6.3820,\n",
            "          -6.1395,  -4.1748,  -3.6695,  -4.9389,  -3.7257,  -2.8386,  -5.8598,\n",
            "          -6.2686,  -6.0288,  -5.4051,  -5.3068,  -7.9263,  -5.6214,  -3.4163,\n",
            "          -7.7426,  -7.4349,  -3.6326,  -5.9201,  -6.7627,  -5.3890,  -3.9974,\n",
            "          -7.9099,  -6.1060,  -7.0047,  -5.1479,  -5.9679,  -3.7786,  -4.5260,\n",
            "          -9.2975,  -9.3337,  -9.7944,  -5.5706,  -6.6895,  -7.8285,  -3.8173,\n",
            "          -2.5364,  -0.9789,  -4.9323,  -9.9794,  -3.5101,  -8.8167,  -4.7176,\n",
            "          -8.1049,  -7.0193,  -7.8427,  -8.2873,  -6.6064,  -5.5890,  -5.2150,\n",
            "          -6.6111,  -6.3013,  -5.0888,  -5.5663,  -3.9230,  -7.7296,  -6.3861,\n",
            "          -9.1239,  -6.9042,  -4.0733,  -3.3164,  -6.0130,  -8.8130,  -4.4334,\n",
            "          -9.0695,  -6.6200,  -4.7958,  -7.0181,  -7.5202,  -8.3474,  -7.7652,\n",
            "          -5.2273,  -9.2472,  -4.9711,  -2.8242,  -4.3451,  -7.2753,  -3.6932,\n",
            "          -8.0524,  -6.1506,  -5.0953,  -6.4664,  -6.3430,  -5.9067,  -7.2556,\n",
            "          -6.2777,  -7.6059,  -8.3240,  -8.6042,  -6.9636,  -7.5957,  -4.3767,\n",
            "          -4.5769,  -6.2438,  -7.2217,  -7.9211,  -3.7969,  -9.8327,  -7.4198,\n",
            "          -5.9509,  -6.1460,  -8.9125,  -4.8817,  -4.8656,  -5.4866,  -5.6943,\n",
            "          -5.6250,  -7.1516,  -4.9980,  -6.4021,  -4.6273,  -5.8713,  -2.7904,\n",
            "          -6.4486,  -6.8482,  -4.2355,  -8.8015,  -6.1545,  -4.7039,  -4.1448,\n",
            "          -7.4454,  -4.7585,  -5.5135,  -6.0122,  -6.6339,  -8.2244,  -4.4240,\n",
            "          -7.6617,  -6.0516,  -5.0324,  -9.9832,  -4.6766,  -4.8792,  -7.8218,\n",
            "          -6.3612,  -9.8704,  -6.9433,  -5.4853,  -7.8906,  -8.9924,  -7.6417,\n",
            "          -7.1406,  -7.4249,  -6.0830,  -4.5690,  -4.7588,  -6.3608,  -8.8848,\n",
            "          -3.2126,  -5.2338,  -6.7341,  -8.7219,  -7.8809,  -5.6176,  -4.2484,\n",
            "          -7.2007,  -7.1628,  -7.9080,  -9.5447,  -4.3803,  -5.4110,  -6.4926,\n",
            "          -7.3717,  -8.2906,  -5.6457, -10.7320,  -6.6323,  -8.4052,  -4.9733,\n",
            "          -2.6113,  -3.3933,  -4.5180,  -5.9009,  -5.7130,  -4.4821,  -6.0722,\n",
            "          -7.7913,  -7.1196,  -8.2845,  -7.4696,  -9.8048,  -4.0973,  -5.5110,\n",
            "          -8.8519,  -4.8651,  -9.5542,  -4.5359,  -9.0517,  -3.8184,  -6.0536,\n",
            "          -6.2420,  -5.0465,  -5.4074,  -4.3496,  -5.7898,  -5.9930,  -8.1470,\n",
            "          -7.1062,  -4.3977,  -5.9059,  -4.3421,  -7.5674,  -5.8697,  -7.6428,\n",
            "          -5.4445,  -5.0357,  -6.3238,  -6.1519,  -7.0606,  -6.0725,  -8.9481,\n",
            "          -5.1763,  -6.4644,  -5.2476,  -8.4629,  -4.0249,  -5.6055,  -4.6653,\n",
            "          -6.1556,  -7.2054,  -5.0860,  -6.8727,  -7.5759,  -4.5089,  -5.4795,\n",
            "         -10.2943,  -4.5383,  -5.5795,  -7.8081,  -8.4499,  -7.8309,  -8.5999,\n",
            "          -5.9917,  -7.2652,  -3.7218,  -6.4514,  -5.1401,  -6.3149,  -4.9545,\n",
            "          -5.0049,  -9.7813,  -8.7558,  -4.3037,  -5.2839,  -6.5676,  -4.2186,\n",
            "          -5.3463,  -8.1270,  -6.9953,  -8.8545,  -7.4400,  -9.7827,  -4.3704,\n",
            "          -5.6741,  -7.9667,  -9.4685,  -4.4675,  -6.4926,  -8.6188,  -7.4294,\n",
            "          -5.0913,  -6.2138,  -3.9547,  -5.1414,  -8.6413,  -5.8677,  -5.9059,\n",
            "          -6.4434,  -3.9577,  -5.6375,  -6.6085,  -3.4353,  -6.0163,  -7.2340,\n",
            "          -9.2553,  -4.4136,  -5.2262,  -5.8521,  -9.0915,  -4.0162,  -7.9715,\n",
            "          -8.4059,  -7.0479,  -5.1035,  -7.6473, -10.1657,  -4.5177,  -9.4155,\n",
            "          -8.0349,  -3.4860,  -5.2101,  -8.6828,  -5.7949,  -5.0457,  -4.6106,\n",
            "          -6.8840,  -9.4241,  -4.6991,  -3.6772,  -9.1417,  -7.3323,  -4.1914,\n",
            "         -11.2859,  -6.9368,  -5.9603,  -4.1406, -10.6090,  -6.4059,  -8.6754]])\n",
            "\n",
            "tgt:\t tensor([ 14, 294,  45,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t9.689438\n",
            "pred:\t tensor([[-10.8383, -11.1278,  -0.4216,   0.6466,  -4.8972,   0.7832,  -2.7132,\n",
            "          -3.9044,  -8.4744,  -3.4724,  -6.4935,   1.1728,  -7.0381,  -8.2040,\n",
            "          -4.5343,  -5.8945,  -5.5742,  -7.0801,  -6.3765,  -5.7590,  -5.2849,\n",
            "          -4.2134,  -8.2990,  -7.5374,  -0.5058,  -4.8838,  -9.7260,  -7.2645,\n",
            "          -6.3498,  -9.1940,  -8.2279,  -7.6110,  -5.9016,  -6.3398,  -4.0011,\n",
            "          -0.6453,  -5.9089,  -2.5209,  -6.0026,  -9.6411,  -5.4556,  -3.5037,\n",
            "          -4.2853,  -6.5058, -10.5518,  -5.1248, -10.3091,  -9.4664,  -3.3528,\n",
            "          -4.4231,  -6.1771,  -6.4774,  -4.5597,  -9.6691,  -5.2587,  -8.5858,\n",
            "          -5.0605,  -8.0006,  -6.1668,  -8.9144,  -8.5256,  -6.7466,  -8.8913,\n",
            "          -6.8110,  -3.1260,  -8.4415,  -9.2991, -11.3809,  -5.9603,  -8.9711,\n",
            "          -6.5310,  -5.0570,  -3.8810,  -3.8459,  -4.1073,  -5.8019,  -7.8839,\n",
            "          -5.3081,  -3.8534,  -6.1188,  -6.9869,  -5.2033,  -6.7141,  -7.9831,\n",
            "          -6.0289,  -4.4540,  -6.0540,  -6.5280,  -4.8222,  -8.3961,  -4.2874,\n",
            "          -8.6632,  -8.7626,  -8.9448,  -7.0702, -10.0170,  -5.0146,  -9.6324,\n",
            "          -9.4881,  -7.8402,  -8.2311,  -3.5322,  -6.0822,  -5.6685,  -4.4708,\n",
            "          -4.6239,  -5.7556,  -3.8040,  -5.2311,  -7.8025,  -5.9111,  -8.4641,\n",
            "          -6.4129,  -3.0551,  -6.7914,  -7.0393,  -3.7386,  -4.3173,  -3.7847,\n",
            "          -9.5724,  -6.8209,  -7.3610,  -7.6583,  -8.1542,  -7.1501,  -2.3635,\n",
            "          -6.3375,  -6.3101,  -4.6409,  -8.3656,  -4.8540,  -7.6164,  -6.3428,\n",
            "          -7.7490,  -8.1776,  -6.9099,  -3.4917,  -7.2806,  -9.6072,  -4.2343,\n",
            "          -9.6708,  -9.7448,  -9.2956,  -3.5233,  -6.6840,  -8.2700,  -4.5925,\n",
            "          -1.5391,  -3.6162,  -5.9111,  -8.2778,  -2.9250,  -7.0824,  -5.2670,\n",
            "          -8.0658,  -9.2961, -10.1648,  -8.8506,  -6.1795,  -5.7052,  -7.6737,\n",
            "          -7.2388,  -8.5694,  -7.3727,  -3.4971,  -7.0631,  -6.6784,  -7.3647,\n",
            "          -7.1745,  -7.4159,  -6.1283,  -3.0552,  -6.7244, -10.3305,  -3.4266,\n",
            "          -8.1077, -10.1193,  -4.9585,  -9.2440,  -6.9508, -10.1020,  -7.0767,\n",
            "          -4.7849,  -9.6398,  -4.5811,  -6.2325,  -4.4133,  -7.9526,  -4.4914,\n",
            "          -7.5546,  -7.6181,  -6.9684,  -6.6368,  -8.7489, -10.1362,  -7.8476,\n",
            "          -8.5929,  -7.5792,  -8.1805,  -7.5504,  -9.1853,  -9.2023,  -4.7983,\n",
            "          -6.2191,  -8.5251,  -8.3756,  -7.2627,  -3.9289,  -8.4083,  -7.7298,\n",
            "          -8.9124,  -7.2459,  -8.3354,  -8.7402,  -6.1479,  -5.1560,  -6.4627,\n",
            "          -6.5480,  -6.1122,  -5.3275,  -7.8244,  -4.7757,  -5.2867,  -8.6674,\n",
            "          -6.1315,  -8.7697,  -2.5569, -10.9767,  -6.0374,  -4.6330,  -4.1573,\n",
            "          -8.6363,  -6.5051,  -7.2774,  -5.2502,  -7.3005,  -6.2282,  -5.9147,\n",
            "          -8.2420,  -5.9332,  -4.8860, -10.6906,  -6.3920,  -9.1349,  -9.5865,\n",
            "          -6.3165,  -6.9139,  -9.4933,  -7.1274,  -8.5651,  -6.4899, -11.5800,\n",
            "          -6.7399,  -9.9482,  -6.9710,  -7.2525,  -5.5732,  -5.0413,  -7.9762,\n",
            "          -5.0486,  -5.6977,  -7.7794,  -7.2104,  -9.0837,  -5.5057,  -8.0168,\n",
            "          -9.6330,  -7.8836,  -7.9849,  -8.2739,  -3.1293,  -3.3452,  -6.1307,\n",
            "         -11.0029,  -7.8647,  -6.9501, -12.7682,  -7.5877,  -6.1885,  -7.8424,\n",
            "          -5.5907,  -8.4668,  -6.2143,  -6.0821,  -7.1872,  -6.1239,  -8.2738,\n",
            "          -9.4975,  -8.1905,  -7.2697, -10.0700, -12.1302,  -5.1066,  -7.8332,\n",
            "          -9.5872,  -4.5930,  -6.7168,  -2.8913,  -7.3684,  -6.6451,  -2.4484,\n",
            "          -6.3091,  -6.5864,  -9.3517,  -5.8908,  -7.4329,  -7.5329,  -9.7489,\n",
            "         -10.5939,  -7.2597,  -6.7107,  -9.6074,  -9.7057,  -7.1709,  -8.3762,\n",
            "          -6.0962,  -7.6905,  -6.3874,  -7.1443,  -7.9221,  -4.6621,  -9.5543,\n",
            "          -7.2071,  -8.1633,  -7.0676,  -6.2956,  -6.5341,  -6.5771,  -4.3629,\n",
            "          -7.6419,  -7.5153,  -6.7374,  -7.7125,  -8.8264,  -5.1265,  -8.6736,\n",
            "          -8.2905,  -6.7675,  -7.2627,  -5.6544,  -8.9950,  -8.9848,  -8.7771,\n",
            "          -4.7488,  -3.2767,  -5.5262,  -6.5876,  -5.3638,  -6.4635,  -7.5070,\n",
            "          -6.3466,  -9.5974,  -5.8015,  -7.4166,  -4.7227,  -8.8538,  -5.5175,\n",
            "          -6.7791,  -9.6405,  -9.6751,  -6.7814,  -8.0100, -11.1032,  -7.3677,\n",
            "          -5.9174,  -7.4144,  -7.3777,  -6.6195,  -5.5913,  -7.6220,  -7.6578,\n",
            "          -6.4092,  -8.6609,  -4.5565,  -4.9815,  -9.7444,  -8.6704,  -9.3355,\n",
            "          -9.2403,  -6.4614,  -8.6468,  -5.7775,  -8.2532,  -6.1667,  -7.6768,\n",
            "          -8.4864,  -7.4296,  -7.3343,  -6.3261, -11.2624,  -8.1045,  -7.8136,\n",
            "          -8.9591,  -5.4986,  -7.4565,  -8.8399, -11.6675,  -9.0301,  -8.7608,\n",
            "          -9.6352,  -7.9906,  -6.9701,  -8.8056,  -7.2692,  -8.8457,  -7.9518,\n",
            "          -9.6408,  -9.2440,  -5.6708,  -7.5852,  -7.1948,  -8.3040,  -6.8435,\n",
            "         -10.4592,  -6.1938,  -6.8660,  -4.2497, -10.3173,  -7.9639,  -8.1438]])\n",
            "\n",
            "tgt:\t tensor([ 47, 256,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t7.667546\n",
            "pred:\t tensor([[-11.5598, -11.8327,  -1.2829,   0.9473,  -7.2715,   1.2735,  -4.0105,\n",
            "          -2.5675,  -7.8058,  -2.5138,  -8.3618,   0.3240, -10.0719,  -9.8027,\n",
            "          -4.6895,  -6.2728,  -7.2280,  -7.6598,  -7.3771,  -5.9298,  -5.8350,\n",
            "          -6.1137,  -8.6272,  -9.8137,  -0.7632,  -5.8922,  -9.1949, -11.0998,\n",
            "          -7.8523,  -9.0394,  -8.6822,  -6.8590,  -8.5383,  -7.5116,  -4.2792,\n",
            "          -0.5604,  -6.2606,  -4.2728,  -6.6891,  -7.5246,  -3.1184,  -3.4223,\n",
            "          -5.4009,  -7.6872,  -8.4621,  -5.7439,  -8.7059,  -9.7246,  -3.8469,\n",
            "          -4.5164,  -7.4403,  -6.9517,  -5.4983, -10.9018,  -7.9414, -10.6055,\n",
            "          -8.3319,  -6.9035, -10.0110,  -9.5671, -10.6271,  -8.2027,  -8.8978,\n",
            "          -7.0997,  -5.9064,  -7.8767, -10.2327, -12.9529,  -8.5102,  -9.1137,\n",
            "          -6.6166,  -4.4020,  -5.3551,  -7.2164,  -4.7179,  -5.4180, -12.2182,\n",
            "          -7.2086,  -4.9057,  -5.7853,  -9.0505,  -6.2723,  -7.7965,  -9.4432,\n",
            "          -5.5233,  -3.8603,  -5.4094,  -7.3389,  -5.6787, -10.7112,  -5.3256,\n",
            "          -9.5467,  -9.1137,  -6.0394,  -4.5458, -10.8060,  -6.2490,  -9.8114,\n",
            "          -8.5319,  -6.1236, -10.8503,  -3.8240,  -8.1419,  -5.8624,  -7.3920,\n",
            "          -6.5692,  -7.3224,  -3.6512,  -6.3461,  -9.9301,  -6.2464, -10.4096,\n",
            "          -6.8449,  -6.5324,  -6.4134,  -7.5545,  -4.5621,  -6.3822,  -6.4598,\n",
            "          -8.0301,  -8.4907,  -6.3426,  -7.5259,  -8.7248,  -6.6050,  -3.3403,\n",
            "          -8.6350,  -6.5806,  -5.0259,  -6.9609,  -5.3464,  -9.7931,  -5.0032,\n",
            "          -8.8496,  -7.1501,  -6.7331,  -7.3909,  -8.3018,  -7.4587,  -7.6347,\n",
            "          -9.7624,  -9.1332, -10.2530,  -4.1401,  -7.3291,  -9.2549,  -3.0063,\n",
            "          -1.9368,  -4.4939,  -7.6823, -11.9192,  -4.2436,  -8.3075,  -5.3000,\n",
            "         -10.2292, -10.0956, -10.6406,  -8.7513,  -8.9054,  -8.6707,  -6.2466,\n",
            "          -7.6652,  -9.6073,  -4.2233,  -4.1815,  -5.8988,  -6.7136,  -9.0993,\n",
            "          -9.1403,  -5.8958,  -4.5342,  -4.4392,  -5.9164, -10.7750,  -6.2705,\n",
            "          -9.3742, -10.7368,  -6.0473, -10.7533,  -7.3394, -10.1620, -10.0272,\n",
            "          -5.9002,  -9.9434,  -6.1425,  -5.0027,  -8.2681,  -7.5928,  -6.3930,\n",
            "          -5.5500,  -7.5970,  -4.9545,  -8.4074,  -7.9882, -10.1123,  -8.9834,\n",
            "          -8.6740,  -8.6319,  -9.5480,  -7.1300, -10.0604,  -8.2606,  -6.2882,\n",
            "          -6.9857,  -7.8864,  -8.7801,  -8.2196,  -5.2720, -11.0341,  -6.9246,\n",
            "         -11.4710,  -9.7734, -10.0171,  -8.0479,  -7.9334,  -8.1577,  -9.5519,\n",
            "          -5.9686,  -6.2064,  -6.9530,  -6.2177,  -7.9073,  -7.0992,  -7.5287,\n",
            "          -7.8839,  -9.3681,  -3.8576,  -9.6943,  -6.2019,  -5.8640,  -5.7905,\n",
            "         -10.3055,  -8.7301,  -8.0402,  -5.3331,  -6.8466,  -8.5505,  -4.0117,\n",
            "          -9.5455,  -9.5968,  -8.0379, -11.9197,  -6.8923,  -8.6504,  -9.3953,\n",
            "          -8.3030,  -7.8938,  -9.8305,  -5.9769,  -7.8145,  -9.7153,  -8.3511,\n",
            "          -6.3589, -10.1720,  -7.2444,  -6.9327,  -9.8449,  -8.3550, -10.6032,\n",
            "          -6.7615,  -7.0774,  -9.5173,  -9.7298,  -9.0168,  -6.0281,  -7.1365,\n",
            "          -8.6694,  -8.8780,  -7.7711, -11.4221, -10.0404,  -5.8294,  -8.7661,\n",
            "         -10.5535,  -8.7371,  -6.2377, -12.3964,  -9.2500,  -7.7662,  -6.8390,\n",
            "          -4.5301,  -7.1093,  -6.9000,  -7.9840,  -8.2386,  -7.3471, -10.0042,\n",
            "          -8.7377,  -7.5408,  -8.0154,  -8.3882, -11.4107,  -5.0845,  -7.4947,\n",
            "         -10.4131,  -7.4794,  -9.5827,  -7.8906,  -5.3801,  -6.9060,  -6.4787,\n",
            "          -6.9127,  -7.4121,  -8.3386,  -7.1352, -10.9435,  -7.7584, -10.8377,\n",
            "         -12.5027,  -7.2321,  -5.3639,  -5.4611, -10.4977,  -6.6863,  -9.8707,\n",
            "          -6.5229,  -6.6460,  -5.6951,  -6.3218,  -7.9319,  -6.6152, -10.9460,\n",
            "          -4.6175,  -9.9792,  -9.8224,  -8.5839,  -7.1913,  -6.3422,  -6.1435,\n",
            "          -7.3115,  -5.8161,  -4.7437, -10.0815,  -6.6834,  -6.3008,  -8.5094,\n",
            "         -10.1630,  -9.0383,  -9.0863,  -8.4696,  -8.6931, -10.6051, -10.6499,\n",
            "          -3.1017,  -1.8924,  -7.7389,  -5.6909,  -5.7402,  -8.6354,  -8.3554,\n",
            "          -6.1315, -10.9995,  -5.2360,  -9.0894,  -7.4063, -10.2732,  -6.1457,\n",
            "          -8.3133,  -8.0930,  -8.7069,  -7.9642,  -7.8762, -10.1495,  -7.7750,\n",
            "          -4.7456,  -8.8689, -10.6704,  -5.4378,  -4.0753,  -9.3478, -10.4849,\n",
            "          -5.2414,  -8.9538,  -7.2617,  -6.2472,  -9.6728,  -9.2876, -10.7332,\n",
            "         -11.0080,  -6.7683, -10.1606,  -8.8090,  -5.3336,  -7.4035,  -7.5759,\n",
            "         -10.0567,  -9.4184,  -7.5221,  -6.5324,  -8.5540,  -8.3170, -11.3210,\n",
            "         -10.3405,  -8.0385,  -6.7154, -10.6756, -12.7167,  -9.1658,  -7.3641,\n",
            "         -12.0373, -10.0090,  -5.6098,  -5.7985,  -4.6840,  -8.0758,  -8.8351,\n",
            "         -10.7159, -10.8965,  -8.7400,  -7.4980,  -6.2436,  -9.0518,  -9.6886,\n",
            "         -12.5862,  -6.7997,  -7.3555,  -5.4777, -11.1519, -10.5047,  -9.8811]])\n",
            "\n",
            "tgt:\t tensor([166, 132,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t9.220886\n",
            "pred:\t tensor([[-12.1689, -12.4358,  -1.8870,   0.7283,  -7.7890,   0.1288,  -4.8031,\n",
            "          -4.8108,  -8.9701,  -4.5788,  -7.7890,   1.4359, -10.4248,  -9.5665,\n",
            "          -5.4133,  -5.4992,  -7.4245,  -7.5975,  -8.8850,  -6.3911,  -6.9769,\n",
            "          -6.5998,  -8.4027, -11.8385,  -1.5840,  -5.5370,  -8.5114, -10.0019,\n",
            "          -9.3588, -10.0752,  -8.0568,  -7.3679,  -7.9882,  -7.4673,  -6.1674,\n",
            "          -1.6086,  -6.7266,  -4.9869,  -6.3667, -11.7127,  -6.4292,  -5.5988,\n",
            "          -6.4562,  -3.6058, -11.2376,  -5.5475,  -7.7835,  -9.5540,  -5.3816,\n",
            "          -6.0622,  -9.3438,  -8.4389,  -7.3828, -12.4430,  -8.1254, -12.2212,\n",
            "          -8.2993,  -8.6968,  -8.5804,  -9.9760,  -8.1204,  -9.4551, -10.9948,\n",
            "          -9.0865,  -4.0526,  -8.3273,  -9.7723, -12.8769,  -8.7226, -10.5799,\n",
            "          -5.2027,  -6.1189,  -6.0982,  -7.9330,  -5.0228,  -5.6413,  -9.5511,\n",
            "          -7.7493,  -5.3996,  -8.4140,  -8.4245,  -7.4015,  -8.4868,  -9.7268,\n",
            "          -6.0434,  -4.4932,  -5.0371,  -8.0954,  -6.5160,  -9.5699,  -7.0579,\n",
            "          -7.6488,  -8.7642,  -8.6054,  -6.0060,  -8.6502,  -5.6390, -12.4185,\n",
            "         -10.3462,  -8.1784, -10.0022,  -5.0713,  -7.5144,  -5.5320,  -5.1020,\n",
            "          -7.8778,  -7.3887,  -4.2068,  -6.2557,  -7.2090,  -9.3434, -10.7679,\n",
            "          -4.9535,  -5.9660,  -7.5561,  -7.0775,  -4.8772,  -4.5402,  -5.9990,\n",
            "          -7.5526,  -9.2813,  -8.4500,  -8.3061, -10.5805,  -8.4935,  -4.8179,\n",
            "          -7.9047,  -8.7265,  -7.8839,  -4.1833,  -8.0998,  -9.3811,  -4.9135,\n",
            "          -8.8523,  -9.3966,  -7.2451,  -5.6949,  -8.4142,  -9.5805,  -6.7788,\n",
            "         -12.4911, -13.4927,  -7.2643,  -3.5997,  -6.4069, -11.5011,  -7.8403,\n",
            "          -3.9763,  -4.3445,  -8.8833,  -9.2271,  -4.5407, -11.0312, -10.4517,\n",
            "          -9.0715, -11.6558, -11.3571,  -9.5342,  -3.4217,  -9.8487,  -6.1795,\n",
            "          -8.5538,  -9.7450,  -7.5985,  -5.1134,  -5.7532,  -7.2132,  -9.5362,\n",
            "          -9.6992,  -6.7817,  -6.0946,  -5.6655,  -7.6810, -11.0250,  -5.8225,\n",
            "          -8.6148, -12.8267,  -5.8337,  -9.7832,  -9.3467, -11.8915, -10.5901,\n",
            "          -6.6629, -12.9202,  -5.1203,  -6.7525,  -7.7505,  -9.3352,  -8.8685,\n",
            "          -7.9991, -11.1044,  -7.3034,  -9.7272, -10.1479, -10.4317,  -7.9359,\n",
            "         -12.2413,  -9.1200, -11.3064,  -8.6868, -10.1925, -11.8500,  -6.0949,\n",
            "          -8.7623,  -8.1162,  -8.2609,  -9.5244,  -6.3969,  -9.4994, -10.3004,\n",
            "          -9.9106,  -7.7897,  -8.6315,  -8.5183,  -7.7439,  -7.1044,  -8.6792,\n",
            "          -6.6766,  -9.7754,  -6.5895,  -9.4181,  -7.9584,  -9.9171, -12.7901,\n",
            "          -7.7842,  -5.9594,  -4.8357,  -8.4855,  -6.2288,  -3.2932,  -5.7554,\n",
            "          -8.4919,  -6.5387,  -5.3730,  -6.1656, -10.5338,  -9.8754,  -6.8457,\n",
            "          -9.7014,  -7.1604,  -7.8473, -15.1787,  -8.5417,  -8.6595, -10.4025,\n",
            "          -8.4658,  -9.2349,  -8.2852,  -3.8989, -11.7473,  -8.2310,  -9.2372,\n",
            "          -8.4732,  -8.2674,  -7.2699,  -8.9238,  -9.7253,  -8.6962, -10.8173,\n",
            "          -6.4877,  -7.2824,  -9.8888,  -8.5877, -11.3878,  -6.7127,  -7.9979,\n",
            "         -10.1287, -10.7578,  -5.6313,  -9.4688,  -8.3351,  -7.3903,  -8.3578,\n",
            "          -9.9968, -11.2381,  -8.1031, -11.8992,  -9.8850,  -6.7820,  -7.9917,\n",
            "          -6.0656,  -8.8042,  -9.1335,  -6.8899,  -4.8724,  -8.1231, -10.6558,\n",
            "          -8.8026,  -8.7408,  -8.1616, -11.4734, -14.1583,  -6.5965,  -9.5917,\n",
            "         -13.0566,  -7.8831, -11.8180,  -5.5835, -10.5443,  -6.7220,  -5.7439,\n",
            "          -4.1868,  -6.4762,  -7.6965,  -8.3958, -10.0358,  -6.8736, -10.7794,\n",
            "          -9.9727,  -6.8192,  -5.6892,  -6.8848,  -9.6696,  -5.3770,  -8.3715,\n",
            "          -7.0800,  -8.4957, -12.8140,  -9.0101,  -9.3302,  -6.6099, -12.4023,\n",
            "          -2.9587,  -7.9195,  -6.7562,  -9.5768,  -9.3777,  -5.5327,  -5.5845,\n",
            "          -8.2957,  -8.2809,  -7.9648, -12.1789,  -6.0769,  -9.2309,  -8.4799,\n",
            "         -10.3565,  -8.9369,  -9.0998,  -7.8277, -10.1803, -10.2132, -11.0446,\n",
            "          -8.3517,  -6.5162,  -4.1560,  -6.6161,  -6.9132,  -6.8677,  -6.5411,\n",
            "          -6.4657, -11.2491,  -8.1319,  -8.4186,  -8.5389,  -7.2473,  -8.6586,\n",
            "          -7.9504,  -7.5205,  -9.6485,  -8.6879,  -9.1272,  -7.6830,  -8.2569,\n",
            "          -6.1510, -11.0802, -10.7526,  -7.4985,  -5.9499,  -7.8926,  -8.2502,\n",
            "         -12.2983,  -9.7612,  -9.8318,  -4.2636,  -9.7940,  -6.7802,  -9.8723,\n",
            "          -9.3671,  -7.7631,  -8.9808, -10.0178,  -8.8578,  -6.7068,  -9.6387,\n",
            "         -12.5492, -10.1759,  -7.7914,  -4.7045, -10.3747,  -8.5637, -10.1935,\n",
            "          -8.2587,  -6.9743,  -9.0677,  -9.8621,  -9.0171,  -8.6784,  -9.9192,\n",
            "          -9.7000,  -8.8219,  -8.5617,  -9.9429,  -8.2252, -10.3630,  -8.2943,\n",
            "          -9.7912, -10.6329,  -4.5076,  -5.3135,  -7.6432, -10.3476,  -9.4038,\n",
            "         -12.6084,  -6.7301,  -5.2775,  -7.6317,  -7.7325,  -9.7994,  -9.7454]])\n",
            "\n",
            "tgt:\t tensor([15,  3, 75,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t8.165549\n",
            "pred:\t tensor([[-12.6703, -12.9686,  -2.0259,   0.0400,  -7.9484,   0.5252,  -4.8318,\n",
            "          -6.3491,  -7.5066,  -4.8436,  -7.8988,   1.4164,  -8.9708,  -9.0402,\n",
            "          -5.1316,  -6.8631,  -7.4884,  -8.2192,  -9.1224,  -7.1292,  -7.1488,\n",
            "          -7.8040, -10.3325, -11.2322,  -1.5967,  -7.3202, -11.3587, -11.1139,\n",
            "         -11.4846,  -8.9737, -10.2269,  -7.3857,  -9.1150,  -8.2078,  -6.1210,\n",
            "          -1.2067,  -7.4238,  -3.9247,  -6.8573, -11.2999,  -5.5102,  -4.9196,\n",
            "          -5.4534,  -4.9739, -13.3770,  -6.2294,  -8.9501, -11.4831,  -4.1271,\n",
            "          -6.2704,  -9.8383,  -6.9509,  -7.1078, -12.2843,  -6.0641, -12.1973,\n",
            "          -9.2253, -10.6398, -12.1330,  -9.8062, -12.6691,  -8.1197, -10.9686,\n",
            "          -8.6961,  -5.6821,  -8.1293,  -9.7504, -10.6997, -11.2395,  -8.8675,\n",
            "          -7.1285,  -5.2731,  -5.8541, -10.9952,  -6.4253,  -7.3378,  -9.9993,\n",
            "          -7.7382,  -6.2040,  -9.0675,  -6.5548, -10.9793,  -9.6623,  -9.7394,\n",
            "          -8.6153,  -3.7553,  -6.3404,  -9.4290,  -6.4416, -12.5457,  -7.1081,\n",
            "          -7.9352, -10.2715,  -8.9259,  -9.0475,  -8.7800,  -3.5136, -10.0142,\n",
            "         -10.1014,  -8.1650, -10.8968,  -6.7704,  -7.3787,  -5.8437,  -6.4186,\n",
            "         -10.8075,  -7.1406,  -5.1417,  -7.7444,  -8.3573, -11.0959, -10.7980,\n",
            "          -7.1955,  -6.5649,  -6.4207,  -8.5929,  -5.8255,  -7.4547,  -6.3017,\n",
            "          -8.8585,  -8.9292,  -9.3470,  -7.9943,  -9.1803,  -6.6130,  -5.5065,\n",
            "         -10.6231,  -6.6226, -10.3226,  -6.3273,  -6.2003,  -8.5174,  -5.7851,\n",
            "          -7.9589,  -7.0745,  -9.2386,  -7.2337,  -8.1760,  -9.5097,  -7.1454,\n",
            "         -11.3448, -13.4493,  -8.7873,  -7.3380,  -4.7237, -12.6853,  -4.2373,\n",
            "          -3.9704,  -6.1225, -10.7149, -11.8248,  -3.7540,  -9.2238,  -6.7228,\n",
            "         -10.3172, -12.9850, -12.7635,  -9.8025,  -5.7563,  -8.4627,  -7.6223,\n",
            "          -7.9124,  -9.3530,  -7.1330,  -6.7185,  -5.1265,  -8.0764, -11.6243,\n",
            "         -10.9957,  -7.7604,  -7.4790,  -6.5652,  -8.7166, -13.6036,  -5.4953,\n",
            "          -9.5520, -12.6876,  -7.1790,  -8.2461,  -8.4072, -13.5928, -10.1281,\n",
            "          -7.6949, -11.6813,  -6.8838,  -6.5259,  -7.0907,  -8.3959, -11.2793,\n",
            "          -9.4300, -11.9855,  -7.0906,  -9.3973,  -9.4977, -11.1183,  -7.2187,\n",
            "          -9.2280,  -9.1167, -12.9037,  -8.8176, -10.8865, -10.7004,  -6.9903,\n",
            "          -8.8595,  -9.1929, -10.5637,  -8.4499,  -7.5737, -10.7336,  -8.9999,\n",
            "          -9.9073,  -7.9063, -10.2041, -11.5099,  -9.5703,  -9.5762,  -9.3963,\n",
            "          -6.3687,  -9.5308,  -8.2832,  -6.9926,  -9.7262,  -9.1541,  -9.4986,\n",
            "          -8.5438, -10.0356,  -3.6296, -10.9683,  -6.3505,  -4.7781,  -6.3509,\n",
            "         -12.1703, -10.1425,  -8.5256,  -6.0921, -10.0464, -12.2941,  -7.5171,\n",
            "          -9.6166,  -7.2565,  -7.8371, -13.2376,  -7.4069,  -9.0596, -12.9918,\n",
            "          -9.7367, -13.0868, -10.6544,  -5.6056, -10.4043, -10.1137, -11.3280,\n",
            "          -8.5027, -10.8605,  -9.2176,  -8.3401, -10.1706,  -8.3257, -12.3034,\n",
            "          -6.5215,  -6.5645,  -8.6977,  -9.5179, -10.1804,  -5.4142,  -9.7513,\n",
            "         -13.0270, -10.2170,  -7.1143, -11.6161,  -9.2595,  -6.2665,  -8.5570,\n",
            "         -12.8976, -10.7158, -10.4158, -14.2874, -10.4567,  -8.4760,  -9.9509,\n",
            "          -7.2254,  -7.3848,  -6.6194,  -9.5394,  -8.7352,  -8.5369,  -9.7473,\n",
            "         -10.8894,  -9.2396,  -8.4197,  -9.6402, -11.0032,  -5.4461, -10.3587,\n",
            "         -12.3238,  -7.9610, -10.7282,  -8.5889, -10.4971,  -7.9358,  -6.3385,\n",
            "          -5.9594,  -6.9599,  -8.8084, -10.7300, -12.0541,  -5.2150, -11.2777,\n",
            "          -7.2764,  -8.9106,  -6.6593,  -6.8120,  -9.7771,  -8.8070, -10.5464,\n",
            "          -7.4061,  -7.6816,  -6.8925, -10.2442, -10.3012,  -7.5266, -12.6205,\n",
            "          -6.1618,  -8.9861,  -8.7525, -11.5402, -10.0164,  -5.6296,  -6.3646,\n",
            "          -7.9029,  -7.6869,  -5.9053, -10.1860,  -7.8384,  -9.3517,  -8.5237,\n",
            "         -10.9610,  -9.6994,  -8.0242,  -5.4770, -10.2769, -10.1337, -11.8296,\n",
            "          -5.5447,  -5.2738,  -7.0618,  -6.9441, -10.9819,  -9.6675, -10.5532,\n",
            "         -10.4952, -13.9786,  -8.4576, -10.5588, -10.1215, -10.0363,  -8.9521,\n",
            "          -5.4561,  -9.3971, -10.1845,  -8.7849,  -7.8711, -11.5946,  -9.1167,\n",
            "          -7.9757, -11.9808,  -7.6076,  -9.5461,  -9.4080, -11.3696, -10.0226,\n",
            "          -2.9357, -11.0630,  -8.1916,  -5.9356, -10.3972,  -9.0407, -10.7894,\n",
            "         -11.3391,  -8.6413,  -8.1568,  -7.5225,  -9.9930,  -8.8552, -10.4005,\n",
            "         -11.8435, -10.5932,  -8.5959,  -8.1314, -13.2286,  -9.9945, -15.7926,\n",
            "         -13.4245,  -7.2179,  -8.8834, -10.3651,  -9.7735, -11.9188,  -9.5272,\n",
            "         -15.9977, -12.3493,  -7.3521, -11.2629, -11.1723, -10.4213,  -9.0208,\n",
            "          -8.8780, -10.0967,  -7.7881,  -4.7719,  -7.4898, -10.0809,  -9.2130,\n",
            "         -13.0235,  -9.3426,  -6.4406,  -7.8138, -10.5391,  -9.1715, -10.6649]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116,  72,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t5.944228\n",
            "pred:\t tensor([[-13.1137, -13.3456,  -2.2540,  -1.1838,  -7.7613,  -0.1918,  -6.1282,\n",
            "          -4.6728, -10.3256,  -3.8990,  -9.4285,   0.2929, -11.1348,  -8.8218,\n",
            "          -4.7286,  -8.6834,  -8.8012, -10.0446, -10.6596,  -8.0541,  -5.3472,\n",
            "          -7.6761, -10.0423, -12.4108,  -1.8766,  -9.3954, -11.7938,  -9.1649,\n",
            "         -11.3701, -11.4236,  -8.9704,  -7.3066,  -7.0699,  -8.7596,  -5.5318,\n",
            "          -0.5532,  -7.0671,  -4.5393,  -5.9277, -10.6558,  -5.5039,  -4.8975,\n",
            "          -6.7618,  -9.3415, -13.3747,  -8.4294,  -9.6399, -11.3271,  -3.1047,\n",
            "          -4.4441,  -9.1611, -10.2801,  -6.3095, -13.1755,  -7.5168, -12.0130,\n",
            "          -9.1793,  -9.3053, -12.4267, -10.7317, -11.8442, -10.0243, -12.9934,\n",
            "         -12.9425,  -5.9879,  -8.5441, -11.1407, -12.3372, -10.6319, -11.4728,\n",
            "          -6.9986,  -7.3183,  -5.4273,  -8.5232,  -6.7383,  -7.9620, -12.6738,\n",
            "          -8.9367,  -6.8910,  -6.5896,  -8.0391,  -8.4690, -10.6765, -10.5225,\n",
            "          -9.2391,  -6.3487,  -8.5321,  -8.7616,  -8.0437, -11.5026,  -5.9296,\n",
            "         -10.7678, -10.5843,  -9.9498,  -6.5470, -15.2023,  -3.8562, -14.5861,\n",
            "         -12.3492,  -6.5203, -11.5128,  -5.2083,  -8.6413,  -5.4566,  -9.4183,\n",
            "          -9.2385,  -9.4979,  -5.8792,  -7.2036,  -9.2778,  -9.3402, -14.0711,\n",
            "          -6.9351,  -3.3594,  -7.5427,  -9.3894,  -5.9089,  -7.8568,  -5.1012,\n",
            "         -10.1748, -10.0374, -10.6091,  -6.4465, -10.5578,  -7.0608,  -1.5934,\n",
            "          -9.0539,  -9.3548, -10.5031,  -5.5290,  -5.7538, -12.0404,  -5.9609,\n",
            "         -12.0010,  -8.7696, -10.1206,  -5.2573, -10.2197, -11.1724,  -7.7395,\n",
            "         -11.9774, -12.4569, -12.2875,  -6.5800,  -9.8726, -15.4477,  -5.0355,\n",
            "          -3.7685,  -4.9985,  -7.8591, -13.9174,  -6.2511,  -9.5712, -10.6937,\n",
            "         -10.8404, -12.2282, -11.5965, -11.7177,  -9.3309,  -9.2190,  -7.9234,\n",
            "          -9.4147, -12.0833,  -7.1806,  -5.1958,  -5.0131,  -8.7077, -11.6543,\n",
            "         -13.3256,  -8.9998,  -6.7550,  -5.6789,  -7.8143, -12.8976,  -5.0103,\n",
            "         -11.3199, -13.2989,  -8.8917,  -9.6951,  -9.0878, -10.7115, -10.6074,\n",
            "          -9.2231, -12.4092,  -8.0343,  -6.3741,  -8.3251,  -8.6371,  -9.8107,\n",
            "          -7.5564, -12.8666,  -8.3127, -10.5369, -10.1418, -11.6916,  -9.9831,\n",
            "          -7.6780,  -9.9420, -13.7185, -11.0560, -10.8943, -13.0080,  -7.3285,\n",
            "          -8.7700, -11.2098, -10.2465, -10.0976,  -8.1480, -10.7958,  -8.6522,\n",
            "         -11.2337, -11.6566, -12.0766,  -9.3206,  -8.9248, -13.2597,  -8.6217,\n",
            "          -7.2870,  -7.7521, -10.3512,  -7.8521,  -9.4985, -11.2952, -11.1761,\n",
            "          -9.3066,  -7.9015,  -5.8998, -12.3858,  -7.4831,  -4.4471,  -6.7627,\n",
            "         -13.7519,  -6.6114,  -8.5420,  -7.9697,  -9.1880,  -8.5056,  -6.4889,\n",
            "         -11.1261,  -7.0766,  -8.9111, -16.3955,  -7.3055, -11.2618, -14.0220,\n",
            "          -9.8307,  -7.7682,  -9.8621,  -6.2580, -11.7350, -13.2524, -13.4538,\n",
            "          -8.7429, -12.4392,  -8.5955,  -8.0746,  -9.3687,  -7.2220, -12.3764,\n",
            "          -6.6209,  -8.8368,  -8.1008, -11.9228, -11.3929,  -8.1506,  -8.5861,\n",
            "         -11.6119, -12.4102,  -8.0351, -10.5307,  -7.1552,  -6.5180,  -6.6222,\n",
            "         -13.2489, -10.5251,  -8.6595, -15.9630, -10.1932,  -8.6599,  -8.4133,\n",
            "          -4.7842,  -8.6699,  -7.6334, -10.8551,  -9.1006,  -8.8838,  -9.2469,\n",
            "         -12.4353, -10.9987,  -7.3726, -10.4778, -14.3118,  -6.4867,  -8.6734,\n",
            "         -13.1772,  -9.0045, -12.4566,  -9.2748, -11.7412,  -9.5190,  -7.3924,\n",
            "          -9.6174,  -8.8791,  -9.6213,  -8.4613, -10.6350,  -7.6819,  -9.3991,\n",
            "         -11.8837,  -9.6541, -10.1484,  -9.0355,  -9.7460,  -7.6348, -12.3508,\n",
            "          -8.6465, -10.5031,  -7.3533,  -8.0178, -10.2739,  -7.3181, -11.7619,\n",
            "          -6.0553, -10.3422, -11.7452,  -9.9629,  -8.2487,  -8.2935,  -7.6575,\n",
            "          -9.6436,  -9.8865,  -7.6216, -13.5739,  -6.3597,  -9.1384,  -9.9954,\n",
            "         -10.8117,  -8.1095,  -9.7412,  -8.3163, -12.5863, -11.5905, -13.4826,\n",
            "          -9.6876,  -8.3005,  -7.8777,  -7.8606,  -9.8833,  -7.3546,  -8.8745,\n",
            "          -8.1850, -12.8158,  -8.1520, -13.4625,  -9.9485, -11.9459,  -7.1413,\n",
            "          -8.7070, -11.3494,  -9.5441, -10.7301,  -9.8068, -12.4290, -10.4317,\n",
            "          -9.4265, -11.5987, -10.9633,  -7.2373,  -8.4640, -10.4300, -10.8251,\n",
            "          -5.5757,  -7.0859,  -9.7431,  -5.9987, -11.8595,  -7.3257,  -9.1920,\n",
            "         -12.6212, -11.4688, -12.8861,  -7.9755,  -8.5726,  -7.9183, -11.2406,\n",
            "         -12.0848, -10.7175,  -8.8583,  -7.0721, -11.8127, -11.6530, -10.7386,\n",
            "         -12.0800,  -9.0625, -12.2212, -12.8443, -14.4199,  -7.7707, -11.0720,\n",
            "         -13.3715, -11.5070,  -9.8439, -10.9236,  -6.6508, -11.4300, -12.4799,\n",
            "          -9.3005,  -8.4976,  -9.6063,  -7.8061,  -7.5017, -12.0954,  -9.0142,\n",
            "         -13.6070,  -9.3006,  -7.5135,  -8.4180,  -9.6858, -10.5773, -11.6006]])\n",
            "\n",
            "tgt:\t tensor([46, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t5.707947\n",
            "pred:\t tensor([[-13.4762, -13.7586,  -2.7557,  -0.9191,  -8.6580,  -0.1739,  -6.0376,\n",
            "          -4.5317, -11.3084,  -6.5488,  -9.2009,   0.3831, -11.9089, -13.0198,\n",
            "          -5.3348,  -7.5731,  -7.8391, -10.2901,  -9.8236,  -8.3977,  -7.6998,\n",
            "          -6.3075,  -9.1299, -12.5884,  -1.0684,  -9.0852, -13.7169, -11.7351,\n",
            "         -11.1798, -11.3625, -10.2729,  -9.6981,  -6.1779,  -8.7028,  -7.5705,\n",
            "          -2.3584,  -7.4997,  -6.0713,  -6.9466, -11.0608,  -4.7862,  -4.4065,\n",
            "          -5.6195,  -9.2683, -14.9627,  -6.5686, -11.7424, -11.0721,  -4.7461,\n",
            "          -6.1321, -10.4654,  -9.3095,  -7.1431, -11.1976,  -7.1029, -14.4045,\n",
            "          -9.9453,  -9.3951, -14.0782,  -9.5226, -11.4066,  -9.3724, -13.4179,\n",
            "         -10.6311,  -5.4507,  -8.7559, -14.2065, -11.6465, -13.4765, -11.7651,\n",
            "          -6.8999,  -7.2771,  -5.2902,  -8.5974,  -7.1353,  -7.4791, -12.7528,\n",
            "          -9.4238,  -7.2431,  -7.7711, -10.0721,  -9.3246,  -9.1189, -11.5408,\n",
            "          -8.6365,  -7.9044,  -7.6419,  -9.6919,  -7.5553, -12.8723,  -6.1640,\n",
            "         -10.6753,  -9.4285, -11.9205, -10.2512, -11.4712,  -6.2112, -13.8440,\n",
            "          -8.5736,  -7.8729, -11.6743,  -5.2792,  -7.0597,  -5.5643,  -7.0256,\n",
            "          -9.6563, -11.4853,  -5.7831,  -8.3276,  -9.4539, -10.0181, -13.3857,\n",
            "          -6.1571,  -5.2813,  -8.2793,  -8.7084,  -6.9909,  -7.7987, -11.9324,\n",
            "          -9.1036, -10.5625, -11.3273, -10.2565, -11.2807,  -6.1348,  -5.0689,\n",
            "         -10.6120,  -7.5295,  -8.3035,  -6.1850,  -7.0947, -10.4337,  -7.4187,\n",
            "          -9.2311, -11.0231, -11.3241,  -7.2792, -11.6379, -10.1811,  -8.0812,\n",
            "         -12.3926, -14.4515, -10.4941,  -7.5332,  -8.7284, -13.9962,  -5.3814,\n",
            "          -2.6772,  -6.0349,  -7.5569, -11.8648,  -5.6947, -11.6028, -10.3179,\n",
            "         -13.3933, -12.9513, -13.3223, -12.9283,  -7.5754,  -8.5334,  -7.3372,\n",
            "          -9.3399,  -9.4465,  -7.3346,  -6.4060,  -6.8681, -10.9941, -12.0706,\n",
            "         -12.2734,  -9.6900,  -8.6248,  -7.3734,  -8.8578, -13.0575,  -5.5230,\n",
            "         -11.0334, -13.6905,  -7.0567, -12.8528,  -9.0970, -13.3232, -12.2663,\n",
            "          -8.2349, -13.4615,  -6.6050,  -6.9562,  -9.3242, -10.0986, -10.8945,\n",
            "         -10.3976, -10.3941,  -7.4318, -11.1073,  -9.9299, -10.3178, -10.8721,\n",
            "         -10.6729,  -9.3961, -14.7988, -10.9156, -13.4404, -11.3415,  -7.2142,\n",
            "          -7.5893, -10.3533, -11.5879, -12.7429,  -7.6012,  -9.8667, -12.0850,\n",
            "         -13.1689, -10.6473, -13.2875, -12.1534,  -7.5705,  -8.7466,  -7.5301,\n",
            "          -7.2389,  -7.5512,  -8.2956,  -7.4518, -10.5283, -10.1014, -11.6013,\n",
            "         -12.2735,  -9.8286,  -6.8677,  -9.8238,  -7.8446,  -6.8845,  -8.5147,\n",
            "         -13.7646,  -8.0734, -11.1174,  -7.1490, -11.6347,  -8.4322,  -8.9905,\n",
            "         -11.2262,  -8.5983,  -7.8871, -16.2681, -10.2156,  -9.3647, -12.5583,\n",
            "          -9.3427, -11.4837, -12.8266,  -8.0004, -10.7762, -12.9773, -12.7866,\n",
            "          -6.7073, -11.9392,  -9.9293, -10.4563,  -8.8752,  -8.6083, -12.9974,\n",
            "          -6.6119,  -7.7508, -11.4458, -12.2969, -15.7288, -10.4571, -10.0303,\n",
            "         -13.4433, -12.7992,  -8.3922, -12.2783,  -7.6961,  -7.1312,  -8.0574,\n",
            "         -12.3364, -11.9592,  -9.9712, -13.3531, -10.6224,  -6.9238, -11.0591,\n",
            "          -8.5333, -11.0347, -10.6663,  -7.9952,  -9.0149, -10.0257,  -9.4133,\n",
            "         -13.1150, -10.5294,  -8.1503,  -9.7039, -15.9043,  -4.7442, -10.8494,\n",
            "         -13.1786, -12.3204, -11.0970, -10.8472, -10.9314,  -9.1328,  -5.5920,\n",
            "          -8.3412,  -8.9333, -10.4109, -11.4826, -13.8459,  -8.0528,  -8.9809,\n",
            "         -11.5974,  -8.1347,  -9.8381,  -6.0736, -10.6547, -10.1453, -12.1046,\n",
            "          -8.2809,  -9.1818, -12.5644,  -8.9813,  -9.7204,  -9.0723, -11.9187,\n",
            "          -7.1807, -10.8139, -10.1784, -10.0252,  -9.1303,  -8.6103,  -9.3808,\n",
            "          -6.6668,  -6.9218,  -7.7206, -10.7806, -10.3012,  -9.1515, -11.9723,\n",
            "         -12.5544, -12.4995, -11.5159,  -6.9935, -11.5608, -11.9566, -12.8015,\n",
            "          -6.9739,  -5.6033,  -6.8365,  -5.3436, -11.5775,  -9.1180,  -6.7801,\n",
            "          -8.9714, -11.6182,  -8.6760, -10.7451,  -9.7950, -10.2391,  -8.1830,\n",
            "          -7.3144, -11.9149, -11.3957, -10.8458,  -8.9080,  -9.6005,  -8.2707,\n",
            "          -8.8762, -11.9793, -12.0782, -10.9847,  -8.3094,  -9.7686, -12.2907,\n",
            "          -8.0592, -12.9493, -12.2114,  -8.9459, -11.0066,  -9.3000, -13.8536,\n",
            "         -15.1453,  -9.0491, -11.7371,  -9.9991,  -8.1045,  -8.3542, -10.0883,\n",
            "         -12.0451, -10.2648, -11.7648,  -5.9220,  -7.7805,  -9.6691, -13.3863,\n",
            "         -16.3391,  -7.8209,  -9.1662, -14.1474, -13.5681,  -9.7435, -11.4353,\n",
            "         -12.0173, -11.3048,  -8.3158, -11.8078,  -8.5805,  -9.6628, -10.3438,\n",
            "          -8.3580, -10.3297,  -6.5760,  -7.7993,  -8.1351, -13.6000, -12.5262,\n",
            "         -12.1552,  -6.5808,  -5.6792,  -7.2672,  -8.8942,  -8.2071, -12.9984]])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t12.835830\n",
            "pred:\t tensor([[-13.6707, -13.9868,  -3.0163,  -1.4049, -11.1357,  -1.1403,  -5.0803,\n",
            "          -5.2989, -10.5042,  -4.6921, -10.6567,  -1.3516,  -7.8872, -11.1092,\n",
            "          -6.1379,  -8.4498,  -8.2698, -10.1738,  -8.6088,  -7.6895,  -7.1279,\n",
            "          -8.1337, -11.4519, -11.8694,  -0.4815,  -6.8605, -11.3050,  -9.9949,\n",
            "         -11.5368, -10.9218,  -8.6235,  -9.2927,  -6.6638,  -9.1751,  -6.7213,\n",
            "          -2.6180,  -7.5638,  -4.3334,  -7.6520, -12.7643,  -5.9216,  -4.9677,\n",
            "          -6.7065,  -6.5882, -11.8423,  -8.2576,  -9.8328, -12.7613,  -5.7819,\n",
            "          -5.9715,  -8.3560,  -8.4700,  -7.0207, -12.3244, -10.2711, -13.7143,\n",
            "          -9.5351, -10.4899, -10.6527,  -9.8075, -10.1318, -10.5932, -12.7169,\n",
            "         -11.5461,  -5.7503,  -8.2973, -11.7795, -13.9843, -12.4742, -12.7210,\n",
            "          -8.0455,  -8.5895,  -5.1418,  -8.4664,  -6.7893,  -7.1905, -11.8492,\n",
            "          -7.4823,  -6.0658,  -7.7915,  -7.6628,  -7.0641, -10.5742, -12.0109,\n",
            "          -6.8605,  -5.6815,  -9.7448, -10.1921,  -7.6332, -13.0870,  -6.7469,\n",
            "         -10.6138, -10.3964, -12.2652, -11.2592, -12.7153,  -5.2190, -11.0571,\n",
            "         -10.9959, -10.1361, -12.8212,  -5.5905,  -7.9943,  -8.0537,  -9.7704,\n",
            "          -9.7990,  -8.4397,  -7.1747,  -9.3196, -10.0457, -11.7920, -11.6188,\n",
            "          -6.3201,  -5.5516,  -7.0465,  -8.0874,  -6.5530,  -8.7378,  -8.4553,\n",
            "         -11.5131, -10.9143,  -9.3527,  -8.1512, -11.9690, -10.1297,  -4.4533,\n",
            "         -12.4293,  -9.9632, -10.4077,  -4.4148,  -6.1088, -11.3227,  -7.6950,\n",
            "         -11.0265, -10.5967, -11.8815,  -8.7190,  -9.6718, -12.0932,  -8.8373,\n",
            "         -11.4378, -15.1534, -11.0911,  -6.9685,  -7.1134, -14.3897,  -8.7360,\n",
            "          -3.3333,  -5.4017,  -9.3525, -13.2669,  -5.7195, -12.4635,  -9.3874,\n",
            "         -10.7783, -13.9494, -13.5517, -11.5925,  -8.4375,  -8.7563,  -8.3529,\n",
            "         -11.2952, -10.7793,  -7.4819,  -5.6173,  -8.6246,  -9.7138, -13.1715,\n",
            "         -11.4963,  -9.4689,  -7.4302,  -6.8991,  -9.0116, -14.5545,  -4.7911,\n",
            "         -11.6695, -13.7685,  -8.5573,  -9.5590,  -8.2825, -13.7462, -12.7042,\n",
            "          -8.7953, -15.4597,  -7.4865,  -7.1209,  -8.3396, -11.3435, -11.2779,\n",
            "          -8.9339, -13.8151,  -5.8087, -11.0463,  -9.3900, -10.9598,  -9.0979,\n",
            "         -15.6101,  -9.0137, -15.5442, -12.1611, -10.6657, -13.2363,  -7.1410,\n",
            "          -8.7296, -11.8957, -12.4800,  -9.6323,  -8.2835, -15.0920,  -7.7740,\n",
            "         -11.3876, -10.4140, -12.8023, -10.2629,  -9.4933, -10.4954,  -9.4092,\n",
            "          -8.4473,  -7.9863,  -7.8708,  -7.7638,  -9.0145, -11.8792,  -9.0936,\n",
            "         -11.2547,  -8.7297,  -7.4130, -12.4408,  -8.7323,  -7.3447,  -9.8670,\n",
            "         -13.0821, -10.3460,  -9.9440,  -8.4536, -11.4549,  -8.9391,  -8.5820,\n",
            "         -12.2335, -10.7162,  -7.9403, -16.6870,  -9.5819, -12.3954, -14.8291,\n",
            "         -10.0365, -12.0920, -10.3371,  -7.6221, -12.6595, -12.1583, -15.6092,\n",
            "         -10.9309, -10.7129, -10.4384,  -6.8011,  -7.8718,  -9.0385, -11.8072,\n",
            "          -6.6903,  -8.9397, -10.8158, -14.1746, -13.4698,  -7.3377, -12.6527,\n",
            "         -14.0770, -11.4129,  -8.2805, -11.6858,  -9.2978,  -9.4809,  -9.7941,\n",
            "         -13.7514, -10.9476,  -9.8191, -15.2258, -10.7254, -12.5572, -10.1557,\n",
            "          -8.3309, -11.3520,  -8.8947, -12.7102, -10.2293,  -8.2671, -11.7930,\n",
            "         -14.0508, -13.2737,  -8.5196, -12.8441, -15.7159,  -7.1172, -10.5119,\n",
            "         -14.2930,  -9.7097, -11.8778,  -8.8611, -10.5417,  -7.3306,  -8.9083,\n",
            "          -7.7830,  -7.7831,  -9.2659, -14.0158, -11.1092,  -9.3882,  -9.7371,\n",
            "         -10.7920, -10.6236,  -7.9055,  -9.3198, -10.2128,  -7.0772,  -9.5013,\n",
            "          -7.6121,  -9.4422, -13.5087,  -9.3769, -11.2612, -10.5897, -14.6718,\n",
            "          -9.1932,  -8.9260, -10.0788, -12.0119,  -9.2046,  -6.5729, -10.0954,\n",
            "          -8.1834,  -8.3682,  -7.1159, -14.6903,  -7.9150,  -8.0370,  -9.9264,\n",
            "          -9.2285, -10.2413, -11.4555, -12.8833,  -9.5169, -11.4594, -13.7120,\n",
            "          -7.0009, -10.2006,  -9.9005,  -7.9697, -12.3610, -11.1143,  -9.8374,\n",
            "          -7.8620, -13.9696,  -7.0367, -10.0490,  -7.0840,  -9.8282,  -9.8615,\n",
            "          -9.2459, -10.3554,  -9.5897, -10.1162,  -9.7781, -12.9066, -10.5381,\n",
            "          -9.5165,  -9.7707, -10.6232,  -9.2412,  -9.9761, -11.9515, -11.8703,\n",
            "          -8.3095, -11.8441, -11.2726,  -9.8195, -11.7524,  -7.8952, -10.9879,\n",
            "         -12.4025,  -7.9370,  -9.9916,  -8.3887, -11.2736, -11.8941, -11.6034,\n",
            "         -11.1770, -10.9385,  -9.2130,  -9.0842, -10.2347,  -8.9457, -12.0728,\n",
            "         -13.5798,  -9.0282, -11.9080, -12.2384, -14.0266, -11.2589, -12.5591,\n",
            "         -14.1647, -12.7370, -10.4045, -13.5534,  -9.6071, -12.1664, -10.7566,\n",
            "         -11.5303, -10.9175,  -8.8757,  -8.5497,  -7.2586, -13.7985, -11.1904,\n",
            "         -14.2273,  -6.7442,  -9.5982,  -6.2901,  -8.5153, -12.9532, -11.0646]])\n",
            "\n",
            "tgt:\t tensor([93,  3, 24,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t6.762620\n",
            "pred:\t tensor([[-15.0044, -15.4983,  -3.6477,  -0.7176,  -8.3821,  -1.7584,  -7.8254,\n",
            "          -5.6431,  -9.8857,  -3.9974,  -8.1911,   0.2141, -10.1142, -12.6437,\n",
            "          -7.1215, -11.4633,  -9.3123, -11.0381,  -9.2523,  -9.1261, -12.7471,\n",
            "          -9.9230, -13.8838, -10.1871,  -5.5714, -10.6584, -13.2126, -11.3769,\n",
            "          -8.5369, -11.9454, -13.4752,  -8.4475, -10.1585,  -7.0354,  -7.7979,\n",
            "          -6.5260, -12.5868,  -4.9021,  -8.5664, -15.2481,  -6.1336,  -4.5315,\n",
            "          -8.3307,  -8.1170, -13.6217,  -6.9165, -13.5174, -11.0337,  -8.9616,\n",
            "         -11.2410, -13.2373,  -8.8231,  -7.1501, -10.2674,  -8.4434,  -9.5633,\n",
            "          -9.0607,  -9.3028, -11.1684,  -9.1842,  -9.4132,  -8.4570,  -8.6786,\n",
            "         -10.3668, -10.3404,  -8.9969, -13.9315, -12.2550, -10.4454,  -8.7192,\n",
            "          -6.4990,  -9.8045,  -5.4986,  -7.6930,  -6.6212,  -7.4106, -13.5769,\n",
            "         -13.6056, -10.6718,  -7.6607, -11.1961,  -6.8433,  -5.9053, -18.4626,\n",
            "         -10.2069,  -7.5117, -10.0091,  -9.7131,  -8.8498, -10.6783, -10.3321,\n",
            "         -13.5220,  -7.9563, -17.7343,  -9.7057, -10.2220,  -9.5819, -13.1148,\n",
            "          -9.4039,  -9.9761, -13.2224,  -8.6687, -11.5047,  -9.7470,  -7.7730,\n",
            "         -10.1618,  -8.4462,  -3.5595,  -6.3165,  -9.5294, -12.2212, -16.8031,\n",
            "          -8.9787,  -7.6543,  -6.7662,  -7.2246,  -5.9974,  -9.3006, -10.6506,\n",
            "         -10.7732,  -9.8030, -10.6399, -10.1925, -14.9589, -11.2314,  -8.3050,\n",
            "         -14.4452, -11.9144,  -9.6031,  -9.9693,  -8.8170, -15.3223, -11.5951,\n",
            "         -12.0043, -10.9830, -15.4561, -11.9626, -13.2732, -14.4067, -10.5224,\n",
            "         -13.0141, -15.5473, -13.3801, -11.3375,  -9.2000, -19.0241,  -9.5512,\n",
            "          -6.1362,  -7.2923,  -7.4610, -12.4025,  -9.9647, -12.9592,  -9.3994,\n",
            "         -11.0069, -11.2665, -13.3531, -11.0463, -10.2850, -11.5616, -10.5738,\n",
            "         -12.0296, -13.3833,  -9.9588, -12.1250,  -8.9075,  -7.5824, -17.3928,\n",
            "         -14.6359,  -9.9517,  -8.8036,  -3.7790, -12.5376, -16.1797,  -6.4639,\n",
            "         -13.8318, -13.5409,  -7.4605, -11.9503, -13.9725, -13.9123,  -8.8376,\n",
            "          -7.5880, -14.3716,  -7.0706, -12.7123,  -9.2098, -10.2501, -15.8548,\n",
            "         -15.1477, -16.2280,  -8.1805, -13.4180,  -9.8653, -10.0163, -10.6194,\n",
            "         -12.2348, -12.5808, -21.9431, -19.4314, -13.5291, -12.0879, -15.4731,\n",
            "          -8.4863,  -8.0212,  -6.7314, -11.3472,  -8.1496, -12.6775,  -8.9100,\n",
            "         -14.1744, -10.6883, -14.5963, -14.3797, -11.6582, -10.5935, -11.4043,\n",
            "          -8.9397, -10.9287, -10.3756, -14.3389, -15.5366, -10.9961, -14.9608,\n",
            "         -12.9184, -13.0881,  -7.1666, -15.3061,  -9.5292,  -7.6013,  -9.8175,\n",
            "          -8.8362, -11.3024, -15.0834, -11.5329, -11.1584, -12.8712,  -8.6069,\n",
            "         -11.4997, -14.5414,  -8.7880, -16.4448,  -7.0842, -10.9395,  -9.4440,\n",
            "          -8.1744, -12.5918, -11.7413,  -7.4971, -11.2793, -14.9843, -15.9641,\n",
            "          -9.2841, -16.1734,  -5.3548, -10.7018, -10.9625,  -8.4694, -10.9965,\n",
            "          -4.9741,  -9.3335,  -8.8852, -13.6491, -17.4312, -11.4046,  -7.7593,\n",
            "         -14.1796, -15.3537,  -9.4780, -11.5820,  -8.7962,  -7.2458,  -8.0672,\n",
            "         -10.5700,  -8.9980, -12.6844, -11.1145,  -9.6030,  -8.8055, -10.6496,\n",
            "          -8.1550,  -9.6170, -10.0693, -11.2946, -10.4495, -10.5253, -16.3076,\n",
            "         -11.6593, -14.1062, -12.4298, -16.0161, -10.2350,  -6.8745,  -8.1547,\n",
            "         -10.8556, -13.2770, -11.9695,  -9.0548,  -9.1058, -15.6809, -11.2744,\n",
            "         -12.7139, -12.8786, -11.4862, -12.8621, -14.6231, -11.6857, -11.3769,\n",
            "         -12.4175, -10.4281, -10.5579, -10.3502,  -8.9687, -10.8004, -10.2206,\n",
            "          -7.2408, -11.4922, -14.5686,  -6.3278,  -8.4805,  -9.4016, -10.0543,\n",
            "          -8.9515, -12.4393, -13.8798, -18.2473, -14.8791, -10.9515, -12.3044,\n",
            "          -8.9287, -13.9821, -10.7057, -14.3438, -14.0047, -10.9515, -11.1126,\n",
            "         -17.5009, -14.2621, -11.3185, -14.2363, -11.4247, -10.9849,  -6.8640,\n",
            "          -7.2733, -12.8461,  -8.0919,  -8.6903, -11.1407, -16.5376,  -3.1267,\n",
            "         -10.3839, -16.6183, -11.3254,  -8.9868, -10.6498, -15.5899, -11.5826,\n",
            "          -7.5896, -13.8908, -11.0437,  -7.5918, -12.8006, -17.1752,  -8.9384,\n",
            "         -17.5797,  -6.5189,  -8.2535, -13.8281, -10.0270, -11.0159, -14.5662,\n",
            "          -6.6054,  -8.0450,  -7.5949,  -9.6829, -10.9675, -13.8336,  -9.6478,\n",
            "         -12.2751, -14.2440, -16.5053, -11.5107,  -9.6711,  -9.2908, -12.0285,\n",
            "         -12.4836,  -9.1108, -12.1844, -13.8415, -12.2844, -10.5397, -14.9180,\n",
            "         -10.9335,  -8.5200, -10.5620, -13.6070, -15.3119, -13.5829,  -9.2014,\n",
            "         -17.1254, -13.5571,  -9.1602,  -5.7675, -11.2118, -10.2919, -12.8439,\n",
            "         -13.1310, -16.7416,  -7.9801, -10.4059,  -8.7781, -15.2543, -10.7242,\n",
            "         -13.6817,  -8.5812, -12.3717, -10.2393, -13.6780, -10.6031, -15.0395]])\n",
            "\n",
            "tgt:\t tensor([ 48, 164,  35, 227,  37,   3,  11,   2,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t6.815993\n",
            "pred:\t tensor([[-14.4258, -14.6954,  -3.6743,  -2.2996,  -8.7992,   0.8190,  -6.8128,\n",
            "          -4.6788, -11.4113,  -6.8157, -11.6918,  -1.2888, -11.8507, -12.2689,\n",
            "          -6.0745,  -9.9289,  -9.8811, -10.9222, -10.6897,  -8.5171,  -7.5305,\n",
            "          -8.8751, -10.9920, -16.2706,  -3.2011,  -8.7793, -12.0427, -15.5088,\n",
            "         -13.5756, -11.8056,  -9.7121,  -8.3498,  -7.1599,  -8.3747,  -6.9199,\n",
            "          -1.4027,  -8.9920,  -5.5718,  -8.2428, -13.1709,  -7.0345,  -5.1644,\n",
            "          -8.9858,  -6.3290, -13.1384,  -9.6350, -10.5090, -11.1735,  -4.0373,\n",
            "          -7.5236,  -9.7256, -10.7632,  -9.2778, -11.7003,  -8.6764, -11.4841,\n",
            "         -12.2150, -11.3305, -13.8096, -12.2702, -11.9449, -10.4979, -15.3060,\n",
            "         -12.5594,  -8.1267, -10.3446, -11.2276, -12.0568, -12.7153,  -9.7269,\n",
            "          -8.7983,  -8.6584,  -7.4627, -10.8573,  -8.3490,  -9.8089, -11.5438,\n",
            "         -10.9484,  -8.1009,  -7.7322,  -9.5121,  -8.7797, -11.8062, -12.7621,\n",
            "          -8.9531,  -6.7854, -11.1124, -10.0171,  -9.6423, -16.0443,  -8.6261,\n",
            "          -9.1976, -11.0623, -15.9026, -10.9083, -12.8675,  -7.1167, -13.1635,\n",
            "         -11.7297, -10.6612, -12.6491,  -5.6708,  -9.5056,  -7.3859,  -9.6325,\n",
            "          -9.6584,  -8.4868,  -6.1452,  -9.9112,  -8.4706,  -9.0523, -14.2468,\n",
            "          -7.9655,  -8.0943,  -7.4491,  -8.6343,  -7.6534,  -9.5715, -10.1641,\n",
            "         -13.1817, -10.9909, -12.8071,  -7.1020, -10.9381,  -9.2442,  -2.5517,\n",
            "         -11.8530, -11.1822, -11.0883,  -9.9644,  -8.1025, -10.5750,  -5.3226,\n",
            "          -8.4035, -12.7430, -11.9857,  -6.9338,  -9.8246, -11.6269,  -9.7055,\n",
            "         -15.3990, -16.4310, -11.5174,  -9.1061,  -6.1128, -16.1330,  -6.1367,\n",
            "          -4.3646,  -5.0077,  -9.5355, -13.1007,  -9.5986, -12.8918, -11.5234,\n",
            "         -13.6568, -10.9069, -16.0723, -13.6022,  -9.3923,  -9.3053,  -8.3132,\n",
            "         -10.2228, -11.8584,  -9.0044,  -6.3766,  -7.4086, -10.2115, -13.8312,\n",
            "         -13.8127,  -9.7412,  -9.0029,  -6.8966,  -7.5178, -19.5559,  -7.3012,\n",
            "         -17.8284, -15.5715, -11.4761, -11.4061, -12.5933, -17.2865, -13.5932,\n",
            "          -8.3169, -13.4546,  -7.9479,  -8.6386,  -9.9024,  -9.5394, -12.1574,\n",
            "         -10.3992, -15.2854,  -7.8073, -10.8127,  -9.7709, -15.2427,  -8.4151,\n",
            "         -14.4495, -12.9110, -16.2315, -12.7367, -11.5745, -15.7867,  -9.8144,\n",
            "          -9.8871, -10.8120, -14.1411, -13.8248,  -9.7434, -12.4713, -10.7224,\n",
            "         -13.3685, -10.7541, -14.7227, -12.4038, -11.0481, -11.5271,  -9.3993,\n",
            "         -10.8232,  -8.7128, -11.0182, -10.9288,  -9.2795, -11.1553, -13.3487,\n",
            "         -10.4324, -10.6824,  -5.0565, -13.1136,  -9.3035,  -4.0733,  -6.5033,\n",
            "         -16.7218,  -9.1904, -13.3840,  -8.1563, -10.5094, -10.4279,  -8.8622,\n",
            "         -12.7848, -12.2529,  -8.1315, -16.9180,  -6.5258, -10.8260, -11.9459,\n",
            "          -9.3351, -12.0701, -12.8176,  -7.0951, -12.7462, -14.1886, -14.2597,\n",
            "          -9.9856, -11.7529,  -8.4332, -10.5030, -11.6035,  -8.3913, -13.9879,\n",
            "          -7.7506,  -9.0395,  -9.5380, -15.3365, -14.0449,  -8.8860, -13.5868,\n",
            "         -12.9842, -13.0628,  -6.9348, -12.7968,  -9.6912,  -6.9708, -11.6326,\n",
            "         -14.3125, -12.4509, -11.8315, -11.6475, -13.6387, -10.6131,  -9.3849,\n",
            "          -9.3517, -12.6300,  -8.8815, -11.1188, -10.7904,  -9.6693, -12.9556,\n",
            "         -14.0396, -11.0981, -11.3118, -15.0671, -13.9963,  -9.5166, -10.8736,\n",
            "         -16.3821, -11.8110, -12.5296, -11.1591, -13.5764, -10.7266,  -7.1437,\n",
            "          -9.8638,  -9.2162, -11.9833, -12.0373, -14.0144,  -9.7745, -10.8620,\n",
            "         -11.0513, -10.9064, -10.1572,  -7.7725,  -9.0374,  -9.9702, -12.3484,\n",
            "          -9.0834, -10.6453,  -9.6292, -12.0228, -12.0456,  -9.9979, -12.0278,\n",
            "          -7.4887, -13.0073, -13.9101, -12.3406, -10.1185,  -8.1549,  -7.2992,\n",
            "          -9.1782, -10.2213,  -7.2686, -15.3278, -11.5039,  -8.7167,  -9.8086,\n",
            "         -15.8357, -12.6336, -12.5604, -10.4486, -12.4178, -12.7530, -13.6418,\n",
            "          -9.6451,  -9.6410,  -6.0355,  -7.5877, -12.5637, -12.4151, -10.5309,\n",
            "          -8.7194, -16.6429,  -9.4769,  -7.9558, -11.0831, -11.0047,  -8.7829,\n",
            "          -8.0456, -13.9100, -10.1217, -13.6432, -10.6939, -11.6090, -10.9754,\n",
            "          -8.5831, -11.1547, -11.3358, -10.6425,  -7.6063, -11.9553, -13.9173,\n",
            "          -8.8933, -15.2557, -10.9204,  -7.9478, -11.6031, -10.3597, -13.5635,\n",
            "         -12.7275,  -9.3202, -10.4227, -11.5075,  -9.9214,  -9.9491, -10.8099,\n",
            "          -9.8688, -12.5270, -11.5733, -11.6761, -12.1893,  -9.8013, -13.8106,\n",
            "         -14.0031,  -8.7166, -11.5002, -16.1938, -12.4159, -10.4408, -11.1039,\n",
            "         -16.9890, -13.7280, -11.7876, -14.1705, -11.5883, -11.4952, -12.3992,\n",
            "          -8.6643, -12.6970, -12.0843,  -9.2716,  -9.3041, -12.3235, -13.2196,\n",
            "         -15.1255, -10.9050, -11.1407, -10.5190,  -8.3452,  -9.9857, -15.1498]])\n",
            "\n",
            "tgt:\t tensor([79, 53, 41,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t5.586442\n",
            "pred:\t tensor([[-15.7382, -16.2521,  -4.1978,  -1.2934,  -8.1365,  -2.9624,  -8.5076,\n",
            "          -5.2846, -13.2717,  -4.9550, -13.3833,   0.2088, -10.8664, -14.2418,\n",
            "          -7.3058, -13.4390,  -9.8303, -12.9746,  -9.7392,  -9.0264, -11.1073,\n",
            "          -8.7993, -13.3599,  -9.2458,  -5.7007,  -8.7868, -14.2221, -11.1206,\n",
            "          -9.4554, -13.8200, -14.4040, -11.4126,  -7.9834,  -9.8249,  -7.2120,\n",
            "          -6.1444, -12.1698,  -6.0928,  -8.3491, -13.3113,  -8.0023,  -5.8042,\n",
            "          -9.9272,  -6.4959, -16.1125,  -8.7713, -14.1337, -14.0695, -10.9265,\n",
            "         -11.6142, -16.0467,  -7.5627,  -8.5116, -11.8099,  -8.1460, -11.1159,\n",
            "          -9.0839, -12.7866, -11.8878,  -8.3305, -11.8595,  -7.4283,  -9.6308,\n",
            "          -9.5860,  -9.4932,  -8.1516, -15.1896, -16.4507, -11.3633,  -9.4421,\n",
            "          -7.3635,  -8.9766,  -6.4585,  -7.0125,  -6.7429,  -6.7183, -13.0753,\n",
            "         -18.2378, -12.0647,  -8.7960, -12.0239,  -5.8913,  -9.6184, -16.3708,\n",
            "         -12.6383,  -7.9580,  -9.8251,  -9.3280, -10.4579, -11.5476, -10.8492,\n",
            "         -14.3121,  -8.7330, -18.8058, -10.0405,  -9.4465, -10.0336, -12.3888,\n",
            "          -8.3077,  -8.8719, -10.9950,  -7.2203, -13.9326, -11.8586,  -8.7267,\n",
            "         -12.8764,  -8.9925,  -3.7672,  -8.1676, -10.3436, -13.5103, -20.4416,\n",
            "          -8.7895,  -8.4653,  -7.0001,  -9.9125,  -5.5239, -13.1912, -11.6097,\n",
            "         -10.8935, -12.5837, -10.1513, -11.0468, -13.3127, -12.4532,  -9.0050,\n",
            "         -15.8893, -12.2972, -10.6209, -11.2098, -11.1656, -13.4660, -14.4532,\n",
            "         -12.8976, -14.3220, -17.1489, -14.2340, -14.5692, -14.8336, -13.1118,\n",
            "         -13.4142, -15.3906, -11.2421, -12.0082,  -8.2183, -17.6584, -10.8990,\n",
            "          -8.0056,  -9.4943,  -9.9790, -10.7356, -10.3699, -12.9011,  -9.9322,\n",
            "         -11.8006, -11.2566, -13.3754, -10.3733, -11.2367, -11.3768, -12.1438,\n",
            "         -11.4564, -14.0754, -13.6660, -15.1892,  -9.1108, -10.9665, -19.1830,\n",
            "         -14.2567, -10.1411,  -8.7651,  -4.1329, -12.9023, -19.6432,  -8.2337,\n",
            "         -15.4983, -13.0700,  -7.4391, -13.3002, -14.8587, -15.5848, -13.0298,\n",
            "         -10.3593, -12.4742,  -8.8001, -12.6128, -10.3246, -12.1419, -20.0026,\n",
            "         -16.0239, -15.9115,  -6.7715, -11.1344,  -8.8085, -11.5193,  -9.7356,\n",
            "         -13.9935, -12.9774, -19.2909, -21.4719, -13.0567, -14.6547, -16.7726,\n",
            "          -8.9912, -10.1003,  -8.2480,  -9.9446,  -8.0151, -11.9179, -10.5550,\n",
            "         -12.2588, -11.5270, -14.7950, -12.0459, -15.2144,  -9.9243,  -8.9532,\n",
            "          -8.2037, -11.0780,  -9.9181, -12.8532, -13.8236, -15.5947, -15.2354,\n",
            "         -15.2902, -14.3777,  -8.2537, -16.5303,  -9.9884,  -9.4684,  -8.5833,\n",
            "          -9.6363, -12.5248, -12.9264, -12.9752, -10.9547, -10.8950,  -8.3810,\n",
            "         -12.5858, -13.4428, -10.3049, -18.6945, -10.6269,  -9.0098, -11.2790,\n",
            "         -10.5348, -13.1301, -13.1892,  -7.0008, -11.3832, -13.7721, -16.6403,\n",
            "         -14.7781, -17.5160,  -7.3423,  -9.3889, -11.9147, -11.8897, -13.5613,\n",
            "          -6.9815, -11.5775, -10.3991, -12.6079, -15.2791,  -9.6285, -10.8142,\n",
            "         -13.9999, -14.9196, -11.4132, -13.6641, -12.6944,  -3.8746,  -7.5876,\n",
            "         -12.6425,  -6.5035, -14.3376, -10.4885, -13.6578, -12.6682,  -9.0585,\n",
            "         -10.2584,  -9.8237, -10.8124, -13.4791, -10.0805, -13.8114, -18.6194,\n",
            "         -12.2049, -17.8816, -10.0683, -18.5286, -12.4933,  -9.9422, -11.1282,\n",
            "         -14.9176, -13.4135, -15.3064, -12.1352,  -9.3117, -10.2595, -12.3109,\n",
            "         -11.6112, -13.7208, -10.5952, -12.2475, -15.2733,  -9.9177,  -8.9196,\n",
            "         -11.9339,  -9.5191, -10.7542, -10.5037, -10.4053, -11.6529, -12.5638,\n",
            "          -8.2291,  -9.4941, -13.7432,  -5.4768,  -8.2654, -12.0861, -10.5944,\n",
            "         -10.1821, -13.5223, -13.5769, -15.7648, -17.7886,  -9.1639, -12.6584,\n",
            "         -13.1593, -16.1277, -11.2911, -13.5350, -14.4789, -10.5169,  -9.5892,\n",
            "         -16.6277, -16.7111, -13.5485, -13.0703, -12.1924, -11.5963,  -7.6399,\n",
            "         -10.7560, -11.2564,  -4.7111,  -9.3033, -14.9256, -15.1392,  -9.7393,\n",
            "         -11.0469, -17.6329, -10.1449, -12.0774,  -9.8406, -18.6902, -11.5813,\n",
            "          -8.8697, -15.0636, -12.1068, -11.1830, -14.6844, -16.4286, -11.9864,\n",
            "         -16.0803,  -8.8429,  -7.8152, -10.9512, -10.7028, -10.6286, -12.5991,\n",
            "          -7.8842,  -9.1745, -10.5288,  -8.7988, -13.3028, -15.0248, -13.1890,\n",
            "          -9.2959, -13.2533, -16.5431, -10.5271,  -9.5782, -10.3660, -11.2988,\n",
            "         -10.1580, -11.3035, -12.7769, -13.3586, -10.5055, -12.9094, -17.4835,\n",
            "         -11.4696,  -9.6536, -12.4569, -16.1304, -12.1565, -12.9064, -10.3203,\n",
            "         -18.0705, -11.7947,  -9.8529,  -9.2601, -11.7773, -11.0420, -14.5083,\n",
            "         -10.5693, -17.0966,  -9.7407,  -9.8762,  -9.7779, -17.5431, -10.8892,\n",
            "         -12.3799,  -7.8294, -12.7025, -12.6669, -12.6760, -12.3859, -15.9412]])\n",
            "\n",
            "tgt:\t tensor([ 15, 193,  75,   3,  37,   3,  11,   2,   0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t6.422325\n",
            "pred:\t tensor([[-15.1074, -15.3645,  -4.4108,  -1.8254,  -9.8414,  -1.1285,  -7.4073,\n",
            "          -5.2787,  -9.4571,  -5.7280, -11.3023,  -1.5520,  -9.8019, -13.4149,\n",
            "          -5.8174, -11.0752, -10.7086, -12.0626, -12.3155, -10.3179,  -8.2220,\n",
            "          -8.1024, -14.9771, -13.6922,  -3.3506,  -9.7742, -14.7090, -12.8792,\n",
            "         -13.2438, -13.6708,  -9.1446, -10.0793,  -7.8055,  -9.9999,  -7.7786,\n",
            "          -1.9146,  -8.6850,  -6.0098,  -9.0868, -14.1016,  -7.2019,  -6.9918,\n",
            "          -8.2064, -10.3132, -17.5268,  -9.7191, -11.9725, -13.5932,  -5.6995,\n",
            "          -7.5279,  -9.0082, -14.1408,  -9.4712, -13.7631, -10.7370, -13.9586,\n",
            "         -11.9137,  -9.1408, -14.9111, -12.9351, -11.6827, -12.1831, -14.9499,\n",
            "         -11.9248,  -8.9761,  -9.5890, -10.1904, -12.8280, -12.9128, -12.5723,\n",
            "          -9.2002,  -8.1457,  -7.4191, -10.5244,  -7.5658,  -8.9822, -12.6050,\n",
            "         -11.9910,  -7.6552,  -9.2135, -13.4369,  -7.8007, -12.1148, -13.5923,\n",
            "          -9.8220, -10.7826,  -9.4173,  -8.8195, -10.8059, -16.6147,  -8.5273,\n",
            "         -11.7116, -11.7126, -16.2111, -10.6073, -13.5855,  -5.8548, -15.6288,\n",
            "         -11.1037,  -9.0247, -11.7582,  -7.0461, -11.3526, -11.0576, -10.1458,\n",
            "         -13.9552,  -7.2902,  -8.6082,  -9.2146, -10.0429, -11.9778, -13.6712,\n",
            "          -7.8179,  -7.2716,  -8.4259, -10.0548,  -9.4037,  -6.5916, -10.1471,\n",
            "         -14.4788, -13.2717, -11.6858,  -9.2370, -11.6318, -10.8925,  -4.8809,\n",
            "         -10.4384, -11.8144,  -8.1921,  -8.5308,  -6.4115, -10.5431,  -9.3047,\n",
            "         -10.2893, -13.7873, -10.9280,  -7.6393, -11.0819, -12.6968, -10.5900,\n",
            "         -12.8218, -19.5358, -12.5528,  -7.1000,  -8.4583, -16.7040,  -8.1986,\n",
            "          -4.6793,  -6.8173, -10.7692, -13.0885,  -9.9504, -11.0308, -10.2960,\n",
            "         -13.5582, -14.1538, -15.6216, -13.4520, -10.4934,  -8.4466, -10.6191,\n",
            "         -16.0521, -11.0293, -10.1449,  -7.3922,  -9.1054, -13.7278, -15.3517,\n",
            "         -15.5931, -13.1119, -10.0709,  -8.8314,  -8.7275, -18.0916,  -5.5841,\n",
            "         -14.9892, -17.1898, -10.0420, -13.1864, -11.2434, -11.8489, -12.9381,\n",
            "          -9.8430, -14.7119,  -8.8753,  -8.7079, -10.7789,  -9.3653, -10.7328,\n",
            "          -8.9620, -13.2168,  -8.1707, -13.9068, -10.7198, -13.3135, -11.7532,\n",
            "          -9.2497, -12.6656, -14.2133, -16.1307, -11.8457, -15.7243, -11.1163,\n",
            "         -11.8155, -13.6869, -15.0417, -11.0975, -10.1002, -10.9117, -11.7774,\n",
            "         -16.2812, -11.0393, -14.3634, -11.1134, -11.8475, -11.0488, -11.2246,\n",
            "         -10.3179,  -9.4245, -10.2748,  -8.2790, -12.1357, -11.9019, -12.5120,\n",
            "          -8.8445, -12.5547,  -6.9963, -14.5499,  -9.4027,  -7.4874,  -8.0985,\n",
            "         -18.2105, -13.7685, -14.0760,  -7.9467, -11.9512, -11.6401,  -9.8211,\n",
            "         -13.1664, -11.8058, -10.6889, -17.1823, -10.9494, -11.2028, -12.9988,\n",
            "         -12.4559, -12.9793, -11.7120,  -6.5031, -13.7076, -13.1266, -15.4205,\n",
            "          -9.3689, -12.4252, -12.3714, -12.2402, -10.5213, -10.0877, -13.6361,\n",
            "          -6.7280,  -7.9965, -11.8090, -13.8998, -18.4608,  -9.3424, -13.8819,\n",
            "         -14.3454, -12.6739,  -9.3580, -15.8586, -10.7651,  -9.7301, -11.2040,\n",
            "         -15.4762, -13.5442,  -9.6114, -17.3454, -14.0021, -13.2013, -11.0687,\n",
            "         -10.9846,  -9.0324, -10.7755, -11.0538,  -9.6488, -11.5139, -13.1383,\n",
            "         -15.9541, -13.5384, -10.3669, -16.6830, -17.3805,  -8.6169, -11.6073,\n",
            "         -12.5122, -11.0028, -14.0090, -11.1015, -12.9265,  -9.9364,  -8.3698,\n",
            "          -9.4688,  -8.5393, -12.6964, -10.4700, -15.6133,  -8.6146, -14.9461,\n",
            "         -11.0238, -10.8466, -12.1009,  -8.8201, -12.2740,  -7.6818, -16.4060,\n",
            "          -8.4537, -11.1252, -11.9233,  -9.6040, -11.3663, -10.8083, -16.2539,\n",
            "          -7.2552, -11.7684, -12.3406, -13.2650, -11.1909,  -8.6617,  -9.0653,\n",
            "         -10.1635, -11.1029,  -9.4782, -17.2128, -12.9835,  -9.5713, -11.8524,\n",
            "         -14.6764, -15.1482, -13.1023, -12.0264, -16.8305, -12.5065, -14.7389,\n",
            "          -8.7750,  -8.5521,  -8.6464,  -8.3284, -12.4460,  -9.7210, -10.7297,\n",
            "         -10.4584, -17.2798,  -9.1680, -13.4818, -11.0733, -12.4101,  -9.4807,\n",
            "          -7.1604, -14.9059, -11.5855,  -8.3367, -12.9574, -12.4388, -12.3896,\n",
            "         -12.2166, -14.0602, -15.0788,  -9.2585,  -9.1699, -14.4394, -14.0331,\n",
            "          -7.9785, -17.1779, -13.4402,  -8.5312, -12.1790, -12.0882, -13.1936,\n",
            "         -13.9039,  -9.4392, -16.3522, -11.8857,  -9.8571, -10.9657, -14.0244,\n",
            "         -15.6973, -10.2803, -11.5029, -10.8005, -12.2995, -11.1530, -15.0030,\n",
            "         -13.8020, -11.0428, -13.9498, -14.5462, -11.6051, -13.0116, -11.0648,\n",
            "         -14.9104, -13.4998, -13.8674, -15.3951,  -9.2356, -12.0018, -12.4962,\n",
            "         -13.5194, -16.7641, -10.5083, -10.6549, -10.5267, -13.6130, -12.6847,\n",
            "         -16.4244,  -9.8626,  -7.6004, -10.8445, -11.5724, -11.9413, -12.5072]])\n",
            "\n",
            "tgt:\t tensor([  3, 320,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t3.718872\n",
            "pred:\t tensor([[-15.3042, -15.6381,  -4.6925,  -2.8675,  -9.7808,  -1.1741,  -5.9789,\n",
            "          -6.1043, -10.3350,  -6.1958, -11.6761,  -1.2765, -11.6538, -13.0222,\n",
            "          -6.2941, -10.4642,  -8.7926,  -9.9102, -13.1324,  -9.0881,  -6.6055,\n",
            "          -7.3308, -14.1705, -12.9420,  -4.4956, -11.7804, -10.9824, -14.0337,\n",
            "         -14.9437, -13.0122, -11.1148, -12.3616, -10.7263,  -8.4776,  -8.4672,\n",
            "          -3.5009,  -7.8750,  -7.1710,  -7.3942, -12.1987,  -7.4665,  -6.8673,\n",
            "          -7.5603,  -9.6877, -17.2563,  -9.5856, -10.8108, -12.8937,  -5.7576,\n",
            "          -7.0451,  -7.4246, -14.2684,  -9.0724, -15.4044, -11.7493, -11.5981,\n",
            "         -12.3583, -13.4005, -16.2036, -13.5767, -11.1818, -12.9789, -17.9715,\n",
            "         -13.7704,  -7.2720, -11.6762, -14.4389,  -9.8598, -15.9579, -10.9673,\n",
            "          -8.4663,  -8.8346,  -7.0086, -14.3533,  -8.0467,  -8.9508, -13.7912,\n",
            "          -9.7908,  -8.9153,  -9.8784,  -9.3737, -10.4358, -11.7560, -15.3202,\n",
            "         -10.9127,  -8.3989,  -7.7509, -11.0601,  -9.3930, -16.8166, -10.2009,\n",
            "         -10.7646, -12.3562, -16.0636,  -8.5312, -12.8272,  -7.2267, -14.3279,\n",
            "         -13.8681, -12.8084, -12.4558,  -7.4338, -11.6895,  -6.3647,  -8.5297,\n",
            "         -10.3051, -10.9047,  -8.6713,  -9.6378, -11.6287, -10.5411, -14.1210,\n",
            "          -7.1666,  -7.4043,  -7.8007, -11.9981,  -8.7992, -10.9365, -11.1604,\n",
            "         -11.0622, -12.2317, -12.3374,  -9.9090, -14.2243, -11.1643,  -5.0160,\n",
            "          -9.6945, -12.8554, -14.1170,  -7.4738,  -6.5593, -10.8776,  -7.4970,\n",
            "         -13.0323, -10.9255, -14.1454,  -9.5112, -10.8969, -12.5546, -10.6140,\n",
            "         -16.0818, -18.4412, -12.7221,  -9.9965,  -7.2668, -18.6825,  -7.0180,\n",
            "          -4.2618,  -7.2483, -11.3894, -15.2809,  -9.3451, -12.7035,  -9.6958,\n",
            "         -13.1762, -16.7173, -14.9733, -14.0778, -10.3318, -10.8684, -10.7768,\n",
            "         -10.0245, -13.5410,  -8.3719,  -6.5082,  -8.5392, -12.2560, -18.8801,\n",
            "         -16.3366, -11.1112,  -9.8464,  -8.7395,  -8.1429, -17.1545,  -6.6823,\n",
            "         -17.6777, -16.8205, -10.8935, -14.7592, -11.3358, -12.7717, -11.1036,\n",
            "         -10.6029, -13.8389,  -7.4221,  -8.6968, -12.0493, -10.7133, -13.7218,\n",
            "         -11.9241, -15.1854,  -9.4128, -12.8908, -11.6848, -13.5826, -10.7904,\n",
            "         -12.3719, -12.0451, -17.0990, -13.7529, -11.3214, -14.5466, -10.0324,\n",
            "         -12.0135, -14.0114, -14.1091, -12.6677,  -9.0695, -11.4234, -11.2186,\n",
            "         -12.2704, -11.7737, -13.5255, -11.6353, -10.8806, -12.0243, -10.2272,\n",
            "         -10.8138, -11.5855, -11.6401, -10.6906, -12.7430, -10.3258, -11.2621,\n",
            "         -11.3609, -11.5496,  -7.7915, -15.2883,  -9.8779,  -7.6760,  -8.2587,\n",
            "         -14.3719, -10.0321, -14.9633, -11.6921, -10.8834, -15.1792,  -8.4983,\n",
            "         -11.4017, -10.8821,  -8.1229, -15.8332,  -8.4528, -10.5983, -13.9807,\n",
            "         -11.6732, -15.0405, -12.8616,  -9.7276, -13.3511, -15.2496, -15.2507,\n",
            "         -10.8823, -14.2628, -12.7916, -11.8280,  -9.9142,  -9.7642, -15.3919,\n",
            "          -9.1382,  -9.7702, -11.5718, -14.9628, -17.6632,  -9.3164, -12.3320,\n",
            "         -15.1650, -13.9447, -10.0285, -16.6561, -10.7355, -12.3250, -12.9542,\n",
            "         -14.4206, -16.2967, -10.0148, -15.3047, -14.1181, -12.2572, -11.9288,\n",
            "          -8.7362, -11.7600, -11.4160, -11.8277, -11.1691, -15.6413, -14.5063,\n",
            "         -13.9144, -13.7957, -12.3477, -14.0029, -17.0838,  -9.4995, -13.6220,\n",
            "         -10.7610, -13.4402, -13.8562, -11.3490, -11.5135,  -7.2516,  -9.0731,\n",
            "          -8.8304, -10.9226, -10.0296, -12.8728, -12.9152, -10.3316, -11.8161,\n",
            "         -12.7385, -10.5774, -12.5501, -12.3638, -11.7434, -11.0806, -15.0972,\n",
            "         -11.0618, -11.1934, -10.8903, -11.5435, -14.1897, -11.3778, -14.9317,\n",
            "          -7.5971, -15.8848, -12.7800, -19.6991, -13.5234, -12.0421,  -8.8339,\n",
            "         -14.1864,  -8.9188,  -8.3592, -13.8895, -11.9544,  -9.9904, -12.1409,\n",
            "         -14.5289, -14.1107, -13.5526, -10.5453, -14.7762, -12.9474, -14.7147,\n",
            "         -12.2425, -11.1541, -10.8067, -11.3364, -14.7256, -11.8444, -10.9255,\n",
            "          -9.1583, -19.3714, -13.1302, -11.5319, -12.5337, -12.4867, -10.7735,\n",
            "          -9.3257, -14.3266, -10.4082, -14.7842, -10.4982, -13.8207, -10.5677,\n",
            "         -10.8126,  -9.1735, -13.0873, -11.7416, -10.8634, -11.5010, -14.1876,\n",
            "          -8.0079, -12.9089, -13.2971,  -7.9109, -12.6989, -11.2821, -13.9963,\n",
            "         -13.5154, -10.0947, -13.1096, -12.1579,  -9.9283, -11.2859, -11.5907,\n",
            "         -12.0966, -11.9671, -12.6148,  -8.7457, -13.6778,  -9.1778, -15.3836,\n",
            "         -16.7711, -10.0811, -12.5990, -14.1287, -15.0455, -12.6712, -10.3664,\n",
            "         -14.7420, -14.1542, -10.2427, -14.8654, -11.0998, -12.3748, -12.1360,\n",
            "         -11.9066, -12.8636, -10.2966, -10.4185, -10.1394, -13.0577, -12.1808,\n",
            "         -14.8192, -10.8333, -10.9541,  -9.0962, -13.5612, -10.6222, -14.7305]])\n",
            "\n",
            "tgt:\t tensor([  3,  72, 403,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t5.028505\n",
            "pred:\t tensor([[-15.6978, -16.0054,  -4.8967,  -1.6906,  -9.6102,  -0.5946,  -7.6846,\n",
            "          -4.9274, -11.9756,  -7.4649,  -9.2133,  -1.6214, -10.6920, -11.0845,\n",
            "          -6.8971, -11.0706, -10.3507, -11.4413, -11.6767, -10.4532,  -5.4636,\n",
            "         -10.6779, -14.0295, -16.6343,  -3.7555, -10.0300, -14.8942, -13.5505,\n",
            "         -15.5270, -13.6145, -11.0509, -11.3916,  -9.9902,  -9.8700,  -9.3599,\n",
            "          -1.3942,  -8.8205,  -6.8957,  -8.1840, -14.9126,  -8.5148,  -9.0951,\n",
            "          -7.8612,  -9.6579, -16.4823, -10.8074, -12.0556, -14.8846,  -6.0986,\n",
            "          -7.6995, -12.5708, -12.3666, -10.7328, -13.7807, -10.4902, -14.8472,\n",
            "         -12.5552, -12.8915, -12.1586, -11.3932, -11.7971, -13.0398, -15.2315,\n",
            "         -13.0991,  -7.8822, -11.5607, -10.2663, -13.3374, -13.0865, -12.7638,\n",
            "          -8.4092,  -7.4923,  -7.7743, -11.7438,  -9.8334,  -8.8478, -13.7436,\n",
            "          -8.9145,  -9.2274,  -9.9188,  -9.3491, -10.9836, -13.0405, -16.3072,\n",
            "          -8.3398,  -8.8700, -11.1653, -10.9218, -10.4246, -17.8276,  -9.9089,\n",
            "         -10.4974, -13.4970, -17.6349, -11.1834, -16.6744,  -6.5848, -15.7133,\n",
            "         -12.5216,  -9.1020, -14.2898,  -7.1522, -10.4007,  -8.2362, -10.5885,\n",
            "          -9.6997,  -8.8407,  -8.5280,  -9.8627, -12.1898, -12.5506, -13.8886,\n",
            "          -9.5384,  -8.3267,  -9.0286,  -9.7996, -10.4265,  -9.3326, -11.4356,\n",
            "         -13.8873, -13.6702, -15.6828,  -9.6080, -11.8578, -11.1246,  -7.4086,\n",
            "         -11.1847, -11.3375, -11.6739,  -7.1243,  -7.9436, -14.6732,  -7.5654,\n",
            "         -12.8065, -13.2845, -13.4219,  -9.4512, -11.8243, -13.9284,  -9.5387,\n",
            "         -16.2409, -17.1948, -12.3544, -10.5384,  -8.0520, -17.5379,  -8.7689,\n",
            "          -5.7521,  -6.8896, -11.4207, -14.8195,  -9.4883, -10.7292, -13.3570,\n",
            "         -12.4735, -15.4359, -15.0845, -12.6906, -11.8201,  -9.8059, -11.2124,\n",
            "         -13.0257, -10.7620, -10.6798,  -7.9925,  -8.2238, -11.2625, -15.6682,\n",
            "         -15.4437, -12.5090, -13.9563,  -9.0061,  -9.8202, -20.8916,  -5.6137,\n",
            "         -17.2750, -17.1257, -10.2089, -13.5130, -13.3824, -16.4434, -14.7786,\n",
            "         -10.6266, -14.1017, -10.2343, -10.9408,  -9.5555,  -9.2159, -12.2894,\n",
            "         -10.1316, -14.6923, -10.0920, -11.9172, -12.0289, -17.4093, -12.6585,\n",
            "         -12.1983, -12.5169, -16.8771, -13.5287, -12.0281, -15.6225, -11.6984,\n",
            "         -13.0495, -14.2887, -14.9838, -13.7519, -10.0511, -13.3452, -12.8877,\n",
            "         -13.2376, -12.8797, -14.5610, -12.7223, -14.1800, -13.8033, -11.2103,\n",
            "          -8.1599, -12.9940,  -9.8835,  -8.9880, -11.5699, -11.4641, -10.7059,\n",
            "         -11.0744, -10.8430,  -8.1432, -15.0251, -13.3245,  -8.9989,  -8.2784,\n",
            "         -17.4166, -11.1338, -13.1184,  -9.8404, -13.8057, -12.6443,  -9.1185,\n",
            "         -14.3995, -10.5365, -11.2476, -19.3173,  -9.2227, -12.5525, -16.0882,\n",
            "         -15.0612, -12.8249, -15.4448,  -9.6375, -14.8897, -14.5773, -13.4069,\n",
            "         -10.5884, -14.5006, -14.3656, -12.6987, -11.8578, -10.0644, -12.7712,\n",
            "          -7.9519, -10.6931,  -9.5441, -13.1064, -19.3290,  -9.7870, -15.4817,\n",
            "         -14.3403, -14.5636, -11.0792, -17.7544, -13.2431, -12.6815, -11.0163,\n",
            "         -13.4149, -13.7660, -11.0907, -14.8210, -14.5454, -10.9861, -13.2125,\n",
            "          -8.8079, -12.3028, -12.4545, -12.3786, -12.1347, -11.6527, -13.6729,\n",
            "         -14.1471, -13.4893,  -9.7212, -14.7148, -16.9831,  -8.7730, -12.4007,\n",
            "         -13.4116, -11.5151, -14.5278, -10.9558, -11.8531,  -9.5550,  -9.9039,\n",
            "          -9.8438, -10.5510, -10.7244, -14.3214, -14.0822,  -9.4866, -11.7024,\n",
            "         -11.0179, -12.0385, -12.4256,  -9.4022, -11.2093, -11.8647, -16.0517,\n",
            "         -11.4526, -12.0453, -11.4337, -11.5820, -13.2247, -12.7043, -15.7762,\n",
            "          -7.6926, -12.5169, -17.0763, -15.2155, -12.3423,  -8.4859, -13.6773,\n",
            "         -13.1653, -10.8422,  -9.6302, -17.6674,  -9.6281, -11.9863, -14.8119,\n",
            "         -14.1753, -13.9636, -14.9763, -10.6873, -14.0662, -13.3628, -16.7068,\n",
            "          -8.6481,  -9.0391,  -8.7986,  -7.7074, -13.9702, -11.1041,  -8.6779,\n",
            "         -12.6877, -18.7542, -11.2995, -14.1792, -12.6040, -13.9762, -10.5155,\n",
            "          -9.8710, -15.9953, -11.2842, -13.3990, -13.1782, -14.1623, -14.1948,\n",
            "         -10.5291, -13.5126, -15.7480, -12.5396, -11.7833, -14.8719, -14.9630,\n",
            "          -7.2825, -20.4639, -12.5378,  -9.0890, -14.1039,  -9.8518, -15.1335,\n",
            "         -12.8419, -11.2053, -13.8528, -13.3712, -12.7488,  -9.2223, -13.7689,\n",
            "         -14.8636, -13.9596, -11.3348,  -8.1862, -14.8884, -11.2852, -15.8175,\n",
            "         -16.4406, -10.7390, -15.7140, -16.7528, -15.3574, -12.9738, -11.1984,\n",
            "         -14.0405, -14.9886, -13.9682, -13.3306, -10.0595, -14.9580, -13.0072,\n",
            "         -11.1616, -17.4813, -11.5108, -14.4365,  -9.0109, -14.8981, -11.4408,\n",
            "         -16.7203,  -9.5700, -11.3066,  -9.6607, -15.3935, -12.1835, -14.3247]])\n",
            "\n",
            "tgt:\t tensor([273,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t4.814097\n",
            "pred:\t tensor([[-15.9711, -16.1192,  -5.0745,  -1.5489, -11.2897,  -0.0333,  -8.6769,\n",
            "          -4.4181, -11.0762,  -7.6681, -12.1873,  -0.8821, -12.7566, -12.4746,\n",
            "          -8.6726, -14.3401, -11.4457, -11.9899, -12.3891,  -8.5465,  -7.6214,\n",
            "          -8.9088, -16.5249, -16.1739,  -4.8989, -11.0823, -13.6387, -14.2343,\n",
            "         -16.5226, -14.7077, -13.6894, -11.4037, -11.1938,  -8.2166,  -9.6121,\n",
            "          -3.9757, -10.5665,  -7.3783,  -7.8777, -15.3840,  -8.9701,  -7.5034,\n",
            "         -10.9673, -11.5208, -14.4406, -11.5429, -12.0415, -11.6528,  -5.4277,\n",
            "          -8.8155, -10.4594, -12.1837, -12.3510, -15.8101, -10.6699, -14.8545,\n",
            "         -10.2887, -11.3627, -11.0107, -14.7343, -12.8783, -11.6763, -16.7339,\n",
            "          -9.4224,  -7.3708, -12.3015,  -9.8137, -13.2494, -12.4391, -12.3895,\n",
            "          -8.2786,  -8.5699,  -9.2663, -11.4843, -10.1622, -12.6243, -15.1689,\n",
            "         -10.5640,  -8.3564, -10.1161, -10.1254, -10.5052, -15.3188, -13.6184,\n",
            "         -11.1787,  -9.7252, -11.0260, -12.0685, -10.1403, -17.8214, -10.7831,\n",
            "         -13.6874, -13.1262, -19.4889,  -9.6250, -13.8539,  -8.1860, -17.3250,\n",
            "         -10.6834, -12.9258, -13.7169,  -7.3781, -11.1947,  -8.6928, -10.7520,\n",
            "         -11.8862, -12.0565,  -7.9931, -11.7690, -11.6430,  -7.7339, -15.6375,\n",
            "          -9.2051,  -9.4017, -11.5077, -12.0157, -11.6293, -11.1761, -10.2104,\n",
            "         -12.1683,  -9.8030, -12.7754, -12.8108, -11.8222, -10.3079,  -7.0591,\n",
            "         -11.7055, -12.8967, -13.0422,  -7.4693, -11.1077, -14.8678,  -8.9106,\n",
            "         -12.1190, -14.3127, -13.1607,  -7.3473, -14.0013, -14.7827, -11.1268,\n",
            "         -14.8054, -16.3780, -13.0840,  -5.8960, -11.1351, -19.5396,  -6.3042,\n",
            "          -5.6885,  -5.3005, -11.3629, -13.6719, -11.7670, -11.3843,  -9.8437,\n",
            "         -19.2621, -13.1552, -14.0982, -12.2564,  -7.1130,  -9.0314, -11.0885,\n",
            "         -12.1724, -11.8873, -11.7520, -10.3769,  -8.2815, -11.8219, -17.9240,\n",
            "         -19.3847, -10.3758, -12.7847,  -8.2364,  -9.8328, -23.5585,  -9.2345,\n",
            "         -16.8032, -19.2707, -10.6607, -12.6566, -12.6794, -17.5396, -16.0287,\n",
            "         -10.6951, -15.1695, -13.3969, -11.4665, -10.1384, -13.3696, -12.5267,\n",
            "          -9.8512, -16.1840, -10.1607, -12.7432, -11.5150, -13.0534, -12.0923,\n",
            "         -12.3652, -14.0604, -14.7690, -16.4003, -14.0448, -14.6899, -11.3795,\n",
            "         -14.5755, -16.2772, -14.3038, -13.3532, -12.7670, -10.2632, -12.9326,\n",
            "         -14.5504, -12.6811, -15.4060, -14.1440, -13.1682, -16.1792, -11.7093,\n",
            "          -9.4874, -14.6102, -11.0431, -10.2964, -12.4588, -10.6700, -13.0722,\n",
            "          -9.0626,  -9.9074,  -8.1946, -16.3415,  -9.3103,  -8.4254, -12.7924,\n",
            "         -16.5292, -14.5590, -13.2511,  -7.1759, -11.4974, -15.3605, -10.2992,\n",
            "         -13.0586, -11.0759,  -9.2999, -18.8696, -10.3740, -13.2123, -13.9160,\n",
            "         -12.0524, -13.3114, -16.9084,  -7.1996, -15.5230, -14.4569, -14.9713,\n",
            "          -9.6989, -11.8403,  -9.4479, -11.9646, -13.3132,  -9.3609, -13.2470,\n",
            "         -10.2122, -10.2389,  -9.3248, -13.3140, -17.5520,  -8.2680, -15.5555,\n",
            "         -14.5120, -13.9604,  -9.8912, -20.2555, -13.2837, -11.7435, -10.7090,\n",
            "         -18.1294, -14.0102, -10.2321, -19.0311, -16.1897, -10.1590, -11.2482,\n",
            "          -8.9253, -11.9915, -11.1042, -11.8166, -11.7700, -13.2386, -14.7045,\n",
            "         -13.9866, -15.9069,  -9.9206, -15.2330, -15.9725, -11.0180, -11.6795,\n",
            "         -13.4931, -11.6413, -15.9494, -10.3457, -16.6857, -10.7902, -10.6539,\n",
            "         -10.0212, -10.9035, -11.5844, -13.4925, -14.0735,  -8.4865, -10.6494,\n",
            "         -10.0674, -12.8997,  -9.9016, -11.5166, -10.9516, -11.7909, -16.9326,\n",
            "         -11.5926, -11.9259, -11.3064, -11.2513, -13.5099, -11.8620, -15.8986,\n",
            "          -8.1679, -17.9928, -13.1726, -13.7307, -11.8725,  -9.5087, -10.7243,\n",
            "          -9.5964, -10.7578, -11.8055, -16.8621,  -9.8388, -11.7712, -12.3508,\n",
            "         -17.1990, -14.0235, -13.4159, -13.1082, -14.1064, -16.0616, -16.9700,\n",
            "         -10.4256, -10.2000, -10.4869, -11.6813, -15.5328,  -8.7115, -10.1956,\n",
            "         -11.8532, -18.3661, -10.7696, -11.1601, -13.9097, -13.4373, -10.9897,\n",
            "          -8.0334, -17.0373, -13.7018, -11.0846, -11.9267, -15.8521, -12.8419,\n",
            "          -9.2779, -13.8639, -15.4973, -12.6555,  -9.2338, -12.5477, -15.0862,\n",
            "          -7.9466, -11.9397, -12.6073,  -8.4579, -13.3384,  -9.9921, -14.8605,\n",
            "         -14.3330, -11.3142, -14.6810, -13.5654,  -9.7677, -10.0920, -13.8714,\n",
            "         -14.5179, -12.2180, -12.7719, -10.5365, -14.2295, -12.5225, -14.7171,\n",
            "         -16.0923, -11.4810, -13.2515, -13.6471, -17.1914, -13.6055, -10.4735,\n",
            "         -17.2094, -13.0175, -11.4480, -14.4247, -12.1743,  -9.7509,  -9.5746,\n",
            "         -10.8820, -17.6038, -11.9232,  -9.1371, -10.9346, -12.4078, -11.2773,\n",
            "         -14.8901,  -7.3184, -10.6784, -11.1602, -10.0882, -13.3045, -13.9362]])\n",
            "\n",
            "tgt:\t tensor([ 38, 357,   3,   3,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t4.613289\n",
            "pred:\t tensor([[-16.1778, -16.4470,  -5.2001,  -3.3621, -11.6378,  -2.9628,  -6.4843,\n",
            "          -3.9115, -12.5562,  -7.3653, -11.8799,  -1.1171, -12.5301, -13.6860,\n",
            "          -6.7066, -10.7985, -10.7625, -12.5279, -12.1717, -10.3832,  -8.7220,\n",
            "          -8.7817, -15.0403, -17.1398,  -4.4799, -11.3626, -17.8321, -17.1521,\n",
            "         -16.0028, -14.9212,  -8.2293, -10.8951, -10.2642,  -7.6553,  -9.7971,\n",
            "          -1.9457,  -7.8281,  -6.3093,  -8.4353, -15.5304,  -8.2783,  -7.6578,\n",
            "          -9.3054, -11.3987, -16.4153, -12.3153, -12.8866, -11.4365,  -5.6136,\n",
            "          -8.0315, -10.5004, -14.0372, -10.1585, -14.4513, -10.9118, -16.1835,\n",
            "         -12.4314, -11.9375, -13.2313, -12.3163, -12.3626, -12.9639, -15.7999,\n",
            "         -11.4660,  -7.1784, -10.9240, -10.6528, -14.1606, -14.5338, -12.4530,\n",
            "          -9.7077, -10.3193,  -7.9546, -10.6296,  -9.1394, -11.6347, -16.9339,\n",
            "         -10.3845,  -9.6611,  -9.6251, -10.9698, -12.5317, -11.0786, -15.9255,\n",
            "         -11.4004,  -9.3521, -12.2862, -10.7345, -11.1597, -17.7469, -10.1915,\n",
            "         -13.1965, -13.1510, -16.7634, -10.7278, -16.7308,  -7.3837, -18.3725,\n",
            "         -14.2150, -12.0346, -14.6555,  -7.6470, -12.8918,  -9.3720, -10.0584,\n",
            "         -12.9835, -11.7044,  -9.0805, -12.1001, -12.0008, -10.5648, -19.3262,\n",
            "          -7.5693,  -8.1737,  -8.2728, -12.6796, -11.0380,  -8.6959, -12.5850,\n",
            "         -16.1484, -13.1808, -13.0943,  -9.5175, -15.0113, -10.7206,  -8.2764,\n",
            "          -8.1817, -13.9052, -13.6717,  -7.5537,  -5.4586, -14.0802,  -8.7513,\n",
            "         -12.5781, -13.2709, -13.6166,  -8.7179, -12.8507, -11.4923, -11.6724,\n",
            "         -16.5363, -22.7866, -12.5179,  -8.9603,  -8.0419, -19.6048,  -8.4958,\n",
            "          -7.2379,  -7.6102, -10.9554, -12.6430, -11.2009, -13.1254, -14.0315,\n",
            "         -16.6550, -16.2708, -14.6160, -14.5593,  -9.8044, -10.9402,  -8.8162,\n",
            "         -11.3771, -16.2010,  -9.0544,  -7.2132,  -7.7439, -17.0337, -16.9693,\n",
            "         -18.1614, -10.9849, -12.3542, -10.1163,  -6.1631, -24.3610,  -8.3713,\n",
            "         -16.8552, -16.6867, -10.8986, -15.9977, -12.1361, -15.9555, -14.2290,\n",
            "         -11.7759, -15.3646,  -8.6838, -10.0780, -13.8533, -13.8957, -12.5273,\n",
            "          -9.7997, -16.0586, -10.8905, -16.9349, -11.6617, -15.6624, -14.6512,\n",
            "         -12.2778, -12.9596, -17.2661, -16.3512, -14.2595, -18.0068, -10.9753,\n",
            "         -14.7734, -13.4585, -15.2351, -12.5945, -10.5782, -13.9762, -11.1505,\n",
            "         -14.2053, -15.0360, -15.6306, -13.7272, -14.8360, -13.5290, -11.3307,\n",
            "          -9.8482, -10.6365, -10.1726, -11.8633, -12.9627, -10.5752, -14.0387,\n",
            "         -12.4326, -13.0947,  -7.7529, -19.2760, -12.1128,  -6.9037, -10.1817,\n",
            "         -18.2599,  -9.4171, -15.5447,  -8.4123,  -9.7316, -12.3753,  -9.7474,\n",
            "         -11.1813, -11.8078,  -9.5408, -18.1512, -10.8841, -11.9766, -19.0169,\n",
            "         -12.0705, -13.1710, -15.8592, -11.1642, -15.8242, -17.0214, -15.5213,\n",
            "         -11.5896, -11.8071, -14.4504, -16.8214, -10.1160, -10.3741, -14.2545,\n",
            "          -8.5714,  -9.0521, -11.6948, -16.8281, -19.6971, -11.4369, -14.8851,\n",
            "         -16.7207, -17.2411, -10.3361, -18.6763, -11.4047, -13.5635, -13.3333,\n",
            "         -18.2043, -13.9566, -11.6102, -17.3266, -16.6717, -14.5655, -13.0886,\n",
            "         -10.4473, -12.2989, -11.8016, -13.8376, -12.7982, -10.8654, -14.0265,\n",
            "         -16.9907, -12.3659, -12.5111, -17.7668, -18.8360,  -9.3471, -13.0253,\n",
            "         -12.9904, -13.1409, -13.8061, -11.8986, -16.2828, -10.6520,  -9.8072,\n",
            "          -9.6149, -12.5725,  -9.0764, -10.9802, -15.3191, -10.7385, -13.7890,\n",
            "         -11.5052, -13.3003, -13.4471,  -8.7057,  -9.7604, -11.8941, -17.2373,\n",
            "          -8.7134, -11.4107, -14.0326, -14.4836, -14.9004, -11.1691, -15.4493,\n",
            "          -8.8245, -15.2646, -12.4637, -12.7919, -11.5843,  -9.6686, -12.2313,\n",
            "         -10.7007, -10.5252,  -8.9530, -13.5943, -11.4817, -12.5155, -15.3187,\n",
            "         -15.5792, -14.3305, -15.4776, -12.8144, -14.2420, -14.2275, -15.1803,\n",
            "         -10.8217, -11.5310, -10.6850, -11.3160, -16.1970, -12.2350, -11.9976,\n",
            "         -12.0740, -18.9501, -11.9352, -12.8263, -13.6410, -12.4996, -10.6869,\n",
            "         -11.1478, -14.8023, -13.1962, -11.5762, -10.8085, -14.6425, -13.4694,\n",
            "         -14.5354, -12.2023, -14.5702, -12.9428,  -8.3878, -14.6584, -18.7963,\n",
            "          -9.3755, -14.5457,  -9.9236,  -9.1755, -13.1501, -10.2974, -16.6797,\n",
            "         -15.1524, -11.7625, -14.5863, -14.4166, -14.2686,  -9.4657, -11.3853,\n",
            "         -17.7301, -12.2150, -14.3069, -11.6807, -12.6189, -10.4433, -15.4816,\n",
            "         -15.5648, -11.4393, -15.0666, -18.0129, -15.4700, -11.3537, -11.5713,\n",
            "         -15.4563, -13.3313, -12.2001, -14.0788, -12.7573, -12.9496, -12.0877,\n",
            "         -15.4165, -17.9985, -12.4503, -11.3756, -10.9599, -11.8786, -14.0817,\n",
            "         -17.7110,  -9.3802, -11.1754, -12.3027, -12.8358, -12.4078, -15.2766]])\n",
            "\n",
            "tgt:\t tensor([ 36, 135,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t4.679846\n",
            "pred:\t tensor([[-16.5469, -16.8481,  -6.1231,  -2.9279, -12.0637,  -3.0581,  -8.3539,\n",
            "          -6.1854, -11.5141,  -8.6269, -11.6071,  -1.8878, -13.1615, -13.2340,\n",
            "          -7.5161, -11.9871, -10.3815, -12.1618, -16.1621, -11.6728,  -6.0754,\n",
            "         -12.0430, -14.8631, -15.8091,  -4.6240, -12.5959, -15.7810, -14.6574,\n",
            "         -17.0904, -15.7274, -14.0521, -13.1801, -11.5970, -10.5699,  -9.0018,\n",
            "          -2.5356,  -8.3054,  -7.6929,  -7.2767, -13.2075,  -8.7569,  -7.7535,\n",
            "          -7.8356, -10.4979, -14.7209,  -9.0005, -12.6348, -14.6477,  -6.1308,\n",
            "          -9.1213, -10.6121, -14.7292, -12.1423, -16.0911, -11.0634, -13.5242,\n",
            "         -13.0345, -12.5179, -15.3001, -14.0075, -13.5078, -14.9111, -17.4851,\n",
            "         -14.6546,  -6.7303, -10.5515, -10.6239, -15.1616, -14.0446, -13.6530,\n",
            "          -8.7873, -10.5064,  -7.9555, -13.7786,  -9.0151,  -9.7758, -14.6835,\n",
            "         -12.2377, -10.0377,  -9.9320, -10.0354, -10.6922, -14.0009, -17.2233,\n",
            "         -12.1400,  -9.7284, -12.5739, -12.2013, -10.0544, -17.7381, -11.3436,\n",
            "         -12.1055, -14.6341, -17.8341, -12.3085, -16.1247,  -8.5822, -18.5371,\n",
            "         -11.8231, -10.0465, -15.5467,  -6.6784, -12.9761,  -8.9724, -11.7339,\n",
            "         -11.8204, -10.9910,  -9.3393, -11.6553, -13.3368, -11.9021, -15.1132,\n",
            "          -7.4804,  -7.9550,  -9.8870, -12.3099, -12.1675, -12.3830, -11.0872,\n",
            "         -14.5742, -14.0305, -15.9385, -11.6466, -16.6739, -11.8309,  -7.4466,\n",
            "         -11.0414, -13.0889, -14.8030,  -8.1107,  -8.8561, -13.5046,  -8.5469,\n",
            "         -10.0755, -14.5655, -14.2991,  -8.2358, -11.4815, -16.9582, -12.8919,\n",
            "         -17.6913, -18.3396, -12.6275,  -8.7960, -10.1876, -20.0825,  -7.8855,\n",
            "          -4.7312,  -8.6829, -10.4620, -14.3127, -11.1031, -13.0210, -11.7528,\n",
            "         -18.1680, -14.8004, -14.7551, -15.4397,  -9.5146, -11.2920, -11.1746,\n",
            "         -14.4641, -13.3722, -11.6780,  -8.1971,  -8.0266, -11.4799, -17.9273,\n",
            "         -19.2304, -11.9766, -13.3326,  -8.9632,  -9.0870, -22.3982,  -7.5760,\n",
            "         -16.2983, -16.8844, -11.1795, -16.4854, -14.1299, -16.3020, -14.8526,\n",
            "         -13.1979, -13.0261,  -9.7122, -11.9003, -11.0083, -10.5166, -12.2329,\n",
            "         -11.1879, -14.3916, -11.9546, -14.5602, -12.9185, -15.5273, -12.6580,\n",
            "         -11.6944, -12.6104, -16.3549, -15.2695, -14.8155, -16.9479, -13.3652,\n",
            "         -15.4437, -14.3356, -18.7726, -12.1820, -11.3470, -14.0928, -10.9709,\n",
            "         -15.8095, -13.5644, -16.5680, -14.6841, -12.9630, -12.6148, -10.5258,\n",
            "          -9.3661, -12.6715, -11.3571, -11.2754, -12.4274, -13.4970, -15.4678,\n",
            "         -13.8305, -13.5817,  -7.8136, -17.4176, -11.6979, -10.2847, -12.5090,\n",
            "         -17.5899, -10.7934, -15.2434,  -8.7120, -10.6333, -12.4678,  -9.5402,\n",
            "         -13.2915, -11.4091,  -9.5167, -20.7366, -13.0417, -12.2572, -16.7027,\n",
            "         -10.5611, -12.7907, -14.2486, -10.2387, -15.4104, -16.3211, -17.6582,\n",
            "         -13.4096, -12.8483, -13.9622, -11.9575, -12.3567, -11.0934, -15.4296,\n",
            "          -9.1681,  -9.2041, -12.7494, -17.3735, -21.7009, -10.5691, -14.0006,\n",
            "         -15.8995, -15.0706, -11.7115, -20.5152, -11.9833, -12.3268, -10.3814,\n",
            "         -16.8068, -14.9617, -11.4477, -15.9112, -17.3496, -15.2195, -13.2980,\n",
            "         -10.1601, -11.5775, -11.8484, -12.6170, -12.4773, -16.4775, -17.7983,\n",
            "         -16.3256, -14.4548,  -9.9844, -15.0232, -17.5835, -10.0519, -15.2025,\n",
            "         -14.0976, -12.1246, -14.8278, -13.7123, -18.6368, -10.9782, -10.8523,\n",
            "          -9.8444, -13.7454, -11.6591, -13.4418, -13.4574, -11.5802, -14.0222,\n",
            "          -9.9516, -12.7572, -13.8455, -12.0239,  -9.3701, -12.2604, -14.6683,\n",
            "         -10.5118, -13.3517, -11.4312, -12.4687, -15.8924, -13.5015, -18.5409,\n",
            "          -9.2290, -15.4754, -15.5683, -13.7407, -13.8905, -11.5793, -15.1907,\n",
            "         -14.3788, -12.4217,  -9.8090, -16.2664, -14.2041, -10.2914, -12.5304,\n",
            "         -17.3682, -13.9008, -14.2790, -15.4085, -16.2236, -14.1944, -17.8005,\n",
            "         -11.4208, -11.7226, -10.4256, -12.2655, -15.3641, -14.8242, -10.8211,\n",
            "         -10.9773, -19.2564, -11.5726, -11.4016, -12.6624, -15.3858, -10.8729,\n",
            "         -10.2019, -17.1776, -11.3382, -13.4802, -14.4738, -15.7118, -13.5963,\n",
            "         -12.5575, -14.5917, -12.9801,  -9.3651,  -8.5867, -12.7913, -16.8884,\n",
            "         -11.0725, -14.5468, -13.2695,  -7.7697, -12.4160, -12.0416, -12.8909,\n",
            "         -18.5210, -11.7546, -16.0720, -13.0923, -11.2130, -10.3260, -11.4281,\n",
            "         -17.1753, -13.1080, -13.0397, -10.7706, -12.5257, -12.0116, -16.8888,\n",
            "         -15.5573, -11.9308, -14.3999, -16.6830, -16.5285, -13.7354, -14.3206,\n",
            "         -15.9107, -15.9420, -13.8128, -16.2793, -12.3739, -14.8328, -15.5601,\n",
            "         -13.0921, -16.1356, -12.1072, -12.9089, -11.5001, -13.4562, -13.6061,\n",
            "         -18.7573, -13.2613, -13.3425, -12.5041, -12.7153, -11.5794, -14.6876]])\n",
            "\n",
            "tgt:\t tensor([38, 47, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t4.000628\n",
            "pred:\t tensor([[-16.5014, -16.6370,  -5.4822,  -1.7990, -10.5808,  -1.8442,  -8.7583,\n",
            "          -6.6238, -12.1037,  -6.9652, -11.4087,  -0.8102, -14.1146, -12.5378,\n",
            "          -7.5052, -11.6474, -11.7297, -12.2266, -14.1763,  -9.5930,  -7.2769,\n",
            "          -9.4863, -14.3770, -20.2984,  -6.6834,  -7.9643, -16.2210, -17.4708,\n",
            "         -15.4388, -16.5823, -12.5175, -13.1064,  -8.0903, -10.2428, -11.1462,\n",
            "          -3.4862, -11.3070,  -7.3368,  -9.9676, -14.8153, -10.1735,  -8.1059,\n",
            "         -10.1347, -10.1251, -14.5717, -11.9235, -14.1296, -12.7896,  -6.5713,\n",
            "          -8.6515, -12.0452, -13.1726, -11.8402, -17.6248, -11.0688, -15.1369,\n",
            "         -14.8133, -12.2078, -12.9447, -11.6861, -12.8448, -10.9541, -17.9556,\n",
            "         -12.2631,  -8.6362, -11.3364, -15.1054, -14.3325, -17.1324, -13.4891,\n",
            "          -9.3261,  -9.3157, -10.2411, -11.4072, -11.7363, -12.8859, -19.1755,\n",
            "         -11.7477,  -9.8195, -11.1852, -12.0570, -10.5156, -15.8267, -17.3831,\n",
            "         -10.9881, -10.1560, -12.8494,  -8.5766,  -9.9977, -18.8168,  -8.9715,\n",
            "         -13.3966, -14.1382, -18.3837, -10.7904, -15.0580,  -7.3369, -19.3774,\n",
            "         -11.6627, -12.4683, -13.6659,  -7.8250, -14.0808,  -9.0373, -11.8441,\n",
            "         -10.7300, -11.6984,  -8.9891, -10.9718, -14.0327, -12.7885, -16.4584,\n",
            "          -9.3482, -10.5259,  -9.4394, -13.8610,  -9.7640,  -9.7293, -11.2866,\n",
            "         -15.6599, -13.2559, -14.0877, -11.8285, -13.5074,  -8.6158,  -7.4969,\n",
            "         -12.9300, -13.7565, -14.0543,  -8.4674,  -8.6356, -14.2267,  -8.2877,\n",
            "         -13.1104, -16.0846, -12.5631, -10.6042, -13.2731, -17.8890, -13.4806,\n",
            "         -14.1181, -18.9009, -14.3896,  -5.9949,  -8.7695, -20.7445,  -8.5110,\n",
            "          -5.5884,  -7.1515, -11.8370, -15.9622, -10.6766, -12.9407, -11.3499,\n",
            "         -15.5370, -12.7670, -14.5341, -12.9473,  -8.2832,  -9.5785, -14.6937,\n",
            "         -12.6643, -14.8668, -11.7418,  -9.7024,  -8.5176, -13.1878, -19.1608,\n",
            "         -21.0487, -12.6362, -15.0252,  -8.5174,  -9.7139, -24.1445,  -7.4688,\n",
            "         -17.8835, -21.2560, -12.4063, -15.4286, -13.7794, -19.7504, -14.2934,\n",
            "          -9.8412, -17.1441, -11.0026, -12.0618, -13.0626, -12.4857, -13.4788,\n",
            "         -12.0475, -17.6131,  -8.7057, -14.2542, -11.8921, -13.3528, -12.9907,\n",
            "         -11.4601, -12.7938, -17.0955, -16.1319, -13.9566, -18.6082, -12.2911,\n",
            "         -14.1194, -18.2342, -15.8959, -12.0343, -13.3979, -12.7712, -10.5522,\n",
            "         -16.2119, -14.9086, -17.5080, -11.5496, -16.5829, -15.1728,  -9.4878,\n",
            "         -10.3868, -13.9313, -12.1027, -12.2148, -13.6509, -13.6375, -13.7398,\n",
            "         -12.2215, -11.6475,  -9.5914, -17.6211, -11.4525,  -8.7698,  -8.2679,\n",
            "         -17.4692, -11.3979, -16.8076,  -9.1117, -10.7792, -12.6804, -11.3542,\n",
            "         -10.9813, -11.9586, -11.2201, -20.4352, -12.3740, -12.8385, -13.5669,\n",
            "          -9.3568, -14.4916, -16.2199,  -8.4652, -16.3491, -13.7441, -15.4127,\n",
            "         -12.2699, -14.4117, -11.3688, -15.8953, -13.7076,  -9.3452, -15.7070,\n",
            "          -8.3873, -10.4142,  -8.6313, -16.6453, -19.4989, -11.9056, -18.0230,\n",
            "         -17.8167, -14.6391,  -9.7742, -21.0828, -13.6503, -11.7934, -12.9835,\n",
            "         -17.1630, -14.1676, -13.3954, -21.5137, -18.4010, -11.7664, -10.5324,\n",
            "         -13.4087, -12.5113, -13.2868, -13.6364, -13.2624, -10.8426, -15.9101,\n",
            "         -15.3476, -16.1747,  -9.6914, -15.4461, -18.7737,  -6.2583, -15.7581,\n",
            "         -15.7727, -11.7536, -14.6640, -12.1512, -15.4101, -10.0079,  -9.7701,\n",
            "         -10.7529, -14.3554, -12.0382, -13.0779, -13.8238, -11.0744, -11.3762,\n",
            "         -12.3864, -15.4296, -12.0408, -12.3239, -10.3286, -10.6330, -15.4734,\n",
            "         -10.4709, -15.0036, -14.2247, -13.3733, -12.9197, -13.8980, -19.2672,\n",
            "          -9.3239, -18.9078, -15.3068, -17.4576, -13.2189,  -7.3965, -10.6095,\n",
            "         -12.3763, -12.4728, -11.5926, -13.6675, -13.7597, -13.2119, -14.6567,\n",
            "         -18.1771, -19.6645, -14.0121,  -9.8539, -17.5452, -15.5900, -16.9285,\n",
            "         -13.3131,  -9.0328,  -9.5277, -10.3876, -19.1464,  -9.5857, -10.0635,\n",
            "         -11.1925, -23.9929, -10.1440, -12.1211, -13.8748, -17.3728, -11.4733,\n",
            "          -7.7712, -16.0983, -13.9257, -12.5892, -13.7941, -18.0663, -14.5187,\n",
            "         -11.4727, -13.6664, -15.2112, -14.5474,  -9.6932, -15.1418, -15.1121,\n",
            "          -9.6947, -14.5646, -12.4998, -11.5468, -15.2911, -13.3857, -15.2202,\n",
            "         -13.9817,  -8.8003, -16.1529, -11.6307, -12.1010, -12.3642, -11.6543,\n",
            "         -15.4357, -15.7086, -13.6099, -11.1007, -12.9816, -12.3867, -17.0312,\n",
            "         -14.8905, -11.5078, -14.6815, -14.4969, -15.0649, -13.8901, -12.8160,\n",
            "         -20.0662, -12.5986, -11.3802, -15.5641, -15.4282, -13.9989, -12.6583,\n",
            "         -13.1020, -17.5593, -12.1233, -11.6798, -11.9461, -16.2987, -15.5991,\n",
            "         -19.9501, -10.5435, -11.0345, -14.7746, -13.1000, -14.4971, -16.9675]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116,  28,   3,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t4.970859\n",
            "pred:\t tensor([[-17.0422, -17.3210,  -5.8653,  -3.9384, -10.9416,  -2.4291,  -7.7207,\n",
            "          -7.7647, -14.6636,  -7.7693, -12.3793,  -1.8898, -12.2153, -15.8480,\n",
            "          -7.1081, -12.1335, -11.4319, -12.7869, -15.5233, -12.1021,  -7.7791,\n",
            "         -10.2784, -16.6894, -20.1225,  -5.8625, -12.8175, -17.4719, -16.1373,\n",
            "         -17.0192, -15.1094, -13.7672, -13.2271,  -9.7596, -10.2257, -10.2456,\n",
            "          -2.7367, -10.0846,  -7.0815,  -9.2296, -16.2255,  -8.9016,  -8.0194,\n",
            "          -7.9993, -12.6244, -19.2819, -12.8367, -13.6044, -12.8955,  -7.7654,\n",
            "          -9.4154, -11.3024, -14.5183, -11.3251, -15.8271, -13.6904, -16.4599,\n",
            "         -13.5504, -12.8159, -14.1341, -14.6428, -17.6647, -18.1125, -17.8473,\n",
            "         -13.1593,  -7.8208, -10.3112, -13.1178, -14.4696, -15.3062, -13.4896,\n",
            "          -9.8933,  -9.2810,  -8.6169, -12.9160, -10.2138, -13.3652, -15.2978,\n",
            "         -12.0628,  -9.7952, -10.8994, -10.8337, -12.5175, -14.4259, -18.0825,\n",
            "         -12.6046,  -9.1791, -11.7054, -13.1412, -10.5461, -18.5596, -10.1875,\n",
            "         -12.5076, -14.5949, -20.2583, -13.3998, -15.9909,  -7.2871, -18.0173,\n",
            "         -11.8437, -11.7163, -13.7501,  -7.0234, -12.4274, -10.9112, -12.8385,\n",
            "         -12.5091, -11.3538,  -9.6998, -12.3727, -12.3730, -16.3245, -18.6119,\n",
            "          -7.8168, -11.6200, -10.9593, -13.5371, -11.6332,  -9.5575, -12.5936,\n",
            "         -17.5823, -13.2763, -14.9154, -13.5842, -15.5040, -12.4059,  -8.1767,\n",
            "          -9.2504, -12.3296, -12.2313, -11.6722,  -8.8596, -16.1688,  -6.0753,\n",
            "         -12.6320, -11.4158, -16.1254, -10.4846, -14.6141, -16.4402, -13.0322,\n",
            "         -17.3854, -21.3838, -14.5309, -10.5396, -12.0282, -21.5539,  -6.3142,\n",
            "          -5.5793,  -8.4809, -11.0177, -17.9432, -10.4681, -16.6509, -10.9896,\n",
            "         -16.5864, -16.2634, -15.6695, -15.8810, -12.1965,  -8.7208, -11.0534,\n",
            "         -13.2825, -16.3164, -12.9675,  -9.1539, -11.1619, -16.6651, -17.9523,\n",
            "         -22.5806, -13.4547, -11.9462, -10.2162,  -8.3846, -22.7323,  -7.9082,\n",
            "         -16.8545, -19.2614, -12.0458, -15.3169, -12.6070, -17.6481, -15.7668,\n",
            "         -14.6354, -15.4980, -10.1520, -11.5486, -12.2034, -12.1515, -15.6928,\n",
            "         -11.2781, -16.4634,  -9.0017, -15.0785, -12.5533, -16.6148, -13.5616,\n",
            "         -10.9186, -15.0947, -18.5477, -16.7748, -12.9659, -19.3896, -13.9795,\n",
            "         -15.2493, -16.4102, -16.5831, -15.2613, -13.5066, -14.9812, -13.3059,\n",
            "         -14.1831, -14.2853, -14.1015, -13.1898, -14.2959, -15.1798, -12.0833,\n",
            "         -10.1176, -11.8154, -12.4033, -11.0498, -11.7149, -11.7658, -11.7023,\n",
            "         -15.0824, -12.6314,  -8.5244, -19.4002, -11.9979,  -9.2959,  -9.0248,\n",
            "         -20.4058, -13.0633, -17.9246,  -8.3245, -14.8942, -11.6744, -11.4956,\n",
            "         -11.8090, -11.2104,  -9.9796, -20.8612, -11.5408, -14.1956, -17.7116,\n",
            "         -12.5518, -16.5354, -17.0454,  -9.8281, -16.2059, -17.3863, -19.1644,\n",
            "         -13.2413, -16.3170, -15.5773, -14.6031, -14.2024, -11.7625, -14.1529,\n",
            "         -10.0602, -11.3609, -11.8224, -16.3480, -21.8835,  -7.8533, -17.8854,\n",
            "         -15.7949, -15.8185, -13.6327, -21.4896, -14.2558, -13.0270,  -9.1628,\n",
            "         -20.3602, -15.6158, -14.0197, -16.8768, -16.8289, -14.3182, -12.2514,\n",
            "         -10.5135, -14.0361, -13.4312, -14.0577, -12.9347, -15.2151, -15.3196,\n",
            "         -18.7390, -14.0085, -12.0647, -15.4857, -20.1262,  -8.0640, -14.6854,\n",
            "         -16.9894, -13.6869, -15.0396, -14.7267, -16.4189, -12.3182,  -9.9033,\n",
            "         -10.8634, -14.4874, -12.1897, -15.3323, -15.0434, -12.7485, -13.5843,\n",
            "         -13.5446, -12.8333, -12.8521, -11.5926, -12.7910, -12.5504, -18.2661,\n",
            "         -10.8946, -12.9039, -21.4803, -15.4562, -16.1259, -13.0963, -19.8527,\n",
            "          -9.4251, -17.6557, -16.1614, -13.5987, -15.0959,  -9.1823, -11.9787,\n",
            "         -14.7467, -10.4493,  -9.7715, -17.9264, -11.0319,  -9.3564, -12.6662,\n",
            "         -18.8314, -17.2684, -16.7904, -11.6593, -19.0320, -14.6639, -18.5852,\n",
            "         -12.7730, -10.0084, -11.7966, -10.6599, -15.9095, -12.2551, -11.1442,\n",
            "         -12.6542, -25.4602, -12.8103, -12.4967, -14.5181, -13.3646,  -8.8322,\n",
            "          -9.2655, -16.4368, -15.7099, -12.6689, -15.7613, -16.9466, -16.0030,\n",
            "         -13.4397, -12.7254, -11.8531, -13.2028, -10.6278, -17.0650, -19.0774,\n",
            "         -12.5657, -14.2924, -15.6334,  -9.1071, -15.7242, -10.2841, -16.3576,\n",
            "         -16.3900, -11.1855, -13.8202, -12.3915, -13.8025, -14.1545, -14.0227,\n",
            "         -14.4846, -12.5201, -11.6249, -12.1375, -15.7416, -12.7651, -16.4259,\n",
            "         -16.1219, -10.8473, -14.0754, -19.2383, -17.8941, -13.7109, -11.6533,\n",
            "         -15.3455, -17.6348, -10.5042, -15.4287, -14.9748, -15.7015, -10.3873,\n",
            "         -13.1621, -15.3965,  -9.2753, -11.5715, -10.6340, -13.4465, -15.9069,\n",
            "         -20.6482, -12.5871, -12.6174, -12.5841, -15.2850, -13.3018, -17.0781]])\n",
            "\n",
            "tgt:\t tensor([ 14, 312, 277,  96,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t4.220203\n",
            "pred:\t tensor([[-17.3971, -17.7125,  -6.3489,  -3.0751, -12.4448,  -1.2068,  -9.4237,\n",
            "          -7.0768, -12.6027,  -8.3470, -11.3904,  -3.7059, -13.5351, -13.3690,\n",
            "          -7.9053, -11.8805, -12.4099, -12.9046, -15.9658, -11.9807,  -8.3548,\n",
            "         -11.3595, -15.3152, -18.4039,  -4.9152, -10.8733, -16.0264, -17.8532,\n",
            "         -17.3212, -16.8759, -13.7017, -14.6458, -10.5162, -10.6225, -10.4465,\n",
            "          -4.1700,  -9.6271,  -7.8135, -10.4678, -13.4635, -10.6020, -10.0785,\n",
            "         -10.0929, -14.4932, -16.1661, -10.9518, -13.0239, -14.2925,  -8.4063,\n",
            "          -9.8591, -12.5572, -15.5938, -11.7010, -14.6454, -12.3306, -15.7428,\n",
            "         -13.7052, -13.4334, -15.0856, -13.6966, -13.0358, -14.6570, -20.5581,\n",
            "         -13.6734, -10.5359, -11.1203, -11.4728, -13.0864, -16.7452, -13.2325,\n",
            "         -11.0922, -10.2710,  -9.6874, -13.0606, -11.1679,  -8.5822, -17.1983,\n",
            "         -10.8327,  -9.2704, -12.8523, -11.2496, -13.0009, -14.2972, -18.4675,\n",
            "         -10.0685, -11.0835, -13.1437, -12.2913, -13.2231, -20.3435, -12.7372,\n",
            "         -13.5626, -16.0107, -17.9145, -11.6663, -17.6980, -10.5221, -18.0071,\n",
            "         -12.4438, -11.3039, -16.3274,  -8.5792, -12.3052,  -9.2471, -11.7709,\n",
            "         -13.1208, -11.0673,  -9.1689, -13.4018, -11.7314, -11.7073, -14.2141,\n",
            "          -9.0870,  -9.1987, -11.0877, -13.5299, -10.8308, -11.6893, -11.5037,\n",
            "         -17.2507, -15.5320, -12.2209, -10.3046, -13.6220, -11.5959,  -8.5190,\n",
            "          -8.9274, -12.5551, -12.7796, -10.2337,  -7.9829, -12.9256,  -8.3502,\n",
            "         -14.9838, -15.0249, -16.6981, -11.0382, -15.9599, -15.4382, -12.2445,\n",
            "         -17.6128, -20.4751, -14.9280, -10.3985,  -9.6431, -21.0530,  -7.2977,\n",
            "          -5.6811,  -9.6889, -11.3303, -15.4722,  -9.8327, -16.5379, -13.5888,\n",
            "         -17.0621, -17.8568, -16.8066, -17.0162, -11.2119, -10.8060, -12.3920,\n",
            "         -13.8906, -16.0551, -11.8889,  -7.8480,  -9.5868, -15.4182, -18.2275,\n",
            "         -21.6402, -14.2704, -14.7416, -10.4787,  -9.6973, -25.0616, -10.4916,\n",
            "         -18.9823, -18.0513, -12.3270, -16.4604, -14.8922, -17.7143, -14.9328,\n",
            "         -13.2194, -18.3367, -10.4776, -12.3265, -11.5139, -13.5967, -13.9358,\n",
            "         -12.9172, -18.3215, -11.2492, -15.8510, -12.2976, -18.9805, -16.6829,\n",
            "         -15.2650, -14.4927, -17.5004, -17.7910, -15.3478, -19.9822, -14.9529,\n",
            "         -16.2657, -17.4374, -19.2606, -13.4213,  -9.8386, -14.4754, -14.2052,\n",
            "         -14.9784, -13.9744, -18.0297, -14.2957, -14.2206, -12.6305, -11.8260,\n",
            "         -13.8471, -10.1844, -14.3608, -13.2592, -15.8743, -16.2553, -16.2124,\n",
            "         -14.7980, -11.7782,  -8.7532, -19.7506, -13.0477, -11.5718,  -8.0623,\n",
            "         -21.5704, -12.4326, -14.8120,  -9.0047, -12.4605, -13.8925, -10.2254,\n",
            "         -13.6756, -12.7080, -10.0418, -21.5292, -12.3788, -16.4362, -17.6268,\n",
            "         -14.1135, -14.9052, -17.2088, -10.2265, -17.3198, -15.8659, -16.0151,\n",
            "         -12.9831, -15.2073, -15.3713, -15.2750, -12.2365, -12.0913, -14.7528,\n",
            "          -9.7191, -13.2309, -13.6207, -18.2818, -22.1496, -13.5837, -15.4453,\n",
            "         -16.9705, -14.3955, -11.1287, -17.5218, -13.7152, -13.9996, -14.5447,\n",
            "         -16.7083, -14.0132, -12.0727, -20.1850, -18.1316, -16.7454, -12.5438,\n",
            "         -10.5888, -13.8918, -12.0208, -11.4421, -13.9410, -14.7130, -15.1383,\n",
            "         -18.3050, -16.2293, -12.8321, -16.8617, -16.3371,  -8.8826, -14.2623,\n",
            "         -16.3185, -12.8822, -17.4662, -14.5055, -14.7540, -13.7555, -12.7144,\n",
            "         -11.9506, -12.3530, -13.8903, -17.9292, -15.0057, -12.2891, -13.7988,\n",
            "         -14.9320, -13.9890, -13.0205, -13.4922, -10.4030, -14.6299, -16.3969,\n",
            "         -12.9097, -13.5059, -16.2108, -13.5973, -17.2959, -15.3857, -17.1163,\n",
            "          -8.4143, -16.3901, -15.1893, -17.9829, -10.6606, -10.2549, -14.8947,\n",
            "         -12.4251, -11.2498, -11.2739, -17.4861, -14.8921, -12.1368, -15.4157,\n",
            "         -18.4861, -15.2340, -16.9631, -12.7717, -17.0973, -14.1219, -17.6797,\n",
            "         -12.9313, -10.9902, -13.2445, -15.2096, -15.3813, -11.3132, -12.0967,\n",
            "         -10.9237, -21.4043, -12.1630, -16.2020, -12.8176, -17.7406, -10.9313,\n",
            "          -9.9952, -17.1744, -14.4427, -14.6150, -13.3203, -16.2640, -11.9700,\n",
            "         -13.7983, -14.4809, -17.1180, -13.2966, -12.3140, -13.6571, -15.7225,\n",
            "         -10.5699, -17.0204, -14.5913,  -9.7315, -15.8700, -13.8804, -14.4849,\n",
            "         -18.1754, -12.2882, -17.3767, -13.9285, -14.0918, -15.1549, -12.1999,\n",
            "         -13.8528, -11.9998, -13.2246, -13.6625, -14.2352, -13.4441, -17.4922,\n",
            "         -15.3731, -12.6871, -16.1257, -14.5219, -18.2522, -16.0617, -10.8900,\n",
            "         -14.6754, -16.9392, -12.9786, -15.8510, -12.9378, -15.1714, -13.3312,\n",
            "         -16.2892, -16.6500, -12.3108, -11.8452, -12.5582, -17.4355, -14.5446,\n",
            "         -19.4016, -13.5558, -15.5191, -12.6254, -12.4098, -14.9573, -14.3478]])\n",
            "\n",
            "tgt:\t tensor([  3, 207,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t4.193652\n",
            "pred:\t tensor([[-17.6145, -17.7862,  -6.5482,  -3.2900, -11.2409,  -3.6433,  -7.4634,\n",
            "          -8.3450, -14.5279,  -8.0526, -14.0245,  -5.3072, -13.2741, -15.1730,\n",
            "          -8.7286, -14.2837, -12.2155, -16.7437, -16.4952, -12.5546,  -8.4483,\n",
            "         -13.4070, -17.0561, -20.0732,  -2.7521, -11.9790, -19.2970, -16.0801,\n",
            "         -19.0615, -17.1029, -15.2491, -15.0526,  -9.4526, -11.4100, -10.0085,\n",
            "          -2.5810, -10.0752,  -6.3089, -11.8954, -13.5925,  -9.0998,  -8.7453,\n",
            "         -13.4958,  -9.9049, -13.3308, -11.4621, -14.1164, -15.2580,  -5.5321,\n",
            "          -8.4159, -12.4529, -12.6206, -11.2221, -15.5212, -14.3118, -17.3412,\n",
            "         -13.8781, -13.9004, -16.9235, -12.1363, -14.2614, -16.2390, -18.1672,\n",
            "         -13.5804,  -6.9735, -11.4360, -15.2289, -13.3169, -16.9304, -15.7682,\n",
            "          -9.2862, -11.9432,  -8.8301, -12.0735, -11.9704, -12.1897, -17.6012,\n",
            "         -12.3262,  -9.2486, -10.5827,  -9.8567, -11.1296, -15.1507, -20.6070,\n",
            "         -12.4521, -10.1647, -11.1491, -11.4751, -15.2748, -19.4446,  -7.6632,\n",
            "         -11.1728, -15.5887, -18.9791, -13.9659, -20.4512,  -8.8384, -16.2615,\n",
            "         -12.6442, -12.0742, -13.3268,  -7.4374, -11.4219, -10.0155, -15.3020,\n",
            "         -14.8741, -11.1057,  -8.9981, -11.4153, -10.3781, -12.9807, -17.6012,\n",
            "          -9.1919,  -7.0186, -11.1946, -14.5652, -13.6899, -13.6338, -13.1885,\n",
            "         -17.2752, -16.2890, -13.3839,  -9.2974, -15.4412, -12.0861,  -7.9987,\n",
            "         -11.4295, -14.0255, -16.2084,  -6.3405,  -7.0794, -16.0643,  -9.4314,\n",
            "         -13.5465, -16.5242, -15.2322, -10.2583, -10.2681, -14.9281, -15.3676,\n",
            "         -18.2640, -22.2587, -15.7647,  -9.7163, -10.2169, -21.9061,  -8.1498,\n",
            "          -4.1966,  -8.2992, -11.8250, -16.4475, -12.6980, -15.9426, -10.4213,\n",
            "         -16.9802, -13.7260, -15.3908, -14.5164,  -9.0883, -10.3435, -13.1743,\n",
            "         -15.0324, -17.0821, -11.8230,  -8.4762,  -9.2057, -15.8903, -17.5196,\n",
            "         -20.4074, -12.8492, -13.8925, -10.0714, -12.7435, -24.5240,  -8.9320,\n",
            "         -20.3098, -16.5803, -11.6177, -15.9747, -14.0430, -19.4268, -16.5152,\n",
            "         -13.0138, -15.4003, -11.1739, -11.5446, -12.5412, -15.3804, -13.2883,\n",
            "         -10.3784, -14.3824, -12.9880, -17.0785, -14.0720, -15.6976, -15.8104,\n",
            "         -12.4982, -13.1656, -18.3898, -19.4076, -17.5019, -22.9170, -13.0247,\n",
            "         -15.2158, -16.8582, -18.3214, -16.9357, -12.2498, -14.8778, -10.9208,\n",
            "         -17.0882, -18.0820, -17.7992, -14.9089, -17.5177, -16.3070, -13.3079,\n",
            "         -13.8369, -14.9068, -13.1474, -13.4259, -14.5548, -15.5280, -16.0460,\n",
            "         -15.8520, -11.3690,  -9.5671, -17.1138, -11.9654,  -9.1755, -12.0556,\n",
            "         -19.8650, -16.3420, -18.3758, -12.0081, -12.3195, -15.2737, -11.4245,\n",
            "         -16.6084, -11.4388, -10.6931, -25.2076, -13.5606, -14.0124, -18.7774,\n",
            "         -12.1504, -17.3133, -18.4894,  -9.5310, -16.9614, -17.3117, -18.5330,\n",
            "         -13.6321, -15.7591, -14.0038, -14.5470, -11.5159, -11.3996, -19.0337,\n",
            "          -9.8955,  -8.6472, -13.7120, -16.2625, -19.7780, -13.9951, -18.3332,\n",
            "         -13.6015, -13.7350, -12.4388, -19.1820, -16.9317, -12.9947, -12.8191,\n",
            "         -16.8099, -15.8415, -11.7879, -20.2876, -17.9729, -14.0464, -12.3127,\n",
            "         -11.9548, -15.8047, -12.7224, -13.2392, -16.7525, -10.9346, -16.2712,\n",
            "         -17.2786, -19.8200, -12.1584, -18.9873, -21.8459, -10.4013, -16.0439,\n",
            "         -16.4541, -13.5721, -17.1510, -12.8177, -16.3805, -13.9884, -11.1381,\n",
            "         -10.9627, -12.2085, -11.8712, -16.2941, -17.5063, -13.1404, -12.3722,\n",
            "         -13.6710, -16.9164, -13.9626, -14.8099, -11.5054, -10.4692, -20.9765,\n",
            "         -11.2278, -13.4133, -14.1679, -12.0096, -17.6345, -16.6078, -19.5961,\n",
            "          -9.6265, -16.4843, -16.9914, -18.2403, -12.0672, -11.0131, -11.8627,\n",
            "         -13.9300, -10.8606, -13.9185, -15.7028, -14.7765, -11.6010, -14.0469,\n",
            "         -16.6388, -16.3847, -21.2449, -13.7833, -18.5603, -15.9623, -20.7803,\n",
            "         -13.2378, -10.6152, -11.6144, -11.8624, -16.8842, -12.5644, -12.5909,\n",
            "         -12.3306, -27.5392, -14.0870, -16.8732, -15.5186, -16.2344, -14.4734,\n",
            "          -9.2555, -19.2215, -16.0793, -12.4000, -15.4070, -17.5184, -16.2107,\n",
            "         -13.9731, -12.2711, -17.0880, -17.3991, -12.7056, -14.6649, -17.4657,\n",
            "         -11.4639, -14.5503, -15.0331, -11.6757, -15.1532, -15.4971, -15.3395,\n",
            "         -19.3386, -13.4764, -18.3430, -15.6070, -14.8325, -12.0035, -12.5990,\n",
            "         -17.5958, -14.7437, -15.0071, -11.8512, -16.3300, -12.0341, -18.7819,\n",
            "         -19.7463, -13.0570, -17.3942, -18.3282, -17.0064, -13.6652, -12.0876,\n",
            "         -16.7315, -17.0123, -11.4780, -16.1337, -16.2766, -16.7780, -15.9338,\n",
            "         -15.2119, -18.1839, -14.2925, -11.1539, -11.3597, -16.9741, -17.4242,\n",
            "         -20.3412,  -8.9709, -14.1075,  -9.8591, -12.1510, -12.9614, -16.2726]])\n",
            "\n",
            "tgt:\t tensor([90, 78,  3, 24,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t4.454794\n",
            "pred:\t tensor([[-17.7144, -17.9620,  -6.8674,  -3.2024, -11.4958,  -2.8795,  -8.7061,\n",
            "          -6.1906, -14.1221,  -9.4566, -14.1959,  -3.3515, -15.4872, -14.9038,\n",
            "          -9.5404, -11.2383, -12.0201, -14.2412, -16.2653, -13.7595,  -8.7555,\n",
            "         -12.0821, -17.7648, -18.5285,  -5.6057, -11.6210, -17.1730, -16.2117,\n",
            "         -17.8728, -16.9105, -11.6223, -13.2292, -10.7729, -11.0161, -10.4500,\n",
            "          -5.0522,  -8.6204,  -8.4277,  -9.5047, -15.4512, -11.3421, -10.4984,\n",
            "         -12.3095, -14.1703, -16.2303, -12.7747, -15.2428, -13.7760,  -7.8827,\n",
            "         -10.3620, -11.8940, -14.1850, -13.7084, -15.1212, -13.2956, -14.9850,\n",
            "         -15.4883, -13.0044, -15.4809, -10.2576, -13.0319, -14.1472, -20.7973,\n",
            "         -14.3057, -13.1127, -11.4806, -13.2748, -14.1074, -16.3488, -14.0741,\n",
            "          -8.5109, -10.2516,  -9.9098, -13.4496, -11.3454, -12.3518, -20.8776,\n",
            "         -14.3193, -10.2146, -11.9898, -10.1624, -12.1471, -13.4917, -15.9553,\n",
            "         -13.2003, -10.1573, -12.8771,  -9.4302, -13.4650, -20.5562, -11.1474,\n",
            "         -13.4025, -14.1241, -21.0399, -11.3530, -19.2450,  -9.3008, -20.6471,\n",
            "         -12.3016, -14.0397, -14.5350,  -5.5302, -14.4373, -11.1673, -11.3340,\n",
            "         -12.9794, -13.1041, -10.1173, -13.3964, -13.4432, -13.8354, -19.1028,\n",
            "          -8.9596, -10.6555, -10.5984, -13.9037, -12.3018, -11.1163, -14.2090,\n",
            "         -18.1231, -13.8217, -14.0529, -12.3252, -15.7640, -11.3080, -10.4721,\n",
            "         -10.7398, -13.8310, -14.6311,  -9.7896,  -9.9981, -14.6252,  -7.7570,\n",
            "         -12.2994, -16.3471, -14.9222, -11.0502, -13.7546, -16.0315, -14.4358,\n",
            "         -17.1508, -20.1321, -17.4726, -12.2605, -12.6966, -21.0869,  -7.6116,\n",
            "          -6.3749,  -8.9507, -11.4133, -16.7320, -12.4007, -15.4050, -13.4134,\n",
            "         -17.4157, -15.9846, -15.6832, -14.1878, -12.6445, -11.3653, -13.3199,\n",
            "         -13.1598, -10.9504, -11.7349, -10.5084,  -9.0566, -13.3540, -20.4336,\n",
            "         -19.9994, -15.3759, -14.6847,  -9.5176,  -9.6589, -25.1666,  -8.9024,\n",
            "         -19.7002, -17.5475, -11.5018, -19.0662, -16.3588, -19.5703, -18.0312,\n",
            "         -14.2645, -16.7698, -11.2388, -13.5065, -14.6545, -14.4926, -16.1803,\n",
            "         -12.4400, -17.2022, -11.4861, -16.5940, -14.3762, -17.2617, -12.6174,\n",
            "         -16.6180, -17.0756, -18.8422, -16.5985, -16.0014, -20.5973, -14.0937,\n",
            "         -15.5150, -17.4277, -17.5158, -13.4657, -12.7473, -13.2909, -12.2328,\n",
            "         -16.1397, -15.7188, -16.3214, -13.9013, -15.4998, -17.2902, -13.0819,\n",
            "         -12.2636, -14.1077, -12.9827, -12.6995, -14.2987, -13.0245, -15.4585,\n",
            "         -11.9648, -10.9029,  -9.3424, -19.8588, -13.8058, -11.7583, -11.1093,\n",
            "         -22.3243, -16.0059, -14.8274, -11.6188, -13.7694, -13.3808,  -9.4900,\n",
            "         -12.2771,  -9.7434, -12.9947, -21.7935, -13.9717, -13.9546, -21.9478,\n",
            "         -13.4069, -18.0785, -20.5006, -10.8178, -15.6300, -18.2891, -17.4878,\n",
            "         -11.1996, -15.4976, -15.6687, -14.3593, -15.6101, -11.9108, -13.9538,\n",
            "         -12.1829, -11.9602, -12.9317, -18.5428, -21.1702,  -8.9880, -18.2730,\n",
            "         -17.3335, -14.3607, -10.3027, -19.2593, -11.4228, -12.8489, -11.7368,\n",
            "         -20.2346, -14.3400, -12.5089, -20.7417, -19.4403, -14.7768, -11.2255,\n",
            "         -12.3698, -15.1689, -14.8979, -14.3295, -11.3732, -12.9133, -15.8683,\n",
            "         -14.8886, -13.9274, -10.4412, -18.4126, -20.2306, -10.4215, -14.4411,\n",
            "         -17.5321, -14.9236, -16.6181,  -9.2295, -16.7593, -12.1441, -12.7627,\n",
            "         -12.1608, -12.8251, -11.9495, -12.0308, -14.4230, -12.0939, -15.2759,\n",
            "         -11.9534, -14.9860, -14.4334, -15.1564, -13.3726, -13.6945, -19.6845,\n",
            "         -12.7235, -15.7286, -11.1761, -12.6997, -20.8747, -14.6088, -19.6078,\n",
            "          -7.6770, -15.5382, -15.8865, -15.8678, -14.4693, -11.9062, -13.2529,\n",
            "         -13.2182, -11.5068, -11.2180, -16.8410, -13.5910, -13.9644, -18.1754,\n",
            "         -16.9769, -15.9187, -17.4883, -14.1022, -16.5978, -15.3349, -18.9279,\n",
            "         -11.1370,  -9.9865, -10.9432, -13.0597, -18.6060, -13.4145, -13.6866,\n",
            "         -13.5954, -24.2117, -12.4080, -11.1261, -15.1700, -18.7606, -13.5684,\n",
            "         -10.6302, -18.8134, -12.1092, -15.6118, -14.3291, -18.2199, -15.2235,\n",
            "         -12.5351, -15.7454, -16.1380, -15.0872, -11.6405, -15.4627, -18.7669,\n",
            "         -10.4867, -14.3485, -14.7137,  -9.3126, -14.3739, -13.9301, -18.3985,\n",
            "         -17.0470, -11.5238, -14.3112, -14.5960, -12.0721, -15.0304, -11.9765,\n",
            "         -14.9394, -14.6416, -14.3248, -13.4262, -12.8785, -13.4134, -16.8111,\n",
            "         -18.6119, -13.0773, -18.9059, -19.3351, -20.9317, -16.3091, -14.5585,\n",
            "         -17.4868, -14.2325, -15.2070, -17.9986, -11.9875, -16.9645, -15.8605,\n",
            "         -10.4253, -17.5667, -11.7830, -13.2367, -11.3736, -16.0412, -16.8205,\n",
            "         -18.8111, -10.4928, -10.2558, -13.5431, -16.8680, -16.6667, -17.7753]])\n",
            "\n",
            "tgt:\t tensor([ 14,   3,   3, 171,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t4.735178\n",
            "pred:\t tensor([[-17.6966, -18.0047,  -6.7417,  -4.5050, -11.0044,   0.2492,  -6.6960,\n",
            "          -5.2758, -10.1694,  -8.4722, -13.5540,  -6.0860, -13.7827, -13.3751,\n",
            "          -9.3380, -13.9803, -12.1212, -14.6926, -16.0512,  -9.9017, -10.3156,\n",
            "          -8.8429, -17.5720, -20.8061,  -8.0381, -14.4763, -17.6878, -14.2805,\n",
            "         -19.2072, -20.6461, -14.6288, -14.3153, -10.4207, -10.0749, -10.5150,\n",
            "          -3.0904,  -9.0644,  -8.2406, -10.7468, -11.0310, -10.0322,  -6.7821,\n",
            "         -12.1014, -13.0672, -17.5158, -11.7350, -15.7717, -12.1708,  -7.3165,\n",
            "          -9.5705, -11.3549, -14.2016, -12.9473, -13.4573, -12.9960, -13.3735,\n",
            "         -13.0080, -15.2584, -11.3687, -13.9991, -10.1601, -14.4220, -18.8416,\n",
            "         -11.9372,  -8.2834,  -7.6856, -12.9668, -12.4882, -15.7639, -12.2367,\n",
            "          -9.2115, -12.3463,  -9.8612, -11.3539, -12.3573, -11.2216, -15.7605,\n",
            "         -11.6192, -10.1174,  -7.8079, -11.1969, -13.4187, -15.4658, -17.8547,\n",
            "         -12.4335, -10.3068, -12.7700, -13.8545, -14.6716, -24.2379, -12.8667,\n",
            "         -12.9027, -15.7764, -22.9640, -11.5027, -18.9982,  -8.6214, -18.8024,\n",
            "         -13.9497, -12.4336, -12.6757,  -6.8267,  -8.7876,  -9.1266, -11.7717,\n",
            "         -14.1817, -12.0308,  -9.1419, -14.4206, -13.2768, -15.2091, -14.3141,\n",
            "         -10.0961,  -8.8597, -16.3804, -14.8661, -13.6007, -11.2854, -15.5355,\n",
            "         -17.2506, -16.5249, -14.0515, -10.0941, -13.9869, -13.9154,  -7.7064,\n",
            "         -11.7900, -15.2446, -16.8815, -13.5888,  -7.4246, -13.3847,  -7.2808,\n",
            "         -12.8145, -16.9428, -17.3422,  -9.1072, -12.9518, -15.2452, -12.7704,\n",
            "         -16.4989, -21.7260, -11.8669,  -7.6695,  -7.8766, -23.1661,  -8.5976,\n",
            "          -5.7668,  -8.7384, -12.5777, -13.8686, -12.8652, -12.9424, -14.4719,\n",
            "         -17.0361, -17.6813, -16.7086, -15.7170, -10.1428, -10.3782, -10.0516,\n",
            "         -13.8444, -17.3836, -10.8196,  -9.6567, -10.5312, -16.4711, -19.9486,\n",
            "         -19.6794, -11.9059, -17.2307, -10.2901,  -8.4120, -24.9194,  -9.3734,\n",
            "         -21.4876, -20.0878, -15.0293, -16.8746, -14.9078, -16.3627, -15.8036,\n",
            "         -13.1622, -15.6367,  -9.4378, -14.8157, -11.6119, -13.7626, -12.2910,\n",
            "          -9.4459, -18.2808, -14.6815, -17.6843, -15.4112, -18.1072, -15.6508,\n",
            "         -12.8377, -15.8598, -18.1850, -17.1638, -13.6181, -20.3655, -13.2294,\n",
            "         -16.4992, -17.0748, -17.6897, -16.9127, -12.6919, -14.9553, -13.4715,\n",
            "         -15.2594, -16.2957, -16.4207, -13.9863, -15.2253, -15.7548, -13.4343,\n",
            "         -13.9159, -14.5753, -13.2208, -11.6629, -13.6944, -13.5695, -16.8897,\n",
            "         -13.1218, -11.3202,  -7.6000, -21.7738, -12.2130,  -9.3819,  -8.0034,\n",
            "         -23.2026, -15.3395, -14.4462, -12.2053, -10.8943, -16.7067, -11.6494,\n",
            "         -16.1449, -11.2023, -11.3112, -19.9147, -13.0633, -14.3009, -15.4621,\n",
            "         -12.7602, -17.4844, -19.9883,  -8.4225, -15.5441, -18.2532, -17.2599,\n",
            "         -12.6914, -14.5757, -19.5269, -18.3502, -15.1049, -10.0183, -16.6353,\n",
            "          -8.3669, -10.7536, -13.6033, -15.3447, -23.4667, -15.6322, -17.6387,\n",
            "         -16.6203, -15.1352, -15.1284, -23.0317, -15.6284, -13.4700, -13.4849,\n",
            "         -16.5514, -15.8730, -11.9536, -18.4569, -21.4706, -15.4201, -11.3926,\n",
            "         -13.3785, -13.8193, -11.6575, -11.9513, -16.0858, -13.3856, -15.7017,\n",
            "         -18.2490, -17.6787, -13.0097, -19.3525, -22.2373, -12.3433, -15.7761,\n",
            "         -15.7600, -12.1803, -17.9273, -13.3462, -17.8829, -15.0512, -11.5969,\n",
            "          -9.8840, -10.8365, -10.8509, -13.3278, -15.5730, -13.3171, -14.0237,\n",
            "         -13.0750, -14.5021, -14.9348, -13.5048, -10.6521, -10.9912, -14.1869,\n",
            "         -13.4414, -10.4974, -13.8045, -15.3461, -20.0067, -12.6340, -16.5215,\n",
            "          -9.3565, -18.7995, -16.0296, -17.3795, -13.8392, -11.1453, -11.2867,\n",
            "         -13.1672, -11.4127, -11.6959, -16.9131, -13.1318, -11.5093, -16.2889,\n",
            "         -17.7213, -15.2538, -17.7582, -15.2820, -17.6251, -14.6819, -16.5349,\n",
            "         -14.1619, -12.5856, -11.9276, -13.0307, -16.9707, -11.8321, -13.4714,\n",
            "         -15.0635, -22.0186, -12.5701, -16.3765, -15.3471, -16.6443, -11.1950,\n",
            "         -11.9057, -19.1759, -11.5342, -13.0907, -16.0843, -17.2107, -14.0444,\n",
            "         -13.6909, -15.2778, -19.0051, -16.7896, -15.4403, -13.2572, -16.3898,\n",
            "         -12.6000, -12.2691, -15.9592, -11.6319, -14.6739, -13.6617, -17.3221,\n",
            "         -16.4028, -12.2938, -16.8857, -13.5824, -14.4625, -15.5272, -13.8927,\n",
            "         -13.6941, -10.3521, -13.2332, -12.7347, -15.2448, -12.3155, -17.2999,\n",
            "         -17.4000, -12.0527, -16.0930, -16.5386, -15.9736, -15.9555, -14.5075,\n",
            "         -14.5930, -17.2793, -12.6250, -18.1210, -11.0124, -15.5888, -12.2540,\n",
            "         -14.7658, -20.4039, -12.2043, -15.0548, -13.4939, -17.8852, -14.1270,\n",
            "         -18.4212, -10.6491, -12.8233, -12.3011, -12.6043, -13.1211, -17.2144]])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t4.032969\n",
            "pred:\t tensor([[-18.0621, -18.3625,  -7.2675,  -4.0311, -10.1345,  -1.9532,  -9.6225,\n",
            "          -7.3489, -10.9983,  -8.6087, -14.4266,  -3.7936, -14.8738, -12.2433,\n",
            "          -9.1615, -13.0120, -10.8457, -13.7040, -19.0940, -13.2455,  -8.4255,\n",
            "         -13.6361, -15.9916, -18.2481,  -6.8454, -11.5793, -16.6114, -17.5088,\n",
            "         -18.9129, -18.0052, -12.3657, -14.8979, -11.1518, -11.1389, -10.9554,\n",
            "          -3.8002, -10.1929,  -8.2807, -11.2314, -15.2170, -11.6377,  -7.9168,\n",
            "         -13.0766, -11.3956, -15.7065, -11.1562, -15.1835, -16.0274,  -8.7932,\n",
            "          -8.1495, -11.4632, -15.3943, -14.2013, -14.9727, -13.5527, -15.7112,\n",
            "         -13.5643, -12.2977, -15.1434, -14.5077, -14.5580, -12.9047, -21.0361,\n",
            "         -12.1112,  -8.9758, -11.0168, -11.4253, -16.4308, -14.7318, -13.8055,\n",
            "         -10.5001, -12.6650, -11.4079, -12.3700, -11.8142, -13.1911, -18.1220,\n",
            "         -12.2794,  -9.7908, -11.0034, -10.8858, -12.1799, -14.4802, -19.1443,\n",
            "         -12.3867, -12.8407, -11.0161, -12.1421, -13.8440, -21.0393, -12.2507,\n",
            "         -13.9054, -16.6883, -22.9429, -13.1202, -18.7728,  -8.6616, -17.6148,\n",
            "         -11.0049, -12.6232, -15.1008,  -7.0427, -13.1507,  -9.0011, -11.9391,\n",
            "         -16.3228, -12.8854, -10.3743, -13.7427, -13.1307, -18.4613, -17.3245,\n",
            "          -9.7772,  -9.7276, -12.7824, -16.1875, -13.9434, -14.0115, -12.9275,\n",
            "         -18.9853, -16.3243, -15.3719, -11.7764, -15.6789, -11.2601,  -9.5899,\n",
            "         -15.8424, -14.3224, -12.3889,  -8.1388,  -9.5381, -16.6800, -11.0008,\n",
            "         -14.3052, -16.5177, -15.3459, -10.9743, -12.8171, -16.8482, -15.0718,\n",
            "         -16.1983, -22.0447, -14.5139,  -9.0922,  -9.4054, -23.5792,  -9.3263,\n",
            "          -5.6405,  -9.0099, -13.8605, -15.5906, -13.2227, -15.1718, -15.7150,\n",
            "         -16.1430, -17.0015, -16.1185, -17.5594, -12.7470, -10.6046, -12.7654,\n",
            "         -12.1196, -15.6318, -12.5796, -11.0692,  -8.9167, -14.6101, -21.2772,\n",
            "         -19.6537, -16.2697, -16.8925, -12.3125, -10.1463, -24.2887,  -7.4561,\n",
            "         -18.9328, -18.8449, -12.0806, -16.6676, -15.9305, -18.7061, -21.6289,\n",
            "         -13.7302, -16.2503, -12.6027, -14.1570, -14.5000, -16.1035, -15.2478,\n",
            "         -10.7942, -17.8857, -12.4895, -17.6052, -13.3157, -16.8112, -15.9278,\n",
            "         -15.5272, -15.9537, -19.9455, -18.2140, -14.1650, -21.4110, -13.0156,\n",
            "         -18.3378, -18.9500, -20.5273, -13.3572, -13.0714, -16.1521, -12.3443,\n",
            "         -14.7461, -17.7611, -17.2504, -17.8865, -18.2932, -15.2686, -12.7207,\n",
            "         -12.1385, -15.6563, -13.3653,  -9.6023, -16.4411, -13.9500, -16.5505,\n",
            "         -16.2043, -11.2050,  -9.1322, -21.0024, -14.6269, -11.7175,  -9.2825,\n",
            "         -19.1623, -15.1826, -14.9604,  -8.9653, -15.2452, -15.4561, -11.1833,\n",
            "         -15.0695, -12.2041, -12.1185, -20.8497, -13.1478, -13.4113, -19.4507,\n",
            "         -13.3003, -16.8999, -19.0031, -10.3644, -15.1954, -18.4864, -19.8434,\n",
            "         -15.2478, -14.0609, -16.9324, -14.3988, -15.4337, -13.5700, -20.1987,\n",
            "          -9.3976, -12.0008, -11.8919, -21.0213, -21.7632, -12.4882, -18.8771,\n",
            "         -16.6418, -16.6506, -12.2166, -21.8671, -12.0914, -14.2636, -14.3276,\n",
            "         -16.7060, -13.2686, -12.9910, -22.7035, -20.4578, -15.3992, -12.7988,\n",
            "         -13.4386, -16.2345, -14.9899, -12.1049, -16.6128, -17.8996, -16.3936,\n",
            "         -16.7515, -17.2646, -13.0599, -17.1778, -21.3994, -11.3547, -16.3448,\n",
            "         -16.4894, -14.1849, -16.7683, -14.8561, -16.3690, -13.0153, -13.7575,\n",
            "         -12.3551, -15.1585, -12.5550, -14.7592, -12.9977, -12.2259, -15.3016,\n",
            "         -14.7035, -17.0422, -14.4923, -13.8072, -11.1028, -10.0207, -19.0122,\n",
            "         -14.2773, -13.8946, -16.0454, -12.6247, -17.3150, -14.9186, -21.6527,\n",
            "          -9.0543, -18.8198, -17.4838, -17.0098, -12.6518, -11.1372, -12.4950,\n",
            "         -15.7260, -12.8071, -13.1730, -16.8219, -13.2074, -12.6915, -17.3759,\n",
            "         -18.8175, -17.0017, -18.4611, -12.0608, -17.9885, -16.9267, -17.2305,\n",
            "         -13.1541,  -9.3115, -12.9906, -13.4985, -20.2652, -12.1219, -11.7320,\n",
            "         -12.8193, -26.8626, -14.3127, -13.1489, -12.5991, -16.3741, -10.9186,\n",
            "         -10.0512, -17.0139, -14.1193, -15.9218, -13.9454, -20.2114, -12.0299,\n",
            "         -14.4055, -15.8903, -16.1022, -12.4869,  -7.5297, -15.3604, -17.0159,\n",
            "         -12.0938, -14.7765, -15.4185, -13.0845, -15.6338, -10.6137, -22.1489,\n",
            "         -18.8500, -14.9125, -15.0586, -14.6693, -16.1065, -15.8805, -14.4684,\n",
            "         -14.8450, -13.8460, -16.2102, -12.7043, -15.4072, -12.3230, -19.8311,\n",
            "         -17.1873, -12.3396, -17.2199, -18.2240, -19.7729, -17.8467, -11.6243,\n",
            "         -17.3688, -15.9959, -12.9889, -17.8862, -14.9231, -15.4000, -13.0340,\n",
            "         -14.3409, -20.3871, -11.6790, -14.2077, -11.6419, -19.7439, -17.9461,\n",
            "         -23.3408, -12.3156, -12.8155, -10.8752, -14.9656, -12.3616, -17.4786]])\n",
            "\n",
            "tgt:\t tensor([41,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t3.897990\n",
            "pred:\t tensor([[-18.3655, -18.6456,  -7.5031,  -3.6567, -11.4254,  -4.0237, -10.0155,\n",
            "          -5.6828, -11.0060,  -9.8081, -15.2871,  -1.4289, -15.4612, -11.7302,\n",
            "          -7.1293, -12.8232, -14.2481, -14.8883, -17.5120, -14.7511, -11.2222,\n",
            "         -13.1473, -20.2479, -19.9490,  -7.3019, -12.1115, -18.4215, -18.3444,\n",
            "         -16.9374, -18.2381, -14.1473, -15.6524, -11.7789,  -9.5094, -10.7934,\n",
            "          -2.9359, -11.3009,  -9.3373, -11.2605, -15.4777, -11.0399, -10.8295,\n",
            "         -13.5583, -13.9417, -19.0983, -12.5163, -15.0921, -17.5939,  -8.1300,\n",
            "         -11.0773, -12.5032, -14.1462, -14.5842, -15.2049, -15.5204, -15.0375,\n",
            "         -14.6287, -12.3171, -17.3694, -12.2029, -14.3526, -16.2055, -18.7767,\n",
            "         -17.0839, -12.0519, -12.1363, -11.9908, -14.5203, -18.2276, -14.2619,\n",
            "         -10.5008, -11.5261, -10.3452, -14.4559, -11.8239, -12.4244, -17.7757,\n",
            "         -17.0416, -12.2928, -12.2550, -11.1697, -13.1037, -14.4985, -20.8817,\n",
            "         -11.3670, -12.2521, -13.5719, -14.5480, -14.7821, -20.8055, -12.1921,\n",
            "         -16.9964, -15.9398, -20.1451, -13.1468, -17.1509,  -8.7502, -18.3518,\n",
            "         -14.6642, -11.5530, -14.5088,  -8.1905, -12.8488, -11.0604, -13.2267,\n",
            "         -13.5695, -10.0929, -10.9375, -14.0397, -11.4457, -16.5062, -16.4352,\n",
            "         -10.7585,  -9.6978, -12.3609, -15.2820, -13.1783, -12.5177, -13.7291,\n",
            "         -18.6931, -16.6673, -13.6812, -13.5585, -16.8260, -13.2658,  -8.5472,\n",
            "         -11.2472, -12.6535, -15.6256, -11.2279,  -6.0209, -15.6377,  -9.0789,\n",
            "         -14.0006, -16.8499, -16.6706, -11.9819, -13.3432, -16.5125, -15.6325,\n",
            "         -19.6396, -20.6313, -14.8236,  -9.9210, -12.1946, -22.7051, -10.0433,\n",
            "          -7.5114,  -9.9525, -14.9097, -13.7149, -11.3441, -17.0531, -15.2532,\n",
            "         -16.5683, -15.3634, -15.6233, -17.4106,  -8.8545, -12.0425, -12.4073,\n",
            "         -12.6739, -15.2765, -14.5114,  -9.0127, -10.2407, -16.5650, -23.2505,\n",
            "         -22.8974, -13.5025, -18.6379, -11.5571, -10.2882, -27.8323,  -9.2012,\n",
            "         -20.0644, -20.4257, -12.6315, -19.4545, -17.9457, -20.6963, -15.7784,\n",
            "         -13.3512, -17.5094, -11.5448, -13.6898, -15.3542, -12.6537, -14.8768,\n",
            "         -12.7874, -18.0781, -12.4187, -16.7619, -13.2840, -19.1337, -12.0132,\n",
            "         -17.3815, -14.6879, -19.6095, -21.2365, -18.3345, -22.8014, -13.9172,\n",
            "         -18.7069, -18.6080, -19.5830, -15.0124, -12.1304, -16.0008, -14.7613,\n",
            "         -15.8137, -17.6754, -17.1302, -14.6447, -17.4076, -14.8611, -12.3528,\n",
            "         -11.8404, -12.9166, -14.3213, -14.9342, -12.6004, -14.5750, -14.2054,\n",
            "         -12.4387, -12.4120,  -9.2789, -20.3014, -16.1488,  -9.6668,  -9.3456,\n",
            "         -18.6392, -13.2812, -14.0517, -10.7156, -13.5668, -14.9044, -11.5876,\n",
            "         -14.6786, -10.3123, -10.5041, -23.3738, -14.8689, -15.3547, -15.6398,\n",
            "         -12.6707, -16.3650, -19.2727, -11.1702, -16.7453, -20.0388, -17.0844,\n",
            "         -14.4241, -15.5693, -17.7191, -15.4475, -16.0212, -12.5479, -22.6341,\n",
            "          -9.9288, -12.8709, -11.9970, -18.1704, -21.8708, -13.7213, -20.5136,\n",
            "         -18.4794, -18.1186, -10.8584, -21.5626, -12.7767, -15.4347, -16.7802,\n",
            "         -16.0246, -18.7835, -14.3566, -20.8687, -21.5668, -17.0248, -12.6452,\n",
            "         -15.3295, -18.3119, -13.4120, -14.5648, -12.5257, -13.4557, -19.2944,\n",
            "         -19.5177, -19.9320, -11.5911, -17.9025, -19.7252, -13.1433, -18.9909,\n",
            "         -18.6687, -15.4842, -15.9934, -12.6597, -19.1570, -11.0032, -13.0423,\n",
            "         -13.4919, -14.7900, -15.6008, -13.8287, -16.1947, -15.9211, -14.1900,\n",
            "         -16.2487, -15.8102, -18.9555, -15.1468, -10.7281, -11.4150, -24.1750,\n",
            "         -12.4459, -15.6763, -13.4566, -12.5284, -16.6780, -19.6499, -20.9352,\n",
            "         -12.8521, -18.8851, -18.1762, -16.5546, -14.7993, -11.9222, -14.7815,\n",
            "         -16.6121, -12.8777,  -9.6819, -15.3017, -12.9607, -11.6297, -16.3322,\n",
            "         -18.7147, -16.6779, -18.3003, -13.1168, -19.4518, -17.2405, -20.4026,\n",
            "         -14.0207, -12.3286, -13.0765, -11.8540, -19.6940, -12.8055, -10.7799,\n",
            "         -14.4357, -24.9546, -13.6063, -13.1470, -12.8342, -18.1001, -13.2217,\n",
            "         -11.7085, -23.2473, -14.2281, -15.1978, -17.1506, -17.6619, -12.8122,\n",
            "         -15.0573, -15.4315, -18.0662, -14.7791,  -9.8332, -16.6594, -18.1933,\n",
            "         -12.5671, -15.9087, -16.8167, -12.5452, -15.6641, -11.4850, -18.3694,\n",
            "         -15.0876, -13.7818, -18.0054, -17.6666, -14.6330, -14.0890, -13.0311,\n",
            "         -18.2339, -15.5427, -17.3929, -16.9625, -16.6293, -15.5324, -18.8720,\n",
            "         -16.2321, -14.7948, -17.7680, -15.8130, -17.9398, -16.4668, -12.9531,\n",
            "         -19.8287, -17.4056, -11.1719, -17.3743, -15.1932, -17.5919, -15.5720,\n",
            "         -12.0149, -21.4240, -13.7670, -13.2550, -12.7518, -17.0698, -18.1955,\n",
            "         -21.3542, -13.6542, -13.6238, -12.9589, -15.3995, -17.9942, -16.2490]])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t4.018249\n",
            "pred:\t tensor([[-18.5900, -18.7991,  -7.1645,  -5.9483, -11.5721,  -4.5499,  -8.0244,\n",
            "          -6.3415, -13.3909,  -8.9486, -15.1650,  -5.2105, -14.2628, -16.1008,\n",
            "          -9.4453, -14.2765, -11.7495, -16.6901, -15.2600, -12.4426, -12.7633,\n",
            "         -14.0179, -19.0156, -20.1214,  -6.6233, -14.5591, -18.7625, -18.6629,\n",
            "         -19.8072, -18.6777, -14.3884, -16.6737, -11.7943, -11.0263, -10.4989,\n",
            "          -3.9777,  -8.8101,  -8.2921, -10.8681, -17.3796,  -9.9280,  -9.3437,\n",
            "         -11.8664, -13.3046, -18.3697, -11.4370, -15.2655, -13.6876,  -4.4610,\n",
            "          -9.7162, -12.0201, -13.7345, -13.5883, -14.8938, -14.9093, -15.9863,\n",
            "         -17.0179, -13.2239, -18.0712, -13.1909, -14.4417, -15.1653, -20.4047,\n",
            "         -11.6257,  -9.3859, -13.0707, -16.2153, -17.3725, -17.7252, -13.2770,\n",
            "         -10.7932, -11.1446, -10.3644, -14.5151, -12.6286, -12.6094, -18.3439,\n",
            "         -11.7624, -12.3275, -10.5413, -10.4021, -11.7236, -15.0265, -20.1378,\n",
            "         -13.3857, -11.0474, -15.1887, -14.5520, -13.7999, -22.2157,  -8.5046,\n",
            "         -14.7450, -17.1397, -19.6426, -13.1022, -20.5343, -11.5881, -22.1223,\n",
            "         -16.2636, -14.3807, -15.6695,  -4.6205, -10.6944,  -9.9137, -13.5588,\n",
            "         -16.4264, -13.0413,  -9.7898, -14.2203, -14.8689, -16.8282, -18.6589,\n",
            "          -9.3946, -10.9299, -10.8992, -14.2228, -14.5253, -14.1809, -13.6633,\n",
            "         -19.3990, -16.0448, -14.2697, -12.2382, -14.5604, -15.7242,  -7.7417,\n",
            "          -9.0423, -14.9619, -17.1645, -10.1889,  -5.1856, -17.9146,  -9.9220,\n",
            "         -13.8461, -15.5160, -14.9393, -11.2118, -12.9712, -17.9663, -14.3618,\n",
            "         -18.7993, -23.6504, -16.7190, -10.8422, -11.2811, -23.9790,  -8.6805,\n",
            "          -5.9077, -10.8258, -13.6637, -20.1763, -12.4725, -16.2761, -14.1815,\n",
            "         -17.9261, -16.6572, -16.6791, -17.9068, -11.5943, -10.9872, -14.1362,\n",
            "         -15.2438, -14.6426, -10.5532,  -8.0970,  -8.8524, -17.2654, -23.0640,\n",
            "         -21.7461, -15.1602, -15.7641, -10.6923,  -8.1809, -28.1403,  -7.4843,\n",
            "         -24.3593, -19.2705, -14.5430, -18.4903, -15.8158, -22.6332, -14.9934,\n",
            "         -16.3440, -16.8214, -12.0462, -14.5395, -15.0279, -15.6484, -14.6751,\n",
            "         -11.9925, -16.7736, -12.7951, -17.4185, -16.1309, -18.9164, -16.2028,\n",
            "         -12.8424, -15.8938, -20.8304, -19.5811, -14.5348, -23.5231, -13.4236,\n",
            "         -17.2810, -19.9646, -21.8377, -16.9794, -14.5802, -16.3100, -14.4765,\n",
            "         -15.7862, -19.5353, -18.3738, -18.6938, -21.0162, -18.2506, -13.8768,\n",
            "         -10.8202, -15.1921, -11.6156, -14.9521, -16.2410, -16.2339, -20.4433,\n",
            "         -14.1385, -12.7179,  -8.9474, -20.9624, -12.6776,  -9.7225, -11.9007,\n",
            "         -23.7139, -14.2122, -15.3672, -11.4334, -15.7528, -15.7851, -15.9307,\n",
            "         -13.9193, -13.2807, -13.2981, -27.1758, -15.9641, -17.3151, -18.5410,\n",
            "         -13.4750, -17.6458, -18.9472,  -9.8937, -17.3499, -22.4287, -18.3749,\n",
            "         -15.7622, -18.4070, -18.3429, -15.5855, -13.7359, -14.7338, -20.8332,\n",
            "         -10.2613, -10.8143, -11.6547, -20.3041, -24.2117, -13.6175, -18.7984,\n",
            "         -17.2439, -16.8308, -12.5644, -20.1750, -14.7950, -16.1011, -16.0840,\n",
            "         -20.6414, -15.9591, -12.7829, -23.1525, -20.6769, -15.0867, -11.0737,\n",
            "         -13.7846, -17.2702, -15.2000, -13.2162, -17.2512, -17.8509, -16.1700,\n",
            "         -19.3821, -19.8516, -12.2972, -19.5540, -19.7547, -10.8514, -15.9708,\n",
            "         -19.4330, -14.1366, -17.5775, -13.0488, -18.7213, -13.0287, -12.9593,\n",
            "         -12.5507, -15.3689, -12.4210, -14.3771, -14.3905, -13.4714, -15.0244,\n",
            "         -16.3751, -17.2886, -18.0120, -11.9040, -11.2063, -13.5271, -21.5364,\n",
            "          -9.8810, -12.2583, -15.4039, -14.4271, -17.3754, -14.8179, -21.6915,\n",
            "          -4.8773, -17.0913, -15.2263, -15.4730, -15.3100, -10.3888, -13.8791,\n",
            "         -12.9727, -11.9924, -15.9119, -16.1936, -16.0616, -12.6886, -18.4644,\n",
            "         -19.5195, -16.1799, -22.7085, -11.0956, -18.6400, -16.6807, -17.5270,\n",
            "         -11.4205, -10.9527, -13.5157, -12.6569, -20.3887, -13.0725, -14.9072,\n",
            "         -14.3572, -24.3152, -11.5239, -14.5717, -16.8580, -17.8232, -11.3449,\n",
            "         -10.9753, -20.2619, -14.7891, -16.2297, -16.2375, -18.2179, -18.0952,\n",
            "         -14.3039, -16.6699, -17.9766, -10.3217, -13.2989, -18.1897, -19.4229,\n",
            "         -14.6549, -17.9603, -15.5302, -12.0522, -14.9831, -13.2418, -16.1555,\n",
            "         -17.6849, -15.0640, -16.9765, -16.8762, -16.9040, -13.6773, -13.1913,\n",
            "         -19.7368, -15.4057, -16.8644, -11.8430, -15.3898, -11.1678, -19.9012,\n",
            "         -20.4702, -12.7278, -18.3722, -17.9519, -21.4120, -17.3044, -13.8296,\n",
            "         -18.1134, -15.0166, -12.9728, -17.2755, -13.4922, -15.6077, -15.4252,\n",
            "         -12.9070, -20.8549, -13.1803, -14.6196, -12.0787, -16.0773, -15.4807,\n",
            "         -21.8243, -12.9484, -11.5212, -10.4247, -14.0922, -12.7423, -21.2213]])\n",
            "\n",
            "tgt:\t tensor([ 48, 143,  34,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t4.823122\n",
            "pred:\t tensor([[-18.8300, -19.1446,  -6.7592,  -7.1975, -13.4350,  -7.4391,  -9.3074,\n",
            "          -5.6884, -13.0424,  -9.5555, -15.5843,  -7.8213, -15.4518, -18.1784,\n",
            "          -9.3194, -14.6935, -13.3808, -15.0028, -18.1859, -12.1513, -13.0435,\n",
            "         -11.8734, -15.7788, -20.6667,  -3.4474, -13.2908, -20.1663, -19.0340,\n",
            "         -18.4616, -19.9507, -14.5611, -18.2352, -11.7976, -13.0086, -11.6238,\n",
            "          -3.3210,  -7.9789, -10.2321,  -8.2578, -14.1723,  -8.5492,  -9.3108,\n",
            "         -10.8619, -13.0667, -17.2737,  -9.7027, -14.4951, -14.7887,  -7.6679,\n",
            "          -9.2582, -11.4832, -15.3389, -13.3803, -16.4940, -16.4217, -16.9458,\n",
            "         -19.1997, -12.8327, -18.5106, -14.7837, -13.6366, -20.1179, -19.9510,\n",
            "         -15.2689, -10.5192, -15.3909, -15.3749, -14.0572, -20.5510, -15.2291,\n",
            "         -10.9768, -11.4706,  -8.3400, -14.7645, -11.3786, -13.6939, -18.6814,\n",
            "         -12.5281, -12.2165,  -8.6187, -10.1104, -12.9448, -15.8872, -21.6721,\n",
            "         -16.2304, -10.4559, -16.3820, -15.6904, -16.7080, -22.3948, -10.1274,\n",
            "         -13.9485, -16.1299, -20.9322, -12.0832, -21.9179,  -6.7515, -20.2519,\n",
            "         -15.4489, -14.5637, -14.5521,  -5.4605, -12.9498, -11.0078, -16.8880,\n",
            "         -16.5287,  -8.8352, -11.0770, -13.7701, -13.5056, -17.5442, -19.8643,\n",
            "          -8.2864,  -9.4528, -10.1115, -16.8924, -15.4077, -12.9656, -13.4260,\n",
            "         -18.9947, -15.6364, -16.7370, -12.1378, -17.5831, -14.6165,  -8.5945,\n",
            "         -11.3854, -16.2487, -16.6353, -10.1035,  -9.2889, -16.9056,  -9.3279,\n",
            "         -15.5994, -17.4988, -18.4040,  -9.3887, -12.2526, -14.0868, -14.6045,\n",
            "         -16.5033, -22.7288, -17.6409, -10.3642, -13.1316, -22.0611,  -8.7246,\n",
            "          -5.4408, -10.2883, -10.8114, -17.7316, -13.7960, -21.0954, -15.9052,\n",
            "         -18.2745, -16.7954, -17.8819, -19.4093, -10.4403, -11.9483, -12.6010,\n",
            "         -15.9225, -17.0088, -11.6402,  -9.2903, -11.6496, -17.4576, -19.7049,\n",
            "         -19.2001, -17.5865, -14.4122, -11.1451, -10.2065, -21.9654,  -8.4922,\n",
            "         -21.4759, -15.4536, -14.5654, -19.4660, -14.4511, -20.8747, -17.7343,\n",
            "         -19.0999, -18.4713, -11.8605, -13.9219, -12.9535, -14.4421, -11.7096,\n",
            "         -12.2042, -16.0241, -15.3417, -17.9195, -14.7758, -19.8675, -18.0379,\n",
            "         -13.2783, -15.9350, -20.3543, -21.4878, -18.2036, -22.1651, -16.1181,\n",
            "         -17.1060, -17.3981, -21.2880, -16.4394, -13.9213, -12.9292, -13.7360,\n",
            "         -15.2247, -14.8564, -17.1085, -15.3302, -17.7496, -15.2540, -13.3871,\n",
            "         -14.2663, -15.4943, -13.4147, -12.1870, -14.2650, -18.4218, -17.8831,\n",
            "         -11.0360, -13.0698, -10.3618, -22.6383, -15.3117, -11.9126, -13.2243,\n",
            "         -22.3558, -14.7234, -14.9050, -13.6226, -16.5390, -18.3965, -14.6057,\n",
            "         -17.6697, -10.4212, -11.5336, -22.5123, -17.5774, -16.5399, -20.0374,\n",
            "         -17.3102, -21.9348, -19.0621, -12.9904, -17.4299, -21.3634, -20.5206,\n",
            "         -16.6445, -17.4020, -15.8364, -17.1552, -12.3805, -12.0523, -18.8325,\n",
            "         -11.7262, -11.5351, -12.8761, -19.8465, -27.1878, -12.5658, -18.2409,\n",
            "         -15.3555, -14.2438, -14.0851, -18.9718, -16.4449, -15.0570, -13.1829,\n",
            "         -23.4227, -20.2138, -14.3822, -22.2359, -18.9089, -14.5342, -15.9173,\n",
            "         -11.4120, -16.1079, -13.2199, -14.3963, -12.9976, -16.3048, -17.7809,\n",
            "         -18.8660, -19.8554, -14.1612, -18.4891, -24.2928, -10.7998, -18.9202,\n",
            "         -17.0046, -14.7228, -19.1653, -13.3335, -19.7922, -11.0589, -12.7249,\n",
            "         -13.0598, -12.0373, -12.4284, -16.6243, -16.8050, -15.0995, -14.8091,\n",
            "         -15.6004, -16.6257, -18.9161, -11.8114,  -8.8763, -11.1023, -18.6138,\n",
            "         -13.5874, -12.0818, -16.3056, -16.7056, -21.9113, -14.0705, -20.8544,\n",
            "         -11.9497, -16.9028, -18.1138, -16.1930, -16.9412, -16.5682, -15.8139,\n",
            "         -13.1821, -13.2961,  -9.5268, -21.0559, -12.9889, -11.3427, -16.6942,\n",
            "         -14.1613, -13.6357, -22.3482, -13.8276, -19.7895, -15.7186, -21.3024,\n",
            "         -14.4631, -16.0518, -11.2746, -14.6397, -15.4421, -13.6990, -15.8650,\n",
            "         -14.1619, -22.4420, -12.1605, -15.3798, -18.1144, -13.5303, -12.1767,\n",
            "         -11.8499, -18.1726, -17.6546, -17.1188, -14.0049, -18.2417, -16.5819,\n",
            "         -15.4434, -16.5393, -17.7596, -13.5292, -12.1466, -14.5344, -16.9170,\n",
            "         -13.6032, -15.7331, -18.4338,  -9.0840, -15.3721, -15.4077, -17.8141,\n",
            "         -15.8392, -13.7107, -15.3285, -15.0753, -15.2789, -15.9364, -13.5936,\n",
            "         -18.5085, -15.8405, -13.8985, -10.7398, -16.4929, -11.3822, -21.6136,\n",
            "         -18.9724, -12.7415, -17.5509, -18.6444, -21.6585, -15.6941, -14.3117,\n",
            "         -14.5911, -16.8949, -10.7469, -15.2464, -13.9232, -18.4280, -13.9754,\n",
            "         -14.8587, -18.8497, -12.1302, -14.4888, -12.2702, -17.8106, -12.6534,\n",
            "         -23.3990, -11.6775, -11.4192, -12.3352, -12.8137, -15.9319, -18.5394]])\n",
            "\n",
            "tgt:\t tensor([170, 117,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t3.322615\n",
            "pred:\t tensor([[-19.0915, -19.3860,  -7.8480,  -5.0428, -12.9525,  -4.2885, -11.0337,\n",
            "          -7.6826, -13.4065, -10.1628, -15.7607,  -2.3740, -16.6678, -17.5963,\n",
            "          -7.8875, -12.0188, -15.2983, -14.8851, -17.0789, -13.7983,  -9.5650,\n",
            "         -14.1585, -16.8934, -23.0936,  -7.9525, -13.5770, -16.5316, -21.9522,\n",
            "         -19.0363, -20.1097, -15.5935, -15.3609, -10.7699, -11.3759, -12.4216,\n",
            "          -4.3445, -11.2081,  -8.9357, -10.4183, -20.1472, -13.0069, -12.0475,\n",
            "         -15.3598, -16.2736, -15.6874, -14.1171, -14.6060, -17.1701, -10.5744,\n",
            "         -11.4208, -15.0261, -15.4446, -14.2690, -16.8558, -14.4424, -18.0930,\n",
            "         -16.1084, -14.3251, -19.3184, -15.8653, -13.6289, -16.9309, -19.1933,\n",
            "         -14.1807, -11.1949, -12.7716, -12.2807, -16.0351, -19.2085, -15.6257,\n",
            "         -10.7736, -15.1779, -12.1826, -14.9308, -14.2784, -13.5210, -21.5584,\n",
            "         -14.0274, -12.7342, -13.2719, -12.8759, -14.3874, -15.3867, -17.6626,\n",
            "         -11.3648, -11.2257, -14.5195, -12.7978, -13.8447, -20.8216, -11.4280,\n",
            "         -14.1530, -14.8871, -20.3605, -12.4888, -20.6497, -14.5964, -18.7155,\n",
            "         -15.3177, -10.3814, -16.2724,  -6.3260, -11.7859, -10.6145, -14.4380,\n",
            "         -15.5429, -12.0327, -12.0887, -14.9493, -16.2342, -14.5009, -20.1204,\n",
            "         -10.0395, -12.0843, -12.0956, -15.8571, -14.6978, -15.2191, -15.8067,\n",
            "         -20.8408, -13.6355, -14.7249, -14.1571, -15.5947, -12.8186, -10.5634,\n",
            "         -11.5664, -17.5158, -14.4815, -13.0442,  -7.4982, -18.0306, -10.5695,\n",
            "         -12.3623, -18.0436, -17.2752, -11.9736, -15.4702, -16.2834, -15.8015,\n",
            "         -20.1584, -22.0367, -17.3549, -10.5892,  -9.6549, -25.2093,  -8.0674,\n",
            "          -5.6924,  -9.1201, -13.2219, -19.3500, -15.8712, -17.7709, -13.9786,\n",
            "         -17.8111, -16.1149, -15.9963, -16.4302, -12.6432, -13.4451, -14.1271,\n",
            "         -14.6283, -16.5199, -12.8355, -11.4629,  -9.9593, -19.8900, -24.0759,\n",
            "         -22.3728, -15.3828, -17.6028, -10.6402,  -9.6632, -26.7714, -10.8846,\n",
            "         -19.4796, -19.8707, -14.4732, -21.7895, -17.7844, -20.7021, -21.1197,\n",
            "         -15.6450, -18.4641, -12.8029, -14.0299, -15.0073, -16.7772, -16.5838,\n",
            "         -12.4805, -18.4986, -13.3410, -18.5215, -15.7402, -19.9366, -17.7460,\n",
            "         -13.8690, -15.6306, -19.6850, -20.4142, -18.1641, -22.3548, -17.5861,\n",
            "         -18.7391, -20.4945, -19.9252, -13.7608, -15.9087, -12.1637, -14.2807,\n",
            "         -16.7168, -17.6779, -17.2231, -15.4546, -16.9691, -17.1104, -14.8153,\n",
            "         -11.1985, -15.1097, -14.9926, -12.4379, -16.3490, -15.7604, -14.7238,\n",
            "         -17.6718, -12.0972, -10.0406, -23.1818, -15.7871,  -9.8059, -12.9080,\n",
            "         -21.1299, -14.6695, -17.9126, -12.3885, -12.6580, -15.8574, -12.5366,\n",
            "         -14.7813, -13.8438, -10.6272, -22.7751, -14.5122, -14.3467, -19.8718,\n",
            "         -15.8935, -21.7426, -20.1548, -12.5493, -15.5818, -21.5073, -19.5162,\n",
            "         -13.6600, -14.8368, -17.7460, -16.4408, -13.1923, -15.7436, -18.9228,\n",
            "         -10.4499, -12.3366, -14.7459, -20.2041, -23.7789, -11.7807, -20.7898,\n",
            "         -15.1421, -14.9068, -12.2044, -22.1956, -14.0778, -14.8717, -12.6443,\n",
            "         -22.4742, -16.6241, -14.4113, -24.4072, -21.2952, -16.8815, -15.9980,\n",
            "         -15.0195, -17.3991, -14.7798, -16.9815, -15.8120, -17.1310, -16.9892,\n",
            "         -17.6336, -21.0870, -14.9766, -19.4443, -21.2104,  -9.0402, -17.8332,\n",
            "         -18.6365, -13.6061, -16.5643, -13.2297, -20.1696, -12.0554, -13.1650,\n",
            "         -13.3389, -16.4902, -15.7572, -14.1249, -17.6967, -16.8141, -15.0578,\n",
            "         -13.8329, -17.3194, -16.3234, -11.4710, -14.2191, -12.3198, -21.6042,\n",
            "         -14.1028, -16.5538, -15.5671, -16.0622, -19.6433, -15.8648, -20.4764,\n",
            "         -10.1733, -15.9183, -16.1335, -12.9494, -17.2839, -13.4781, -14.4018,\n",
            "         -19.1514, -13.9784, -10.9525, -18.3897, -15.5026, -14.9919, -15.3180,\n",
            "         -19.1859, -17.9560, -20.3009, -11.9394, -20.5312, -17.5295, -18.1321,\n",
            "         -10.4646, -12.2985, -14.2629, -19.2905, -22.0318, -13.2781, -13.5706,\n",
            "         -13.6293, -24.0791, -11.9176, -14.9891, -16.4596, -18.1331, -11.5942,\n",
            "         -13.2669, -21.3934, -13.4536, -13.2152, -17.0701, -20.1118, -14.6572,\n",
            "         -14.9372, -18.5828, -16.9421, -12.5281, -11.8812, -16.1197, -22.2065,\n",
            "         -12.5643, -18.2294, -17.7763, -11.9826, -15.7449, -13.0766, -17.8826,\n",
            "         -19.5308, -13.0837, -15.2686, -12.8811, -13.3702, -14.2105, -16.9442,\n",
            "         -18.2041, -14.0667, -16.7698, -14.7411, -13.2946, -11.5498, -21.8475,\n",
            "         -20.4837, -13.0978, -16.7121, -20.4604, -20.4549, -16.3973, -13.4685,\n",
            "         -18.0375, -16.3338, -14.8224, -16.9226, -15.6978, -18.4313, -14.4075,\n",
            "         -14.3182, -23.6194, -13.4723, -13.7397, -13.0104, -17.1285, -15.2903,\n",
            "         -21.9100, -10.4025, -12.8843, -10.9438, -16.1745, -18.7148, -22.9145]])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 120,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t2.600976\n",
            "pred:\t tensor([[-19.0517, -19.2889,  -8.0070,  -6.1041, -11.6249,  -4.3302, -10.9033,\n",
            "          -6.1301, -14.0182,  -9.0622, -16.7241,  -5.1828, -14.2256, -14.6366,\n",
            "          -8.7387, -13.9900, -12.7132, -16.5204, -20.0274, -13.0199, -10.0276,\n",
            "         -13.2037, -17.3039, -21.7312,  -6.1406, -14.3514, -18.7864, -18.8405,\n",
            "         -18.7119, -22.0574, -16.2870, -17.0034, -10.2957,  -9.6444, -10.8524,\n",
            "          -2.6595,  -9.6469,  -9.7279, -12.7820, -15.0540, -11.0989,  -9.5583,\n",
            "         -13.2023, -13.6167, -18.7193, -12.7672, -15.6404, -15.6450,  -8.5766,\n",
            "         -10.8003, -13.8520, -14.3847, -15.3800, -17.1693, -16.6762, -14.4838,\n",
            "         -16.6595, -15.0366, -17.5517, -10.3227, -15.3131, -16.4069, -18.9940,\n",
            "         -16.5434, -10.5603, -13.4058, -12.5797, -15.7980, -17.4103, -16.0938,\n",
            "          -8.9456, -12.3405, -12.2319, -12.9278, -13.5695, -14.3228, -21.5448,\n",
            "         -17.8797, -12.7745, -11.5565, -13.6446, -14.3688, -14.1475, -20.7321,\n",
            "         -13.4344, -13.5005, -14.4845, -14.2430, -15.7051, -22.7049,  -9.4059,\n",
            "         -15.8436, -15.8198, -19.1766, -12.0299, -20.6103,  -8.5254, -22.4842,\n",
            "         -14.9161, -14.6756, -19.3707,  -8.0380, -13.1274, -13.3592, -15.6433,\n",
            "         -14.9526, -13.3363,  -8.6461, -13.9917, -14.1859, -15.7573, -19.0715,\n",
            "         -11.5619, -13.6033, -10.8149, -14.4535, -14.9594, -14.2128, -17.9188,\n",
            "         -22.9785, -17.5744, -13.6044, -11.4961, -16.4046, -11.6385,  -8.2973,\n",
            "         -10.8990, -16.4906, -16.7699,  -9.6621,  -5.8199, -15.6738,  -8.6252,\n",
            "         -12.9870, -19.2702, -17.0858, -12.5379, -14.6636, -17.0937, -17.7184,\n",
            "         -18.5285, -23.6934, -15.6883, -11.7108, -11.6955, -26.9095,  -9.2229,\n",
            "          -6.0376, -10.3476, -13.0412, -20.5689, -11.9664, -16.0120, -12.2047,\n",
            "         -17.3641, -15.9092, -16.6914, -19.0898, -10.0494, -13.4114, -14.5585,\n",
            "         -15.6134, -16.6267, -13.2699,  -9.0672,  -9.0640, -19.3709, -22.3154,\n",
            "         -21.8846, -14.2029, -17.6269,  -9.1701,  -8.4085, -26.8896,  -9.7898,\n",
            "         -20.9970, -19.9520, -14.0057, -17.9635, -16.0656, -20.0677, -16.3028,\n",
            "         -17.5004, -19.0217, -13.4354, -15.6291, -15.4325, -18.0052, -14.3508,\n",
            "         -11.3672, -20.2311, -12.5878, -18.4909, -14.7925, -16.7156, -14.7014,\n",
            "         -14.2442, -15.1207, -21.8377, -20.4058, -16.0839, -25.2821, -18.4136,\n",
            "         -17.5430, -18.6051, -21.8498, -18.0457, -14.2368, -16.7914, -11.6770,\n",
            "         -16.9144, -19.2646, -18.2102, -15.2149, -20.5853, -16.6815, -16.0493,\n",
            "         -12.4407, -15.1925, -15.0800, -14.3957, -15.0967, -15.2561, -16.8566,\n",
            "         -15.3931, -12.0086,  -8.6845, -23.6484, -13.4073, -11.5510, -14.2697,\n",
            "         -22.2679, -15.0181, -18.6058, -13.2008, -16.3553, -14.9400, -10.9040,\n",
            "         -17.6830, -13.8394, -10.4086, -23.0959, -15.8359, -14.3176, -18.3372,\n",
            "         -14.8516, -19.7085, -23.7775, -11.6797, -17.3877, -21.3491, -20.8432,\n",
            "         -16.0918, -14.0738, -16.1127, -17.6722, -16.5482, -14.2339, -19.6703,\n",
            "         -12.4622, -10.9824, -12.3448, -18.9020, -23.4199, -12.6974, -20.1619,\n",
            "         -16.2326, -14.6832, -14.0927, -22.2131, -16.7581, -15.2670, -14.5624,\n",
            "         -22.5931, -16.3331, -16.7346, -22.4800, -23.7826, -15.0031, -16.1924,\n",
            "         -12.1332, -16.9228, -15.9860, -13.4770, -15.9831, -15.2444, -16.4578,\n",
            "         -18.2997, -19.9997, -14.6677, -20.0467, -24.1054, -13.0626, -17.1953,\n",
            "         -18.9565, -16.4427, -19.3842, -16.0894, -20.4183, -12.9722, -14.8327,\n",
            "         -11.0226, -14.8844, -15.6673, -15.7447, -17.3009, -15.1304, -16.2686,\n",
            "         -16.0107, -17.4495, -17.2806, -13.7250, -11.7339, -12.7553, -20.9004,\n",
            "         -11.0623, -12.9229, -16.1733, -12.8988, -19.0425, -15.6761, -18.8747,\n",
            "         -10.9205, -16.4614, -16.8519, -18.1463, -15.2956, -12.9932, -14.2945,\n",
            "         -13.0299, -14.4846, -10.8349, -17.4033, -14.8358, -12.2224, -18.2628,\n",
            "         -19.3415, -14.0575, -22.0413, -15.4164, -19.5319, -16.1884, -19.4714,\n",
            "         -14.4578, -13.6996, -13.9522, -11.5362, -19.0346, -15.1059, -14.0335,\n",
            "         -13.1772, -22.6605, -15.0121, -17.7040, -14.2229, -19.7321, -12.1194,\n",
            "         -10.7424, -21.1937, -14.9204, -14.8788, -12.4439, -22.2609, -16.6000,\n",
            "         -16.4711, -16.9041, -18.0554, -17.4037, -11.3706, -16.7009, -19.4185,\n",
            "         -13.7778, -15.9881, -18.6024, -12.1433, -16.4967, -16.5222, -20.2085,\n",
            "         -19.6982, -17.5766, -17.8020, -13.0140, -19.9123, -15.0620, -14.3893,\n",
            "         -20.1711, -14.9194, -14.3733, -13.5883, -17.9678, -14.6485, -23.9098,\n",
            "         -20.6418, -13.6721, -18.5463, -18.2179, -18.4887, -17.7305, -12.3697,\n",
            "         -18.0108, -16.5439, -14.3511, -17.3036, -14.2672, -17.0413, -16.0111,\n",
            "         -13.1614, -21.2205, -13.5241, -17.5752, -12.9727, -17.4375, -16.0785,\n",
            "         -20.2721, -13.1249, -14.3597, -12.3732, -14.6126, -16.9554, -21.8561]])\n",
            "\n",
            "tgt:\t tensor([171, 262,  82,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.867327\n",
            "pred:\t tensor([[-19.3729, -19.6408,  -8.0941,  -4.8239, -14.6854,  -4.2173, -10.8049,\n",
            "          -6.8591, -12.6728, -10.8401, -16.4340,  -2.0314, -14.7105, -14.1085,\n",
            "          -8.7022, -13.0562, -14.0335, -16.1362, -19.5096, -14.2301, -10.6452,\n",
            "         -13.4454, -16.2959, -23.0293,  -7.6493, -14.2801, -18.8203, -20.3045,\n",
            "         -17.2636, -19.0091, -14.8850, -16.9465,  -8.8948, -10.6644, -11.9680,\n",
            "          -3.4065, -10.9253,  -9.7568, -11.2675, -19.4532, -11.9409, -12.7628,\n",
            "         -14.4974, -16.2447, -19.0102, -14.0659, -14.3478, -16.4920,  -9.4674,\n",
            "         -11.3569, -14.7583, -14.1234, -13.9512, -16.0137, -14.8370, -15.9458,\n",
            "         -16.7841, -13.8963, -16.2958, -11.7943, -12.9192, -17.6706, -20.9748,\n",
            "         -15.1249, -11.9987, -14.9939, -12.9238, -17.2323, -18.7428, -17.0862,\n",
            "         -13.7079, -14.1856, -11.7025, -13.4982, -13.9637, -12.6805, -20.4806,\n",
            "         -17.6340, -13.8069, -13.0876, -13.4008, -12.8125, -16.6590, -20.8622,\n",
            "         -12.8750, -12.8123, -12.1600, -12.7482, -15.5493, -23.0844, -11.0135,\n",
            "         -15.6791, -16.5596, -18.5376, -13.8944, -18.1210, -12.1839, -23.3240,\n",
            "         -14.3854, -11.6009, -17.1827,  -7.8202, -14.5184, -11.3181, -14.2398,\n",
            "         -16.6776, -11.6922, -10.7617, -14.9062, -13.1861, -16.8951, -20.3144,\n",
            "         -10.7478, -12.9820, -14.1594, -16.9354, -14.0319, -14.0984, -14.5463,\n",
            "         -22.2365, -17.7266, -15.1693, -12.8382, -14.7508, -11.5121, -10.3673,\n",
            "         -13.3127, -16.3280, -15.6646, -11.3776,  -9.9459, -17.1978, -10.5094,\n",
            "         -14.4536, -20.1174, -15.9211, -10.0287, -15.7294, -19.0402, -16.7631,\n",
            "         -19.3697, -23.8462, -16.4032, -10.1934, -12.7096, -26.0174, -11.1518,\n",
            "          -7.7977, -10.5160, -14.0305, -19.1651, -14.9653, -18.3973, -13.7723,\n",
            "         -17.9593, -15.9637, -16.9749, -18.6955, -10.3196, -14.5842, -16.0653,\n",
            "         -15.9469, -14.8031, -15.2677,  -9.3724, -11.3663, -18.5670, -26.2678,\n",
            "         -22.8530, -16.5404, -20.2080, -11.7500, -10.8859, -28.3943,  -9.6286,\n",
            "         -23.4679, -20.8987, -13.7449, -18.6206, -19.8974, -23.0465, -20.8768,\n",
            "         -16.7442, -18.2210, -14.4292, -14.8638, -17.0607, -13.2889, -17.4094,\n",
            "         -12.6729, -19.7760, -12.9237, -19.1763, -15.9549, -18.6073, -15.3335,\n",
            "         -15.8898, -15.7892, -20.7933, -19.6148, -20.3529, -23.1823, -16.5204,\n",
            "         -19.9111, -21.8648, -23.9591, -16.5520, -13.1171, -13.9917, -14.1368,\n",
            "         -18.3994, -19.0927, -17.7801, -17.2050, -18.6383, -16.1711, -13.7783,\n",
            "          -9.9427, -15.2993, -15.3734, -13.5586, -14.5280, -15.6973, -17.3172,\n",
            "         -15.4786, -12.0703,  -8.8669, -23.2153, -15.0438,  -9.7866, -10.7980,\n",
            "         -20.7778, -13.1861, -15.1696,  -9.3677, -13.5666, -14.0775, -13.8694,\n",
            "         -14.1814, -13.2602, -15.2709, -23.0528, -13.6554, -14.2258, -17.7758,\n",
            "         -15.6407, -20.1907, -21.5295, -13.0791, -15.6101, -20.1804, -20.5298,\n",
            "         -15.6180, -19.3441, -17.0104, -17.4476, -15.0615, -13.7579, -20.5283,\n",
            "         -11.2386, -11.1192, -12.4981, -19.4810, -24.6539, -16.2710, -21.0682,\n",
            "         -20.2897, -17.7737, -13.4889, -20.6209, -14.4524, -16.2945, -17.7135,\n",
            "         -18.4295, -20.9721, -14.6967, -24.3262, -23.6318, -20.4833, -12.9928,\n",
            "         -16.0876, -20.1014, -14.9165, -12.4095, -15.8204, -14.3564, -19.6906,\n",
            "         -19.7851, -21.7945, -14.3014, -19.5880, -23.6215, -12.0060, -16.4743,\n",
            "         -17.8119, -16.8852, -16.4045, -14.1414, -19.4036, -11.6407, -14.3158,\n",
            "         -13.8314, -15.4089, -16.6682, -15.4313, -13.2349, -15.5844, -16.6086,\n",
            "         -17.4991, -16.8669, -20.4347, -13.0842, -12.4289, -13.5869, -23.7716,\n",
            "         -14.7751, -17.0876, -15.6523, -13.6022, -17.9901, -17.8321, -23.8213,\n",
            "         -11.5488, -19.0322, -18.9478, -14.9277, -14.3811, -10.7840, -15.2117,\n",
            "         -17.2080, -14.7122, -14.0624, -18.3522, -15.7609, -14.8930, -17.0709,\n",
            "         -21.1290, -18.7199, -21.2094, -13.2419, -23.0650, -17.2515, -21.2742,\n",
            "         -14.1820, -12.7846, -16.4592, -12.5862, -21.5977, -13.4862, -13.1932,\n",
            "         -14.4473, -28.0578, -15.8393, -16.1751, -15.4564, -19.2369, -12.4278,\n",
            "         -10.4464, -19.7330, -13.4149, -14.9518, -13.5162, -17.9994, -16.7493,\n",
            "         -13.6684, -18.8891, -17.1548, -18.2540, -11.0573, -18.0053, -19.5626,\n",
            "         -13.9215, -16.1104, -18.8719, -11.8656, -17.3948, -13.8310, -21.9830,\n",
            "         -18.7170, -16.6437, -18.0640, -13.4188, -19.0622, -14.1449, -15.3512,\n",
            "         -17.7934, -12.0138, -16.2274, -15.2305, -20.4886, -15.1971, -21.3494,\n",
            "         -19.6203, -13.1624, -18.5656, -16.7690, -23.1891, -19.0347, -11.5625,\n",
            "         -19.2122, -15.7544, -13.2485, -16.9771, -14.7382, -16.3104, -17.0850,\n",
            "         -12.8014, -22.5453, -14.2034, -17.0443, -14.0267, -20.1272, -17.9010,\n",
            "         -24.8936, -13.7735, -13.6603, -11.1929, -16.4997, -15.0696, -23.2648]])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t2.927133\n",
            "pred:\t tensor([[-19.5581, -19.8322,  -8.5326,  -4.2449, -12.0079,  -4.1447, -10.6625,\n",
            "          -5.0802, -14.2686, -10.3142, -17.2267,  -2.8345, -13.9886, -17.2706,\n",
            "          -8.3464, -12.7045, -14.7012, -16.2453, -20.4098, -14.0858,  -6.9970,\n",
            "         -14.8080, -14.4552, -19.4934,  -8.0078, -13.6462, -20.7745, -19.3906,\n",
            "         -18.4641, -18.8542, -12.5438, -15.8053,  -9.0600, -11.1554, -12.7955,\n",
            "          -4.0486, -10.3430,  -9.6458, -11.9920, -18.8545, -11.5091, -11.4010,\n",
            "         -13.6552, -14.8256, -19.9940, -14.5439, -16.3123, -18.3223,  -9.0439,\n",
            "         -10.8836, -15.3738, -13.8656, -14.7495, -16.2208, -14.1963, -15.4977,\n",
            "         -18.3532, -15.6404, -15.8138, -13.8223, -15.4093, -16.1959, -18.3521,\n",
            "         -14.9285, -11.6709, -14.2613, -14.9832, -18.5073, -17.4719, -14.7208,\n",
            "         -10.3537, -14.8781, -11.3059, -15.9445, -14.9993, -11.4778, -19.8774,\n",
            "         -16.2465, -13.8753, -12.5015, -12.3911, -12.4924, -17.0202, -21.3903,\n",
            "         -15.2087, -12.9448, -15.4673, -16.1212, -14.6348, -24.0809, -12.3628,\n",
            "         -17.7898, -17.5788, -21.2133, -13.6982, -20.2873, -11.7861, -21.3713,\n",
            "         -13.4088, -13.2920, -17.2678,  -6.6736, -13.6231,  -8.3083, -16.2783,\n",
            "         -17.0916, -11.4823, -11.9180, -14.7818, -14.6176, -18.1634, -20.2294,\n",
            "         -11.1728, -12.4200, -15.4815, -18.0563, -14.3103, -15.1813, -14.6154,\n",
            "         -22.9803, -16.6535, -14.4802, -12.4812, -14.6092, -12.6988, -10.0056,\n",
            "         -12.1149, -14.9745, -17.2819, -10.2225,  -8.5114, -18.1132,  -8.8100,\n",
            "         -13.1272, -17.3574, -17.3502, -10.9888, -15.0804, -16.5114, -18.0162,\n",
            "         -20.5707, -23.3646, -15.5391, -13.0604, -13.3888, -26.7714, -10.9645,\n",
            "          -6.5912, -13.1280, -14.7864, -18.5176, -13.6048, -15.5996, -10.0904,\n",
            "         -19.2109, -15.9139, -16.8787, -15.9931,  -9.6402, -13.2642, -15.6579,\n",
            "         -19.8943, -17.2366, -14.5599, -11.1028,  -8.5131, -20.2046, -24.9276,\n",
            "         -23.4477, -18.3699, -20.6761, -11.4869, -10.7896, -27.3268, -10.0263,\n",
            "         -20.1093, -21.7967, -14.7625, -19.7460, -18.2712, -21.4105, -18.9824,\n",
            "         -17.1494, -20.8146, -14.6510, -17.6015, -17.0065, -14.8078, -16.2263,\n",
            "         -13.3274, -19.8838, -14.8816, -18.0939, -18.3045, -18.7462, -15.1399,\n",
            "         -18.4134, -19.0107, -20.2075, -20.0434, -19.4930, -25.2666, -16.4037,\n",
            "         -19.8208, -23.8969, -23.4780, -16.0637, -12.9633, -19.2674, -13.9855,\n",
            "         -19.7235, -19.4601, -17.0329, -16.0311, -19.2719, -19.4588, -15.4050,\n",
            "         -12.4673, -17.1214, -15.3437, -11.0874, -15.7208, -15.1944, -16.9829,\n",
            "         -14.1529, -14.5869, -11.5406, -22.5225, -13.4096, -13.1099, -12.1545,\n",
            "         -23.5948, -12.7889, -16.1175, -10.6179, -12.9952, -17.2267, -15.3345,\n",
            "         -17.1876, -14.7606, -10.5731, -24.6706, -14.1350, -13.3018, -18.3952,\n",
            "         -16.5125, -20.9716, -25.1652, -12.8552, -17.6325, -20.8323, -17.1009,\n",
            "         -14.7690, -19.9280, -19.5635, -17.3412, -13.6523, -14.1096, -24.0093,\n",
            "         -12.1579, -14.6854, -11.9638, -21.8394, -23.3578, -14.6481, -20.9851,\n",
            "         -19.5518, -17.1117, -13.4958, -25.0218, -14.6061, -17.0174, -17.4476,\n",
            "         -19.5687, -22.5552, -15.9374, -23.7867, -24.9912, -19.3191, -15.9954,\n",
            "         -13.2586, -19.2098, -16.7913, -16.7168, -16.5977, -14.5349, -18.1149,\n",
            "         -19.1462, -22.6068, -14.0232, -22.5887, -25.0070,  -9.6520, -17.2397,\n",
            "         -20.4608, -15.8620, -21.0057, -16.0354, -17.3930, -13.7657, -14.2961,\n",
            "         -14.1110, -17.0698, -14.9682, -16.1981, -14.9152, -12.6136, -16.3726,\n",
            "         -15.1535, -15.7271, -18.7048, -13.2754, -15.5358, -14.5945, -23.6117,\n",
            "         -13.4154, -15.5216, -10.8666, -14.4764, -21.0216, -18.8675, -23.5976,\n",
            "          -9.5078, -19.0635, -18.2225, -17.2986, -15.4321, -11.1183, -18.5506,\n",
            "         -14.6589, -15.1448, -12.5277, -18.5319, -15.3975, -14.8861, -16.8008,\n",
            "         -21.6017, -19.9474, -21.5662, -15.4993, -23.0714, -18.2517, -20.4185,\n",
            "         -15.2661, -11.9762, -14.8009, -12.8687, -22.1785, -16.2934, -12.8157,\n",
            "         -14.5770, -27.4700, -13.7099, -14.3322, -15.5642, -21.3096, -12.7045,\n",
            "         -11.0920, -21.6612, -16.1035, -17.8212, -15.5496, -21.2224, -15.1051,\n",
            "         -16.1111, -17.0951, -16.8152, -15.3023,  -9.9991, -14.8581, -23.0418,\n",
            "         -15.2487, -18.1553, -20.1647, -12.8217, -16.6232, -12.6855, -21.5026,\n",
            "         -18.3106, -15.0257, -16.6798, -18.1553, -15.6101, -16.8503, -12.1633,\n",
            "         -16.6735, -13.1639, -16.5640, -16.4480, -18.0135, -13.2042, -22.2920,\n",
            "         -22.3260, -13.8002, -19.2981, -15.0626, -16.2184, -18.9353, -14.8521,\n",
            "         -17.3138, -16.2996, -12.8719, -17.6865, -15.7836, -16.3134, -14.3943,\n",
            "         -14.3644, -23.1495, -12.5543, -13.0589, -13.2804, -18.5104, -20.1247,\n",
            "         -24.2269, -13.3423, -12.2290, -13.2962, -17.1571, -17.9405, -23.0422]])\n",
            "\n",
            "tgt:\t tensor([ 14,  28, 131,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t4.180449\n",
            "pred:\t tensor([[-19.6669, -19.9367,  -8.0689,  -5.4569,  -9.5270,  -2.0123,  -9.6574,\n",
            "          -4.1485, -13.0809,  -9.0207, -18.6077,  -5.7657, -14.7597, -14.7656,\n",
            "         -10.7338, -16.6040, -12.6392, -16.8999, -21.1983, -13.6040,  -9.7085,\n",
            "         -12.6059, -17.7306, -19.3550,  -9.5679, -12.6219, -17.9031, -17.4761,\n",
            "         -21.0234, -21.2981, -16.2549, -19.2002,  -9.9336, -10.3398, -12.1326,\n",
            "          -2.7606, -12.0388, -10.6173, -12.9322, -20.2029, -11.7430, -11.4127,\n",
            "         -15.8781, -16.0512, -19.6710, -13.5923, -17.1411, -15.2582,  -7.7696,\n",
            "         -10.4075, -14.8534, -17.3419, -16.7353, -15.2159, -15.8127, -14.0916,\n",
            "         -16.4126, -15.3575, -16.1835, -14.5451, -12.5065, -15.9610, -20.5126,\n",
            "         -14.6470, -10.3789, -10.5236, -16.9704, -12.6624, -19.6394, -15.5197,\n",
            "         -10.7609, -14.1742, -10.4552, -13.4272, -13.8171, -14.9502, -18.6076,\n",
            "         -15.2725, -12.6069, -11.2873, -10.5181, -13.6286, -18.0253, -19.1849,\n",
            "         -16.2475, -13.0041, -14.7541, -15.2563, -16.0261, -23.5463, -11.3221,\n",
            "         -16.0939, -18.4734, -23.1708, -15.0095, -21.3417, -12.6321, -18.7616,\n",
            "         -15.1891, -12.1389, -16.3528,  -8.0897, -10.4556,  -9.9956, -16.3149,\n",
            "         -16.0417, -11.8532, -11.5715, -15.2731, -14.2134, -15.6922, -18.8246,\n",
            "          -8.3347, -11.3977, -14.6617, -17.0235, -15.5671, -13.4842, -14.3921,\n",
            "         -21.5182, -17.8371, -16.2221, -12.4718, -18.3061, -13.6115,  -9.1281,\n",
            "         -13.0448, -15.6994, -16.1776, -10.7818,  -7.4822, -17.8781, -10.4518,\n",
            "         -13.9687, -19.8970, -17.7560, -10.9872, -14.2471, -17.6857, -16.8757,\n",
            "         -19.0188, -23.9995, -13.1316,  -7.6913, -13.0938, -24.1640,  -5.7248,\n",
            "          -5.4591,  -8.7355, -15.2244, -20.7905, -13.4823, -15.6370, -12.8384,\n",
            "         -19.3561, -18.6342, -18.3215, -17.6961,  -7.5632, -13.0445, -13.4705,\n",
            "         -16.1465, -17.1248, -11.9957, -11.5316, -10.9813, -19.3299, -23.1245,\n",
            "         -22.0672, -14.1933, -18.4699, -13.2928,  -9.2739, -27.9144, -10.8846,\n",
            "         -21.2804, -20.5582, -16.4351, -19.1546, -17.1012, -20.0325, -21.1215,\n",
            "         -17.0442, -16.9675, -10.5186, -15.9068, -17.6120, -18.9689, -15.9288,\n",
            "         -11.8272, -18.1940, -15.2903, -17.5990, -18.2087, -19.6077, -19.6527,\n",
            "         -21.5393, -21.2047, -21.8761, -20.8888, -17.6214, -24.3760, -15.6646,\n",
            "         -20.2077, -20.5216, -21.8196, -19.2780, -14.9110, -17.0393, -15.0536,\n",
            "         -17.4692, -18.9580, -19.7317, -16.0567, -19.8088, -17.8761, -17.4282,\n",
            "         -13.0006, -16.5771, -17.0777, -12.1839, -16.2277, -14.2334, -15.6829,\n",
            "         -16.5895, -11.2890,  -9.0766, -24.5208, -14.9214,  -9.1154, -10.8054,\n",
            "         -21.3900, -17.6768, -21.2713, -11.4680, -13.9504, -16.8959, -16.2883,\n",
            "         -17.2561, -10.7471, -12.5980, -20.6952, -14.5732, -16.3371, -17.1032,\n",
            "         -13.3225, -20.6174, -21.0597, -10.9289, -14.7586, -23.2521, -20.8872,\n",
            "         -16.8452, -17.6435, -22.1746, -16.3477, -17.1378, -14.8068, -20.6169,\n",
            "         -11.0477, -11.5864, -12.8067, -19.2408, -25.5207, -14.5629, -19.4754,\n",
            "         -18.6052, -17.3321, -16.8720, -21.6364, -11.9775, -17.2991, -18.5939,\n",
            "         -22.4047, -16.2936, -12.7511, -22.2321, -22.4191, -18.2238, -13.9546,\n",
            "         -14.0492, -16.7522, -15.2261, -12.9246, -16.0552, -14.0101, -20.6077,\n",
            "         -21.9643, -24.5941, -14.6393, -24.0405, -21.5560, -13.4349, -17.1772,\n",
            "         -22.8809, -17.6599, -21.1005, -14.3933, -19.3342, -16.4388, -14.5501,\n",
            "         -13.0127, -17.2684, -13.4807, -16.0416, -16.2307, -14.8867, -16.0770,\n",
            "         -13.9576, -16.5547, -18.2932, -14.3620, -12.1560, -13.4930, -22.0297,\n",
            "         -15.3773, -14.1838, -17.3798, -14.4784, -19.5257, -15.4486, -21.7064,\n",
            "         -11.5462, -17.8756, -17.2049, -17.3467, -16.7539, -13.1046, -13.5101,\n",
            "         -15.9717, -14.0611, -11.7744, -18.5077, -18.0858, -14.2945, -15.5294,\n",
            "         -20.1287, -17.7120, -23.2426, -13.7819, -20.9215, -17.3862, -18.3351,\n",
            "         -17.4488, -13.0498, -15.2929, -15.3183, -20.1710, -13.6668, -13.9883,\n",
            "          -8.7422, -27.7246, -15.4662, -16.8417, -15.3343, -19.4261, -12.9598,\n",
            "         -12.6870, -22.5201, -14.9339, -15.1673, -15.5870, -23.6087, -19.7474,\n",
            "         -16.4910, -17.8697, -20.2618, -17.7829, -14.3744, -15.1485, -20.9809,\n",
            "         -16.9847, -18.6418, -16.8772, -12.1706, -17.4423, -15.2630, -20.3650,\n",
            "         -20.3270, -17.1208, -21.1861, -16.9671, -19.7072, -14.4546, -14.5284,\n",
            "         -15.5624, -14.3257, -15.7974, -15.8371, -21.0410, -13.2348, -24.2588,\n",
            "         -21.4533, -14.8108, -19.5962, -16.1185, -22.2068, -18.6927, -14.0488,\n",
            "         -19.1900, -16.4014, -14.9421, -18.1864, -14.8108, -18.8596, -10.4236,\n",
            "         -14.0962, -22.0396, -14.3247, -15.1185, -14.2460, -19.6734, -15.8705,\n",
            "         -22.2595, -13.7775, -13.3381, -14.7502, -17.5320, -14.9130, -18.9461]])\n",
            "\n",
            "tgt:\t tensor([ 4, 73,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t3.398884\n",
            "pred:\t tensor([[-19.9194, -20.2188,  -9.6016,  -5.5742, -12.4426,  -7.3217,  -9.8957,\n",
            "          -6.2042, -13.0967, -10.4595, -16.3692,  -3.0668, -16.8204, -17.5631,\n",
            "          -9.2530, -11.8056, -15.2277, -15.0376, -22.2433, -14.1006, -10.9221,\n",
            "         -15.4742, -15.9112, -23.7573, -11.1528, -14.2112, -19.4542, -22.1846,\n",
            "         -20.5543, -17.9587, -14.0606, -16.0007,  -9.4661, -11.1076, -13.8448,\n",
            "          -3.3603, -10.2519,  -9.3737, -11.8199, -16.3829, -13.5919, -10.8165,\n",
            "         -13.2350, -16.0290, -19.1830, -14.8171, -14.5149, -16.7936, -10.2675,\n",
            "         -12.7455, -12.8563, -19.4835, -14.3702, -17.1999, -16.0954, -17.2078,\n",
            "         -20.3029, -15.2564, -17.3182, -17.7215, -15.2124, -18.3694, -22.3584,\n",
            "         -13.8084, -11.3310, -10.9318, -16.0855, -13.8118, -17.7183, -17.2969,\n",
            "         -14.5682, -15.0235, -10.9293, -17.1603, -14.4666, -14.2462, -21.6001,\n",
            "         -16.2908, -13.3853, -13.5492, -12.3687, -13.3110, -18.5262, -22.5758,\n",
            "         -15.3592, -11.2584, -13.5886, -15.5396, -12.8478, -20.5102, -12.4548,\n",
            "         -15.9591, -18.8573, -18.9429, -13.5260, -18.8306,  -9.2644, -22.2393,\n",
            "         -15.6119, -15.9780, -14.7680,  -9.4773, -16.5614,  -7.2716, -15.1403,\n",
            "         -18.5379, -11.0956, -12.8516, -16.3026, -17.6027, -16.2728, -21.0369,\n",
            "         -10.3320, -14.5734, -12.0925, -18.9328, -15.4342, -13.6540, -15.4653,\n",
            "         -20.4938, -15.9214, -12.7841, -13.6244, -16.1964, -13.3376, -11.7471,\n",
            "         -12.6306, -16.7419, -16.5744, -12.0055,  -9.9814, -18.3308, -11.1296,\n",
            "         -12.4288, -17.5352, -20.4596, -12.1400, -16.8967, -19.3398, -17.0907,\n",
            "         -22.0091, -22.1775, -19.3052, -10.5053, -11.7095, -22.8009,  -9.6847,\n",
            "          -6.5741, -13.2403, -12.3422, -19.5321, -14.3236, -19.8825, -14.2335,\n",
            "         -19.2499, -17.7580, -18.3944, -17.4948, -10.4030, -14.2839, -14.6678,\n",
            "         -18.9338, -17.0658, -12.1863, -11.5946,  -9.9019, -19.0369, -26.2723,\n",
            "         -21.1514, -16.8601, -19.3822, -13.7741, -10.6134, -27.3503, -12.3315,\n",
            "         -20.5068, -19.0407, -14.5106, -18.5908, -16.4653, -22.4568, -19.6933,\n",
            "         -18.1669, -18.1243, -15.6074, -16.3858, -15.1163, -15.0964, -16.9590,\n",
            "         -14.4119, -18.6451, -16.0397, -20.0031, -17.2519, -21.2127, -13.7011,\n",
            "         -22.7533, -16.1660, -19.4530, -19.1991, -17.8249, -23.8947, -16.6973,\n",
            "         -20.9941, -20.7663, -23.6314, -14.5194, -13.1243, -17.9546, -18.0414,\n",
            "         -20.2035, -18.9001, -15.7752, -15.2261, -16.0893, -16.2206, -17.1126,\n",
            "         -11.8833, -15.6212, -15.6557, -13.8064, -15.4574, -15.1891, -18.1151,\n",
            "         -19.2130, -13.1346,  -9.1958, -23.5909, -16.9225,  -8.8419, -11.1737,\n",
            "         -22.2710, -16.9212, -17.6862, -13.9530, -13.5014, -13.5656, -16.1195,\n",
            "         -15.9385, -15.6844, -12.9463, -24.1798, -14.0284, -16.5365, -21.6457,\n",
            "         -15.8227, -21.5367, -20.2515, -14.0607, -18.1967, -23.0737, -21.1118,\n",
            "         -19.8007, -17.8389, -21.8329, -17.4195, -14.9876, -16.7068, -17.5512,\n",
            "         -12.3793, -13.5438, -12.0059, -22.9640, -26.8694, -14.3781, -21.5956,\n",
            "         -20.4331, -17.6329, -13.4890, -21.0284, -13.9644, -17.7134, -18.1901,\n",
            "         -23.5113, -17.5436, -15.1109, -23.5988, -22.1999, -19.4155, -14.4250,\n",
            "         -14.2780, -21.5121, -15.8151, -15.2687, -14.8189, -17.2768, -17.9669,\n",
            "         -20.2035, -19.5549, -14.0792, -18.1349, -22.5666, -11.4015, -19.5103,\n",
            "         -19.5284, -11.9533, -19.2695, -15.4457, -19.6583, -13.9010, -16.7103,\n",
            "         -13.8334, -16.7874, -13.4897, -14.0717, -16.5806, -14.2502, -15.1181,\n",
            "         -13.8943, -18.4958, -18.1457, -13.0653, -12.0682, -12.9344, -25.4820,\n",
            "         -13.5706, -17.7155, -15.7477, -13.9902, -18.2451, -16.5302, -23.7181,\n",
            "         -10.7778, -21.2607, -15.6811, -13.2637, -13.6077, -13.6334, -17.9928,\n",
            "         -16.0709, -17.1451, -13.4847, -15.5145, -15.4097, -12.8055, -17.0715,\n",
            "         -22.1252, -19.1221, -22.8750, -13.6256, -20.5991, -18.3126, -19.4235,\n",
            "         -16.0888, -10.4647, -17.3259, -15.8212, -19.6958, -13.3901, -13.6675,\n",
            "         -12.4860, -26.8836, -15.0384, -14.8554, -15.9247, -20.7862, -11.9183,\n",
            "         -14.3767, -22.8276, -15.7787, -14.4265, -14.1031, -16.4630, -15.4917,\n",
            "         -11.9175, -15.9462, -17.3613, -13.0224, -12.3915, -16.1058, -22.8742,\n",
            "         -15.0291, -18.1207, -16.5702, -11.6245, -14.5724, -16.3850, -17.8262,\n",
            "         -22.5559, -13.1889, -16.4569, -15.1123, -16.9656, -13.6245, -15.4321,\n",
            "         -20.4692, -13.4841, -12.1535, -15.8700, -15.9862, -17.9487, -21.1971,\n",
            "         -19.1968, -14.0989, -17.5308, -19.4294, -20.1924, -15.6525, -14.7859,\n",
            "         -19.7396, -17.0331, -13.5586, -17.2989, -14.4046, -19.5340, -15.8791,\n",
            "         -13.4748, -22.6459, -13.5412, -13.8132, -16.0424, -18.8417, -17.6752,\n",
            "         -25.3285, -17.0498, -14.0445, -14.0376, -18.7671, -16.7528, -18.7869]])\n",
            "\n",
            "tgt:\t tensor([15, 31, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t2.094988\n",
            "pred:\t tensor([[-20.0506, -20.2470,  -8.5401,  -5.5230, -11.8737,  -0.2583,  -9.5583,\n",
            "          -5.6976, -13.3317,  -9.3301, -20.1387,  -8.6982, -15.4814, -15.3990,\n",
            "         -11.0295, -16.9839, -14.3572, -18.4005, -20.8420, -13.7777, -10.4534,\n",
            "         -14.3497, -18.1424, -23.4523,  -8.4143, -13.5358, -19.0807, -20.4107,\n",
            "         -21.9907, -21.8733, -14.3405, -23.0537, -11.2896,  -9.8939, -12.8828,\n",
            "          -3.5487, -10.2230, -10.3518, -14.7829, -18.3203, -11.8805,  -8.7368,\n",
            "         -15.7647, -15.1992, -18.5314, -12.5350, -14.7503, -15.8739,  -9.2566,\n",
            "         -11.5466, -16.8903, -14.9526, -16.2065, -14.5073, -13.9204, -13.6293,\n",
            "         -16.7199, -14.4189, -15.5607, -13.2190, -16.7812, -15.1072, -21.6554,\n",
            "         -13.6050, -11.5886, -12.8841, -12.2549, -14.1798, -20.6439, -15.3043,\n",
            "          -8.4785, -15.3615, -11.3163, -13.1590, -15.1581, -15.7548, -16.5089,\n",
            "         -13.5127, -13.8761, -10.9772,  -8.6733, -13.8559, -16.1420, -20.9270,\n",
            "         -14.1710, -12.5703, -17.9037, -16.9018, -15.7933, -24.9402, -12.4040,\n",
            "         -15.3368, -18.0906, -21.1907, -13.9280, -23.8526, -11.9203, -17.9178,\n",
            "         -16.6550, -12.8599, -18.4582,  -8.4385, -14.2306, -11.5467, -15.7100,\n",
            "         -17.2813, -12.0903, -11.2709, -15.9630, -14.3923, -18.0205, -15.3740,\n",
            "          -9.8072, -12.4691, -12.2648, -18.4628, -16.1588, -15.4924, -16.1960,\n",
            "         -24.3332, -20.5792, -16.2293, -13.5378, -18.5446, -13.4586,  -9.1088,\n",
            "         -18.8333, -20.4815, -16.8452, -10.9913,  -7.5409, -19.8609,  -8.2823,\n",
            "         -18.8547, -18.5449, -19.3465,  -9.5218, -14.6537, -19.6743, -16.9646,\n",
            "         -21.1496, -25.2999, -16.8258, -12.1891,  -9.6862, -25.8016,  -8.8298,\n",
            "          -3.6726, -14.4806, -13.5608, -20.1628, -13.3054, -15.8138, -14.0142,\n",
            "         -18.8973, -17.5625, -20.1277, -19.2505, -10.6875, -16.2052, -16.8379,\n",
            "         -17.1058, -15.3585, -13.6550, -10.2034,  -8.9094, -18.5379, -20.7490,\n",
            "         -20.7910, -16.2164, -19.1455, -12.7813, -10.3585, -28.2719,  -8.0800,\n",
            "         -24.2510, -21.4597, -15.1858, -22.3518, -17.3984, -26.1117, -20.8267,\n",
            "         -16.7782, -18.0919, -12.3785, -18.5138, -15.6731, -19.8898, -15.4254,\n",
            "         -13.8347, -20.1679, -12.8216, -18.9694, -18.5486, -19.4763, -18.7274,\n",
            "         -16.4948, -17.4790, -23.2090, -24.6693, -16.4240, -29.7113, -17.8441,\n",
            "         -19.7044, -22.0237, -24.0329, -21.3992, -15.4696, -16.4150, -13.2730,\n",
            "         -18.7866, -21.7225, -20.5234, -18.2291, -19.1535, -19.6825, -14.4762,\n",
            "         -12.6642, -19.4678, -16.4946, -13.6386, -15.9192, -17.1813, -18.3763,\n",
            "         -20.2926, -12.2325, -10.2800, -24.7139, -16.6881, -10.8424, -12.3399,\n",
            "         -24.3525, -16.4422, -20.1127, -13.7751, -15.7759, -18.7224, -13.5487,\n",
            "         -16.2239, -14.3939,  -9.8848, -22.6798, -16.9858, -15.9596, -20.1195,\n",
            "         -14.3996, -21.3394, -23.2846, -11.8049, -16.7721, -21.6906, -22.4823,\n",
            "         -17.9926, -21.8543, -21.0273, -18.2346, -14.5875, -13.6956, -18.2316,\n",
            "         -11.4499, -14.2215, -11.2184, -20.5229, -25.2075, -13.2259, -22.4420,\n",
            "         -16.4297, -17.3212, -16.3990, -21.0751, -14.5511, -14.6148, -15.1260,\n",
            "         -24.9647, -15.2635, -12.8597, -20.1372, -25.2882, -20.8391, -14.8090,\n",
            "         -14.3015, -17.2597, -15.0733, -15.2201, -16.7466, -16.9673, -18.0280,\n",
            "         -20.2308, -22.7445, -15.8413, -21.5343, -25.0207, -14.4109, -15.4879,\n",
            "         -22.8444, -15.8473, -18.8766, -15.8421, -21.8561, -13.5423, -15.4338,\n",
            "         -12.5527, -14.8248, -16.9303, -16.4885, -17.9217, -14.6725, -16.0134,\n",
            "         -16.3715, -18.9047, -19.5065, -12.9519, -11.4342, -15.6067, -22.0280,\n",
            "         -14.9397, -16.8242, -16.6681, -16.1924, -21.9574, -17.9645, -23.2075,\n",
            "          -9.6384, -17.2695, -18.5084, -19.2021, -14.8918, -14.0451, -14.3449,\n",
            "         -15.1099, -14.3826, -14.9406, -18.3761, -16.5193, -14.0094, -17.5582,\n",
            "         -20.4996, -17.7492, -25.7112, -14.5133, -20.3109, -18.2142, -20.2598,\n",
            "         -17.9197, -16.6864, -16.9792, -13.6049, -24.2403, -15.7485, -14.9421,\n",
            "         -12.5373, -28.5682, -14.9793, -20.3747, -17.1067, -16.5498, -13.9197,\n",
            "         -11.8427, -27.4180, -13.6579, -19.9851, -18.4269, -21.7584, -17.7643,\n",
            "         -14.9944, -17.9699, -21.7831, -16.5945, -15.0287, -16.1164, -23.4257,\n",
            "         -13.4068, -16.3462, -18.7618, -12.2103, -17.9558, -18.4166, -21.6864,\n",
            "         -24.0483, -17.1519, -21.1774, -15.7159, -19.6026, -17.4464, -14.2366,\n",
            "         -14.9430, -11.8669, -12.7264, -14.5581, -19.1460, -13.2450, -27.7762,\n",
            "         -20.3384, -13.2521, -21.6958, -17.8525, -19.6172, -19.8727, -13.5494,\n",
            "         -16.9037, -17.1266, -13.9746, -20.1901, -16.9533, -20.6334, -14.2243,\n",
            "         -15.8825, -21.1730, -12.6655, -16.2217, -12.9816, -20.0448, -19.9015,\n",
            "         -21.5444, -12.9015, -16.8151, -15.1819, -14.9860, -13.6455, -21.4202]])\n",
            "\n",
            "tgt:\t tensor([79,  3, 41,  5,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t3.741671\n",
            "pred:\t tensor([[-20.0550, -20.2250, -10.5183,  -3.6564, -14.9682,  -3.2300, -11.4787,\n",
            "          -7.4920, -11.8431, -11.6721, -17.3640,  -0.3242, -15.4899, -16.3831,\n",
            "          -9.3062, -13.3618, -16.5385, -14.6220, -21.1851, -11.3458, -13.3231,\n",
            "         -14.6580, -17.2738, -23.6285, -11.2452, -14.1006, -19.7682, -19.8895,\n",
            "         -21.6402, -20.1502, -12.7120, -16.5550, -12.8111, -11.0210, -13.1791,\n",
            "          -4.2486, -15.1894, -10.7744, -11.7394, -21.6429, -13.5188, -12.1125,\n",
            "         -12.7802, -15.1233, -18.4066, -16.5910, -17.3487, -13.1532, -10.5739,\n",
            "         -12.8447, -17.0453, -15.4471, -15.5700, -17.6170, -15.4888, -17.8905,\n",
            "         -16.2258, -14.9145, -15.0437, -15.9956, -14.2372, -17.7897, -21.2483,\n",
            "         -13.1331, -11.5733, -13.7719, -15.0211, -15.7308, -20.9021, -18.2705,\n",
            "          -9.7544, -14.1904, -11.5666, -15.7667, -15.4198, -14.8930, -22.6191,\n",
            "         -14.8980, -12.7875, -14.2258, -12.0829, -14.5406, -17.6002, -21.3806,\n",
            "         -14.9711, -13.5314, -13.5666, -15.7117, -14.8440, -24.5717, -12.8364,\n",
            "         -15.0572, -17.2244, -23.6836, -12.1899, -21.5085,  -9.9019, -19.2783,\n",
            "         -16.6385, -13.8549, -17.0013,  -8.4830, -13.7647,  -9.8457, -19.4644,\n",
            "         -15.5661, -11.7759, -12.3768, -16.2744, -16.3883, -15.3999, -20.9631,\n",
            "         -10.5359, -16.1501, -14.5062, -17.1713, -15.2595, -18.5988, -16.6782,\n",
            "         -19.7474, -15.6444, -17.6476, -16.3417, -16.4925, -13.9426, -13.7957,\n",
            "         -14.3122, -16.7227, -18.0207, -12.5764, -10.3326, -18.8875, -12.7120,\n",
            "         -14.1819, -18.1895, -15.6611, -13.7699, -17.3953, -20.2223, -18.3915,\n",
            "         -21.8427, -22.1629, -17.6884, -12.5636, -12.0494, -26.5217, -11.3995,\n",
            "          -9.9793, -10.9893, -14.7054, -17.1498, -16.3666, -20.7081, -13.8089,\n",
            "         -18.8222, -16.0451, -16.4111, -14.5677, -11.5089, -14.6670, -15.7087,\n",
            "         -16.8681, -17.2641, -14.3743, -12.8236, -12.0573, -20.0030, -26.5000,\n",
            "         -26.9815, -14.9654, -23.1074, -13.1304, -13.5609, -31.3046, -12.8653,\n",
            "         -22.8132, -23.2492, -16.6503, -22.1470, -20.5186, -24.5221, -18.2403,\n",
            "         -14.4058, -19.8640, -15.0523, -15.9560, -16.2822, -16.1755, -17.2659,\n",
            "         -14.2161, -22.9314, -13.0243, -15.3211, -17.3383, -20.0723, -16.3287,\n",
            "         -19.0575, -16.5892, -20.0696, -19.9064, -18.2675, -23.5542, -17.1668,\n",
            "         -21.5189, -24.0911, -23.6438, -17.0931, -15.6976, -16.7943, -13.9653,\n",
            "         -20.7656, -15.8351, -16.0098, -15.5750, -17.8855, -17.9810, -14.9608,\n",
            "         -11.9647, -17.8170, -15.5247, -14.0257, -17.4099, -16.2655, -15.5683,\n",
            "         -19.1908, -12.0770, -11.7577, -25.5063, -16.2423, -10.8261, -11.9054,\n",
            "         -24.0706, -11.5212, -19.3492, -11.9801, -14.0232, -16.2683, -14.6168,\n",
            "         -15.9629, -14.1713, -15.2959, -20.9559, -13.8090, -13.9579, -16.5891,\n",
            "         -13.9672, -19.7642, -23.8879, -11.5290, -17.8250, -19.7102, -19.9292,\n",
            "         -17.8996, -21.1637, -19.1821, -18.7738, -12.6411, -15.4456, -22.9714,\n",
            "         -11.1959, -15.9429, -12.0706, -18.2302, -24.8605, -11.8922, -22.9708,\n",
            "         -20.9579, -17.9544, -12.8684, -25.8909, -13.1864, -17.6715, -15.5301,\n",
            "         -22.7134, -18.2157, -15.6035, -24.0557, -24.5437, -18.1928, -15.8104,\n",
            "         -15.0918, -17.4930, -17.3819, -13.9868, -19.5416, -16.1833, -19.4338,\n",
            "         -19.3246, -18.3161, -12.6192, -17.0714, -21.2709,  -9.2085, -20.1778,\n",
            "         -20.1746, -16.9423, -18.8427, -13.3914, -20.0806, -12.3226, -14.9541,\n",
            "         -15.0588, -18.4628, -14.3408, -16.0189, -18.7949, -15.2617, -12.8274,\n",
            "         -17.2084, -17.1539, -18.6897, -14.1229, -12.6868, -16.1879, -24.5869,\n",
            "         -15.8548, -19.2467, -13.0904, -14.4138, -17.8391, -18.4711, -27.2985,\n",
            "         -11.4888, -18.8638, -17.1128, -19.3825, -16.4595, -10.1063, -19.5266,\n",
            "         -13.2493, -15.9891, -14.5239, -19.7372, -17.3680, -15.1492, -20.4593,\n",
            "         -25.7244, -21.9942, -19.4622, -16.2469, -21.0201, -19.3218, -21.9996,\n",
            "         -15.5917,  -8.7687, -16.2491, -15.2272, -25.5376, -14.1324, -15.7768,\n",
            "         -13.3345, -28.7607, -14.3386, -19.6439, -16.3919, -19.1049, -14.1741,\n",
            "         -10.2235, -24.4952, -15.5143, -16.0583, -16.1098, -22.0282, -20.3319,\n",
            "         -14.4489, -19.8661, -21.6074, -15.7652, -10.1776, -19.5877, -22.1428,\n",
            "         -13.0184, -15.7428, -19.6986, -13.6637, -17.7880, -15.8145, -18.2929,\n",
            "         -21.0379, -14.2463, -16.1236, -19.5782, -16.0162, -19.3039, -14.5555,\n",
            "         -18.2677, -16.0619, -17.8881, -16.5098, -23.6750, -19.7057, -22.4751,\n",
            "         -21.3917, -17.1466, -17.7906, -15.7828, -22.4799, -17.7012, -13.2031,\n",
            "         -20.5557, -17.8995, -16.0218, -18.2878, -15.5995, -16.7281, -13.7171,\n",
            "         -13.1063, -21.1991, -14.5976, -15.3513, -16.7185, -19.6450, -19.6691,\n",
            "         -26.8085, -13.9413, -14.8817, -14.3446, -18.9722, -17.4658, -20.0551]])\n",
            "\n",
            "tgt:\t tensor([ 14, 211,  37,  74,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t2.523928\n",
            "pred:\t tensor([[-20.5382, -20.8594,  -9.8113,  -4.5795, -14.4633,  -3.0066, -13.0916,\n",
            "          -7.9130, -13.7930, -12.6717, -15.1428,  -2.0670, -16.2917, -17.9577,\n",
            "          -9.8846, -12.1079, -16.0515, -14.1770, -20.9053, -15.4591, -11.3759,\n",
            "         -12.7419, -18.1392, -23.0689, -11.0887, -16.7551, -22.3891, -21.3766,\n",
            "         -20.9625, -21.2623, -13.7778, -16.9366,  -8.6814, -13.9033, -14.4208,\n",
            "          -5.6590, -12.3447, -11.8162, -11.5173, -19.3132, -15.4989, -12.9650,\n",
            "         -15.7137, -18.2698, -19.9333, -13.0111, -14.0003, -18.8617, -12.5474,\n",
            "         -13.0451, -14.2857, -20.0982, -18.3362, -18.6264, -16.6466, -16.4130,\n",
            "         -21.1073, -15.0924, -18.5136, -18.2307, -15.0496, -19.7508, -24.5023,\n",
            "         -14.7683, -13.1058, -16.6147, -14.9815, -12.4658, -21.0834, -17.2119,\n",
            "         -11.4224, -14.2938, -12.6928, -15.3272, -16.2728, -12.5157, -21.9150,\n",
            "         -13.9070, -13.2953, -17.7247, -10.9557, -15.1468, -20.4407, -22.5723,\n",
            "         -14.3471, -14.3924, -15.5202, -16.4811, -14.8375, -21.6012, -15.0346,\n",
            "         -15.9142, -21.3735, -20.9265, -13.8548, -21.2918, -14.8373, -22.0876,\n",
            "         -16.2904, -14.6813, -21.2287, -10.5863, -15.4485, -10.6729, -16.6544,\n",
            "         -15.5683, -11.7227, -14.7567, -17.7152, -13.6730, -17.5235, -18.6346,\n",
            "         -10.3140, -15.3279, -13.0411, -17.6531, -14.9940, -15.3785, -18.8540,\n",
            "         -21.7821, -16.7843, -17.3074, -17.2724, -17.0717, -12.9041, -11.5096,\n",
            "         -13.1814, -16.6912, -15.5888, -10.4274, -10.2399, -17.6803, -12.3083,\n",
            "         -12.7397, -21.5523, -19.0387,  -8.0545, -17.3613, -20.5211, -18.8376,\n",
            "         -23.4518, -21.5047, -17.0410, -13.5334, -11.4422, -25.1378, -11.9060,\n",
            "          -8.4354, -11.0389, -13.5612, -19.8636, -14.8313, -20.9076, -15.1840,\n",
            "         -18.6206, -18.8314, -18.3951, -18.0329, -11.9985, -13.9841, -15.3805,\n",
            "         -18.1909, -16.2179, -14.1691, -12.2786, -11.9563, -21.2927, -25.5484,\n",
            "         -23.6554, -18.7387, -22.5914, -14.8427, -14.4468, -28.9286, -13.3099,\n",
            "         -26.8223, -24.2242, -15.0812, -21.1687, -18.0103, -18.7516, -19.4843,\n",
            "         -18.4445, -21.2715, -14.1912, -16.3411, -18.5027, -18.5889, -15.9656,\n",
            "         -16.4658, -20.9175, -15.6749, -18.3679, -17.0128, -21.3101, -18.5297,\n",
            "         -17.0272, -15.2412, -20.0397, -19.8529, -18.7604, -25.6230, -17.7789,\n",
            "         -23.1186, -23.1563, -24.8558, -16.2758, -13.2189, -18.3680, -16.2331,\n",
            "         -18.9930, -16.0535, -16.1987, -15.2921, -15.4811, -17.7663, -13.2641,\n",
            "         -14.3113, -15.9841, -16.8886, -12.9227, -14.5678, -15.6938, -15.6977,\n",
            "         -19.6464, -14.1874, -11.5601, -25.7264, -16.4248, -14.3916, -11.4838,\n",
            "         -26.0741, -16.6961, -18.2616, -12.7976, -14.5440, -14.5817, -14.6250,\n",
            "         -17.8626, -14.1522, -13.7933, -23.2144, -16.8726, -14.3262, -21.5775,\n",
            "         -16.1604, -22.3921, -23.5778, -12.3087, -17.3187, -20.9628, -19.0358,\n",
            "         -18.6078, -20.5186, -21.5450, -19.0382, -13.9109, -16.7797, -19.9473,\n",
            "         -14.2851, -12.9283, -12.4657, -20.2373, -27.7989, -15.1647, -23.0758,\n",
            "         -20.1415, -18.0090, -14.5761, -25.5510, -13.8576, -18.2244, -14.4986,\n",
            "         -20.8991, -17.5551, -12.2482, -23.5850, -23.7144, -20.5812, -15.2648,\n",
            "         -17.4166, -17.6345, -19.5874, -15.2492, -17.2723, -14.7696, -20.8284,\n",
            "         -21.4068, -18.0162, -14.9181, -17.8944, -22.8475, -11.0290, -17.5478,\n",
            "         -17.9612, -16.2068, -18.5472, -14.4491, -19.3966, -10.8616, -14.3332,\n",
            "         -16.3801, -17.1997, -15.8048, -16.7387, -14.8845, -15.0017, -14.6318,\n",
            "         -16.6518, -17.7386, -16.4342, -12.0203, -13.2523, -12.8669, -27.4910,\n",
            "         -17.8391, -17.6379, -16.2050, -19.1036, -19.7452, -20.0232, -26.2547,\n",
            "         -11.1717, -21.9599, -17.4869, -16.4318, -12.3512, -13.8018, -18.7445,\n",
            "         -15.6487, -15.5004, -17.2511, -19.9444, -17.0606, -17.0733, -16.3347,\n",
            "         -22.5725, -19.6603, -22.5939, -17.4702, -22.2047, -19.0505, -24.8459,\n",
            "         -14.7451, -14.7698, -17.7737, -16.7698, -21.6933, -17.5329, -13.0388,\n",
            "         -14.3365, -26.9607, -16.3455, -14.9932, -13.8469, -20.9777, -14.7198,\n",
            "         -12.4918, -23.0739, -17.7839, -14.9452, -17.5031, -22.5605, -14.5114,\n",
            "         -13.1205, -17.3084, -22.4640, -20.3172,  -9.5270, -16.7829, -19.7820,\n",
            "         -12.2209, -17.8206, -19.2196, -13.3077, -18.8328, -15.4636, -21.1377,\n",
            "         -21.2911, -12.4773, -15.5345, -18.4147, -20.1442, -14.4544, -17.3496,\n",
            "         -18.2477, -17.8229, -14.7389, -17.3232, -17.5267, -18.0900, -19.7787,\n",
            "         -19.8520, -12.6389, -20.5480, -17.2055, -20.8404, -22.0402, -15.6945,\n",
            "         -19.5870, -13.7042, -16.5333, -20.8198, -15.2066, -22.4021, -12.8530,\n",
            "         -14.4908, -20.2130, -13.4421, -16.1926, -15.3087, -18.9754, -19.2521,\n",
            "         -24.5174, -14.8432, -14.7399, -14.7964, -18.1971, -15.9827, -20.4728]])\n",
            "\n",
            "tgt:\t tensor([ 14, 211, 207,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t4.070285\n",
            "pred:\t tensor([[-20.6236, -20.8241,  -9.7113,  -5.6052, -13.3023,  -4.8504, -11.5394,\n",
            "          -6.4729, -14.1910,  -9.9889, -15.9327,  -6.2438, -14.1387, -18.9518,\n",
            "         -11.4504, -14.9845, -14.3401, -19.1961, -21.1250, -15.2278, -11.3498,\n",
            "         -13.3118, -19.7286, -27.4025, -10.9583, -18.0436, -22.7866, -20.2269,\n",
            "         -21.5465, -24.1096, -14.6545, -18.0942, -10.7935, -11.9723, -13.3776,\n",
            "          -2.8368,  -9.7000, -10.8537, -12.1387, -20.5930, -11.4090, -11.5067,\n",
            "         -17.7072, -17.5313, -19.6687, -13.5464, -14.8071, -16.6074,  -6.2731,\n",
            "         -10.8419, -15.3260, -16.9221, -12.8777, -16.7722, -14.7735, -15.9440,\n",
            "         -21.7813, -14.1608, -17.2620, -17.0009, -15.4007, -19.2552, -23.2387,\n",
            "         -17.2691, -12.1123, -13.8366, -15.5305, -13.4126, -22.0088, -18.6893,\n",
            "         -11.1930, -14.3509, -12.7501, -15.1483, -14.1860, -16.1741, -23.0095,\n",
            "         -16.4630, -13.2472, -13.7497,  -9.8418, -15.6141, -17.1715, -21.8291,\n",
            "         -15.9271, -14.7752, -14.4923, -18.0142, -14.9521, -24.4020, -11.2059,\n",
            "         -17.8169, -18.7722, -24.0849, -14.4048, -21.9872,  -9.7332, -20.8827,\n",
            "         -17.6547, -12.5404, -19.9949, -10.9269, -13.4948, -10.9767, -12.6046,\n",
            "         -15.7106, -12.7811, -13.4540, -16.2987, -17.6028, -17.3787, -21.9804,\n",
            "          -9.7378, -11.5691, -12.5597, -18.5022, -17.9988, -13.6623, -20.0409,\n",
            "         -24.8280, -19.6356, -15.3035, -12.6027, -18.9200, -12.9286, -10.6172,\n",
            "         -10.5962, -13.8669, -19.0782, -10.2607,  -9.5829, -20.3082,  -8.3548,\n",
            "         -13.9456, -21.3696, -16.7031, -10.3648, -15.1447, -16.8393, -18.2815,\n",
            "         -21.7460, -24.7549, -12.6588,  -7.6767, -15.7176, -28.3436,  -7.3631,\n",
            "          -6.0810, -12.9787, -15.0849, -20.8491, -15.9989, -19.3565, -14.8344,\n",
            "         -19.6910, -17.0531, -19.2333, -19.1582, -10.8598, -10.9480, -14.0836,\n",
            "         -18.8290, -20.4877, -14.0837, -11.7403, -11.0738, -22.2549, -25.1828,\n",
            "         -21.3841, -17.5767, -20.5556, -13.9622, -12.2963, -28.1649, -11.1437,\n",
            "         -25.4463, -21.9067, -17.5971, -18.5765, -18.4224, -24.9165, -21.3741,\n",
            "         -19.7132, -18.7849, -12.2518, -19.9901, -16.2170, -18.8230, -17.5703,\n",
            "         -13.6403, -21.7480, -14.8619, -19.8820, -18.5927, -21.5844, -18.8608,\n",
            "         -18.8872, -17.0585, -20.2593, -20.1815, -20.8618, -25.4590, -16.2659,\n",
            "         -22.4158, -20.7026, -24.8114, -16.8374, -14.5883, -18.5213, -14.0512,\n",
            "         -19.3350, -21.1022, -17.5615, -16.8243, -17.1051, -17.1867, -17.3674,\n",
            "         -11.6832, -16.3418, -17.7865, -11.7507, -14.0166, -15.9813, -18.9700,\n",
            "         -16.1353, -13.8816,  -8.1486, -26.2982, -15.6410, -10.7399, -11.7608,\n",
            "         -25.9264, -20.5372, -17.9557, -14.6662, -15.1501, -14.6842, -12.4855,\n",
            "         -17.2417, -15.8906, -14.7555, -25.5205, -13.9103, -18.7282, -19.2908,\n",
            "         -16.5987, -24.2078, -23.4717, -11.6580, -18.0213, -23.2262, -24.0092,\n",
            "         -17.6863, -20.3559, -21.4374, -19.1469, -15.7892, -18.7673, -21.3064,\n",
            "         -13.2054, -12.8127, -13.6144, -19.9393, -28.6099, -11.8229, -22.0514,\n",
            "         -19.9969, -18.1765, -14.6653, -23.8595, -15.7753, -18.3748, -19.9617,\n",
            "         -20.9914, -20.2438, -13.7143, -22.1899, -24.3462, -20.7467, -15.6052,\n",
            "         -15.7504, -20.2033, -17.2477, -16.1964, -18.7844, -15.8369, -20.4728,\n",
            "         -19.9996, -22.6229, -15.8654, -21.5049, -23.1965, -13.8062, -20.7081,\n",
            "         -21.6878, -15.3124, -20.6555, -16.2944, -19.6465, -14.8123, -14.0483,\n",
            "         -12.9374, -15.2910, -14.8835, -15.7745, -15.3514, -17.3028, -19.7310,\n",
            "         -20.6015, -16.7442, -18.7415, -14.2045, -12.2967, -13.2834, -22.2639,\n",
            "         -15.5717, -17.2206, -16.0357, -18.0826, -21.8218, -18.2012, -23.8070,\n",
            "         -12.0534, -16.7677, -15.5163, -15.9391, -13.5516, -13.7987, -13.3871,\n",
            "         -11.7709, -14.9516, -13.6843, -17.5507, -17.8054, -14.3516, -15.9356,\n",
            "         -21.1356, -15.8284, -24.4647, -15.1804, -20.0053, -17.7241, -20.4548,\n",
            "         -16.3365, -13.7828, -16.9177, -13.9983, -24.0592, -15.2068, -13.1143,\n",
            "         -15.4061, -29.1442, -14.6198, -18.9350, -15.7017, -19.1973,  -9.5012,\n",
            "         -14.0483, -23.5935, -13.5202, -16.1866, -14.6820, -19.0079, -16.4647,\n",
            "         -17.3020, -17.2247, -18.9312, -14.3263, -13.0730, -18.5302, -23.8759,\n",
            "         -15.7674, -19.2017, -17.1948, -16.9495, -17.1208, -17.4087, -20.1988,\n",
            "         -24.1083, -15.3248, -16.7384, -20.1166, -15.7360, -19.0655, -16.1366,\n",
            "         -18.2475, -17.5842, -15.5649, -15.4191, -17.8976, -15.7125, -21.1359,\n",
            "         -23.2179, -12.5832, -17.7496, -22.2401, -24.8612, -19.8340, -15.4723,\n",
            "         -19.1323, -16.3032, -18.4261, -19.9456, -17.2309, -22.0270, -11.3504,\n",
            "         -13.4685, -24.8558, -15.2345, -18.1374, -16.1651, -21.9474, -19.1849,\n",
            "         -25.3905, -14.8843, -14.9658, -17.9714, -19.5241, -21.1518, -22.7052]])\n",
            "\n",
            "tgt:\t tensor([ 48,   3, 328,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t2.706536\n",
            "pred:\t tensor([[-20.9517, -21.2482,  -9.5195,  -6.7416, -14.1129,  -6.4468, -11.5217,\n",
            "          -7.5490, -14.0709, -11.7700, -17.5111,  -3.9206, -17.9255, -19.3347,\n",
            "          -9.3375, -13.1599, -15.1955, -16.9643, -20.0453, -14.9974, -11.7326,\n",
            "         -16.8562, -17.4396, -22.2535,  -9.4406, -15.2746, -20.5957, -23.4897,\n",
            "         -22.5780, -23.4006, -18.1496, -18.9342, -10.2345, -12.8729, -14.6506,\n",
            "          -4.2300, -12.0515, -11.9896, -11.4573, -21.7101, -14.2308, -13.8050,\n",
            "         -17.2128, -14.9293, -20.3093, -17.6131, -17.1215, -18.3040, -11.4407,\n",
            "         -13.0580, -14.2678, -17.6137, -16.9284, -18.7138, -16.8584, -18.9770,\n",
            "         -18.3891, -14.1889, -19.9423, -13.9521, -16.5847, -21.6636, -22.2011,\n",
            "         -16.1880, -12.1631, -12.0772, -13.5314, -15.3422, -20.2329, -16.7341,\n",
            "         -11.8166, -15.8464, -12.2175, -16.5771, -14.8368, -15.9700, -22.3158,\n",
            "         -18.1527, -14.6800, -14.4501, -11.9846, -14.9012, -18.8619, -21.1984,\n",
            "         -14.8294, -13.6188, -16.4211, -13.4925, -15.7648, -24.2374, -12.3343,\n",
            "         -16.5845, -19.0868, -24.6598, -13.4132, -24.1062, -10.1292, -24.9227,\n",
            "         -18.4315, -13.4467, -19.2676,  -7.5232, -13.8605, -12.5805, -17.9845,\n",
            "         -16.5851, -11.9198, -13.5408, -16.8713, -16.0910, -19.6547, -21.3851,\n",
            "         -10.0266, -13.5799, -15.5799, -19.4450, -15.8733, -17.9983, -17.7896,\n",
            "         -20.4962, -15.8509, -16.5536, -15.4135, -18.8621, -11.9087, -10.0768,\n",
            "         -12.1332, -18.3344, -18.3856, -11.9113,  -9.3242, -17.9674, -11.5684,\n",
            "         -12.0750, -21.6188, -21.2643, -11.9930, -17.2222, -19.5060, -19.1297,\n",
            "         -19.8077, -21.5262, -17.8246, -11.8520, -12.0173, -26.6909, -11.9522,\n",
            "          -6.4265, -11.1075, -15.8417, -21.1595, -17.9141, -21.0718, -15.4643,\n",
            "         -19.8935, -18.8338, -17.4437, -19.3963,  -9.7630, -16.2599, -18.2794,\n",
            "         -18.0471, -15.5704, -14.3152, -12.8982, -10.5560, -19.1928, -24.7843,\n",
            "         -24.0356, -18.2949, -20.3377, -10.9562, -11.1428, -29.1516, -12.4249,\n",
            "         -20.7768, -19.5304, -16.8430, -23.7268, -20.5698, -23.1915, -18.2873,\n",
            "         -19.8478, -20.7755, -12.7938, -17.1149, -16.5316, -17.7742, -16.9943,\n",
            "         -17.1597, -19.9632, -16.4868, -20.5338, -19.1832, -21.0896, -19.0810,\n",
            "         -18.9894, -17.8061, -24.7857, -22.8553, -23.2504, -24.9687, -19.4462,\n",
            "         -21.8161, -21.6115, -23.1840, -18.3349, -16.5433, -16.1777, -15.8503,\n",
            "         -18.1647, -19.6496, -16.3221, -14.8892, -20.8603, -16.7943, -14.5748,\n",
            "         -16.8410, -20.2495, -14.9789, -17.2276, -16.7318, -15.9598, -17.9594,\n",
            "         -18.6637, -13.4211, -10.4947, -24.5931, -17.9098, -14.5096, -15.2589,\n",
            "         -24.9947, -15.3662, -20.0976, -14.0997, -13.5995, -14.9481, -13.9636,\n",
            "         -17.5191, -14.6556, -13.3108, -26.0870, -17.3226, -17.0948, -22.2450,\n",
            "         -19.0036, -22.4945, -24.6558, -15.9449, -19.7605, -21.9684, -22.0303,\n",
            "         -16.4181, -18.7186, -20.9913, -20.8636, -14.5194, -15.1892, -23.5505,\n",
            "         -13.5636, -16.2854, -14.3427, -22.1176, -27.7249, -16.0583, -20.0576,\n",
            "         -18.3774, -15.1019, -15.9047, -25.0193, -13.5507, -17.5355, -15.3806,\n",
            "         -23.4751, -17.9965, -16.3921, -24.2332, -25.3900, -21.1000, -12.4518,\n",
            "         -14.9060, -18.1434, -15.3719, -17.1727, -19.9687, -17.3506, -18.5343,\n",
            "         -21.5233, -21.5148, -16.5899, -19.2195, -25.4331, -14.5299, -20.8900,\n",
            "         -21.0206, -17.5523, -22.7319, -16.1008, -22.2231, -15.4844, -16.4254,\n",
            "         -14.2341, -15.8903, -18.1349, -13.7240, -17.5556, -16.5247, -17.7321,\n",
            "         -17.3603, -20.6576, -22.4505, -13.3764, -14.3478, -12.6730, -25.4515,\n",
            "         -15.0139, -16.5198, -18.8074, -16.4783, -21.9571, -17.1606, -24.2289,\n",
            "         -12.2190, -18.3013, -19.8180, -16.3554, -14.8690, -13.5658, -19.9644,\n",
            "         -17.0748, -16.5199, -14.4422, -22.3078, -15.5288, -13.9428, -20.1242,\n",
            "         -21.2625, -17.9182, -21.5869, -13.9042, -21.9471, -18.9535, -21.2303,\n",
            "         -17.0983, -11.9856, -16.3115, -15.9936, -24.7926, -15.5518, -18.2650,\n",
            "         -15.8643, -26.5277, -14.2345, -19.2049, -18.5519, -22.2149, -12.7926,\n",
            "         -12.6422, -20.8618, -15.7755, -16.6257, -15.7556, -21.1926, -16.9975,\n",
            "         -16.7035, -18.2386, -20.0865, -17.0054, -10.1665, -18.0697, -23.4868,\n",
            "         -16.4166, -18.7073, -18.1875, -12.5368, -16.4392, -16.6253, -18.9382,\n",
            "         -23.1663, -14.7107, -18.3458, -17.2187, -18.2019, -19.5696, -15.2303,\n",
            "         -20.0722, -15.9173, -17.4373, -16.5415, -20.5632, -15.8371, -22.8339,\n",
            "         -25.2178, -15.4407, -24.0932, -23.5520, -20.8384, -18.4077, -15.5319,\n",
            "         -18.0461, -18.0330, -12.5269, -20.3497, -15.6782, -19.1319, -19.6994,\n",
            "         -13.4385, -23.6380, -13.8914, -16.5136, -14.5942, -18.7091, -17.2327,\n",
            "         -24.6559, -15.2151, -14.1104, -14.1147, -22.0855, -20.8476, -20.1134]])\n",
            "\n",
            "tgt:\t tensor([ 14, 208, 378,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t3.086944\n",
            "pred:\t tensor([[-20.8434, -21.1569, -11.0640,  -6.0046, -14.2520,  -4.2125, -12.1639,\n",
            "          -7.2910, -12.5413, -12.2466, -15.5691,  -5.1016, -15.9590, -18.1673,\n",
            "         -10.7572, -12.7960, -14.6460, -15.6798, -18.7068, -14.6856, -11.5387,\n",
            "         -15.2578, -18.5666, -22.0471, -10.8426, -15.9396, -21.2534, -26.2184,\n",
            "         -21.5064, -22.0319, -15.1274, -16.1882, -11.6216, -11.9995, -15.6073,\n",
            "          -5.4761, -11.3043, -10.8478, -11.2595, -19.3509, -14.6876, -12.7180,\n",
            "         -13.6611, -16.3682, -20.5475, -12.8228, -14.6523, -16.3170, -11.1178,\n",
            "         -13.2556, -13.8433, -19.0180, -16.8381, -17.9186, -16.6921, -16.5475,\n",
            "         -20.2742, -14.8991, -16.0535, -17.1362, -13.9044, -23.3021, -23.2297,\n",
            "         -15.9517, -11.8447, -14.7392, -12.4306, -14.2433, -22.1067, -19.8762,\n",
            "         -13.2356, -13.6627, -12.6644, -16.2983, -16.2919, -12.3362, -21.0927,\n",
            "         -15.6594, -15.2102, -16.7845, -12.7746, -14.1431, -20.0460, -24.0290,\n",
            "         -15.3798, -13.1834, -13.6451, -17.2374, -16.3106, -22.5059, -16.0366,\n",
            "         -16.7714, -21.8424, -22.1900, -12.0335, -22.5193,  -9.6367, -24.2732,\n",
            "         -16.6961, -16.3461, -20.3691,  -9.3228, -11.1791, -10.8791, -15.2965,\n",
            "         -18.4243, -11.1294, -15.2328, -18.6969, -17.2504, -20.0317, -20.4908,\n",
            "         -11.2750, -13.3410, -15.5997, -19.5828, -16.7932, -16.4175, -15.9291,\n",
            "         -21.0547, -15.7184, -19.4339, -15.8403, -17.7789, -13.8830, -12.1531,\n",
            "         -11.2686, -14.9397, -14.9360, -11.9866,  -9.6954, -20.8986, -11.0566,\n",
            "         -18.4645, -22.0722, -21.1274, -11.4084, -16.3217, -20.3787, -18.6439,\n",
            "         -21.4923, -23.1996, -17.3607, -13.6573, -12.4428, -25.6061, -11.0944,\n",
            "          -7.7125, -12.6255, -14.4881, -20.6680, -16.6804, -21.0684, -14.3175,\n",
            "         -19.1989, -19.2792, -18.4128, -20.9651,  -9.4813, -14.5244, -13.6459,\n",
            "         -20.9514, -20.4775, -15.8770, -12.3487, -11.9292, -22.0531, -25.7971,\n",
            "         -23.8700, -17.4950, -20.3296, -15.0504, -13.8324, -29.0831, -11.7637,\n",
            "         -21.1962, -22.3582, -17.0511, -22.2705, -20.0674, -22.5864, -18.1343,\n",
            "         -21.0101, -19.1184, -12.8450, -15.4079, -18.3102, -15.5329, -19.8821,\n",
            "         -15.5904, -21.6708, -16.0098, -21.5225, -16.9177, -20.8166, -18.4854,\n",
            "         -17.8807, -18.2798, -20.9899, -20.9280, -21.6907, -25.8142, -17.8239,\n",
            "         -21.5326, -22.1225, -26.3650, -17.1682, -12.0498, -16.9983, -15.8544,\n",
            "         -17.2295, -18.3727, -16.1653, -17.7110, -14.4751, -15.2930, -13.7060,\n",
            "         -19.1658, -15.3747, -17.1745, -14.5234, -13.8914, -15.9510, -17.7771,\n",
            "         -18.0775, -13.8397, -11.9259, -27.4866, -18.6322, -12.6010, -12.7444,\n",
            "         -24.0475, -15.5521, -18.6677, -15.0124, -13.3951, -16.4236, -17.8297,\n",
            "         -15.2741, -13.3416, -13.3078, -23.2916, -13.9384, -17.9089, -21.7381,\n",
            "         -17.7337, -25.0198, -25.5599, -14.9052, -18.2666, -22.2805, -21.2009,\n",
            "         -18.9332, -19.2961, -24.1704, -17.8386, -14.9907, -19.4068, -22.4876,\n",
            "         -13.1925, -12.8003, -14.3784, -22.0126, -28.3508, -15.2396, -19.7984,\n",
            "         -22.4921, -17.0537, -14.0079, -23.5103, -16.6532, -17.1683, -16.1339,\n",
            "         -20.7677, -20.3639, -12.0931, -27.5059, -24.6339, -17.5220, -13.5478,\n",
            "         -17.4114, -18.6331, -15.8901, -17.1806, -16.1167, -14.7888, -18.6165,\n",
            "         -18.9952, -21.7109, -15.8279, -19.2065, -25.0250, -12.2432, -16.6487,\n",
            "         -17.9657, -14.5873, -20.4848, -15.3520, -19.8777, -15.6194, -17.2614,\n",
            "         -12.9916, -16.3453, -16.6327, -16.9997, -19.3753, -12.1776, -17.2303,\n",
            "         -14.6366, -19.5327, -19.8025,  -9.8651, -12.5628, -14.5715, -21.8077,\n",
            "         -16.6068, -16.8722, -17.7408, -17.5244, -22.3649, -20.3186, -22.4075,\n",
            "         -12.8501, -22.1038, -17.0475, -16.5920, -14.8170, -13.5354, -19.9059,\n",
            "         -14.6436, -15.1108, -12.2897, -22.1568, -15.1155, -16.0460, -18.1864,\n",
            "         -19.9386, -20.7447, -22.2649, -13.6737, -21.4012, -18.5911, -19.8343,\n",
            "         -16.1294, -13.2256, -16.8902, -18.2541, -20.7830, -15.3859, -16.3892,\n",
            "         -15.0340, -27.7438, -13.5745, -17.8200, -15.6360, -19.9707, -12.0330,\n",
            "         -11.1365, -22.5025, -14.4820, -18.3938, -13.7880, -23.4074, -17.7453,\n",
            "         -16.8576, -18.1380, -21.4421, -17.8135, -11.1869, -18.1214, -20.8360,\n",
            "         -14.4795, -18.8150, -17.8316, -15.4635, -20.3138, -17.4621, -17.5850,\n",
            "         -21.5082, -11.6925, -15.4297, -16.5778, -16.7725, -16.4469, -17.2583,\n",
            "         -18.7867, -15.9828, -15.5388, -14.3194, -19.4523, -14.6560, -20.8619,\n",
            "         -21.0356, -13.8904, -19.4393, -18.1646, -22.2463, -19.9316, -17.4158,\n",
            "         -17.3049, -17.5680, -15.8621, -19.4731, -15.4986, -21.4740, -13.4329,\n",
            "         -11.1705, -22.9014, -15.9443, -13.6898, -16.2366, -21.6901, -18.9771,\n",
            "         -24.8033, -15.5927, -16.5277, -16.7102, -19.5346, -18.5846, -19.1278]])\n",
            "\n",
            "tgt:\t tensor([ 15, 287,  75, 104,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t2.324763\n",
            "pred:\t tensor([[-21.2476, -21.4638, -10.1926,  -5.0208, -13.9089,  -5.4332, -11.2326,\n",
            "          -5.6922, -14.7641, -12.8738, -18.0555,  -3.5244, -15.7519, -16.9741,\n",
            "          -9.7805, -13.9608, -16.8350, -20.2539, -22.5279, -14.9879, -11.3820,\n",
            "         -16.6119, -16.8251, -26.2029, -10.8300, -13.9099, -20.6670, -21.9633,\n",
            "         -20.6717, -22.0323, -14.8850, -19.7064,  -9.0934, -13.8244, -14.1333,\n",
            "          -3.3217, -11.7099, -11.9592, -13.0984, -22.3023, -12.5844, -14.1236,\n",
            "         -14.9278, -16.2155, -19.5461, -18.0356, -15.5973, -19.6710, -11.7619,\n",
            "         -12.2191, -16.8373, -16.4513, -16.4178, -17.6043, -15.9747, -16.5790,\n",
            "         -20.8978, -16.0160, -17.1483, -14.2558, -15.1550, -18.3974, -20.2630,\n",
            "         -15.4873, -10.4215, -11.3082, -15.1693, -19.6071, -19.6015, -19.1342,\n",
            "         -11.3405, -14.9462, -12.9605, -14.0667, -14.8416, -13.8739, -24.2122,\n",
            "         -17.0285, -13.4629, -13.7833, -10.8650, -14.9529, -18.6168, -23.0430,\n",
            "         -14.5747, -15.3026, -17.6609, -16.6012, -16.3291, -23.8671, -13.2643,\n",
            "         -19.8566, -19.9433, -20.6536, -13.7271, -23.8949, -12.4473, -25.1940,\n",
            "         -17.6336, -15.5493, -16.5921,  -9.0888, -15.1737, -12.5794, -19.0882,\n",
            "         -17.1579, -13.6886, -14.1086, -17.6307, -16.0829, -19.8075, -19.0001,\n",
            "         -11.5614, -14.9995, -15.1776, -18.4033, -17.1756, -15.2926, -18.7131,\n",
            "         -23.8728, -19.2024, -17.1312, -12.9682, -18.3857,  -9.6314, -10.4493,\n",
            "         -13.0145, -17.8511, -19.6178,  -9.8920, -10.3216, -21.3157, -11.5648,\n",
            "         -12.4440, -20.7430, -18.6478, -11.8288, -16.8260, -18.9999, -19.8190,\n",
            "         -22.2747, -24.1676, -17.1303, -10.8756, -15.5160, -30.2775, -10.1173,\n",
            "          -6.8578, -14.7625, -16.9507, -16.9158, -15.5738, -20.0724, -14.7794,\n",
            "         -20.5485, -18.2642, -17.5800, -18.0678, -12.4032, -13.1269, -16.7006,\n",
            "         -20.8486, -15.6283, -14.9405, -11.7219,  -9.2142, -21.1410, -26.7849,\n",
            "         -24.4593, -19.9852, -23.8198, -14.0805, -14.8388, -32.8189, -11.1059,\n",
            "         -21.1813, -22.5799, -16.5576, -20.9996, -19.9249, -23.0246, -19.5343,\n",
            "         -19.7788, -20.9477, -12.8907, -16.8452, -19.5118, -20.0718, -19.7488,\n",
            "         -14.9687, -18.8920, -14.4474, -19.9217, -19.0577, -22.1195, -18.7741,\n",
            "         -18.4653, -16.7341, -23.6642, -20.8984, -22.4010, -28.1398, -18.3375,\n",
            "         -23.5616, -23.7168, -27.9513, -18.5453, -14.6949, -20.8868, -18.1187,\n",
            "         -19.5176, -19.8778, -19.0445, -16.8397, -20.8538, -16.1000, -15.1276,\n",
            "         -15.2036, -18.2242, -16.2302, -14.3951, -18.4559, -17.0505, -16.2880,\n",
            "         -18.3524, -14.2133, -10.7972, -23.4546, -17.8857, -14.1084, -13.2938,\n",
            "         -24.3567, -12.7149, -20.6486, -14.3565, -16.4966, -16.7393, -14.4642,\n",
            "         -16.0971, -14.6518, -13.7905, -24.3138, -18.6130, -16.0401, -20.4114,\n",
            "         -19.3068, -23.2211, -24.6769, -14.4627, -16.5924, -22.1085, -22.1533,\n",
            "         -19.0729, -21.9201, -20.8235, -20.4289, -17.2768, -19.0316, -22.0575,\n",
            "         -13.7176, -13.9885, -13.9578, -22.1334, -25.6035, -15.4251, -25.3146,\n",
            "         -21.3032, -18.1121, -18.1600, -20.7140, -14.1897, -20.1216, -18.0796,\n",
            "         -21.8056, -20.4921, -14.3693, -27.9603, -25.4180, -22.1668, -15.9648,\n",
            "         -17.4988, -22.3051, -18.7418, -14.5660, -16.9515, -16.1505, -19.2649,\n",
            "         -22.6358, -22.7787, -18.6998, -22.1635, -25.5150, -10.7533, -19.6566,\n",
            "         -22.3198, -21.0771, -20.7563, -15.7691, -23.1074, -16.0648, -19.1265,\n",
            "         -14.9799, -16.5426, -17.0414, -18.9165, -14.3110, -21.8214, -19.4499,\n",
            "         -17.5676, -20.9569, -21.2459, -13.8878, -14.0345, -15.4672, -27.1993,\n",
            "         -14.1917, -18.6794, -17.8303, -16.8177, -23.4324, -21.4633, -25.9608,\n",
            "         -11.1697, -17.5930, -19.5067, -19.2661, -16.2346, -11.7157, -20.0278,\n",
            "         -17.3148, -16.8479, -13.7032, -21.8951, -17.6456, -15.3145, -17.9519,\n",
            "         -22.7134, -20.0147, -24.2359, -14.6004, -27.4983, -22.0568, -19.9588,\n",
            "         -16.1074, -10.8280, -17.9985, -13.3134, -24.5729, -18.4193, -13.0304,\n",
            "         -16.4355, -28.2400, -15.0853, -18.7389, -15.5726, -23.8797, -16.8057,\n",
            "         -11.5312, -25.7803, -15.5696, -18.1955, -15.9111, -22.8316, -18.8047,\n",
            "         -18.0482, -18.8391, -21.1331, -16.8512, -12.0796, -20.9004, -20.2827,\n",
            "         -15.6373, -19.4924, -19.7059, -13.6027, -18.8028, -14.0898, -23.8901,\n",
            "         -21.6927, -16.7504, -19.5632, -13.6733, -17.8170, -18.4879, -14.5659,\n",
            "         -18.1933, -15.2978, -18.0351, -22.5222, -21.3345, -18.5973, -24.7293,\n",
            "         -21.9164, -16.1782, -23.5542, -19.6314, -21.0823, -21.5390, -16.9256,\n",
            "         -24.3109, -17.9735, -16.6788, -18.9323, -18.2342, -21.3307, -14.0203,\n",
            "         -17.0742, -24.2229, -15.4909, -15.9805, -16.6587, -22.8416, -20.3682,\n",
            "         -28.3439, -16.2808, -15.2590, -14.5188, -20.5334, -17.2184, -21.1678]])\n",
            "\n",
            "tgt:\t tensor([15, 74,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t2.405669\n",
            "pred:\t tensor([[-21.4515, -21.7037,  -9.4909,  -7.3171, -14.3862,  -8.5086, -10.9205,\n",
            "          -9.1660, -13.6739, -10.0745, -18.3887,  -9.8581, -22.0804, -18.6454,\n",
            "         -11.0946, -14.1850, -13.9920, -17.3693, -17.7638, -16.9240, -15.0902,\n",
            "         -16.6989, -20.9989, -21.6496,  -6.3684, -15.5287, -21.7343, -23.6333,\n",
            "         -23.8716, -21.9388, -16.8351, -19.2127, -11.0030, -12.2991, -15.4353,\n",
            "          -4.1486, -11.7897,  -8.9429, -10.0789, -20.8494, -12.5499, -11.1266,\n",
            "         -14.6408, -12.8074, -20.9662, -15.2308, -15.7146, -18.5941, -10.2278,\n",
            "         -10.7858, -12.4420, -18.4524, -14.0662, -20.7163, -20.4602, -20.1210,\n",
            "         -23.5541, -14.6914, -20.0924, -17.6340, -16.6423, -21.3092, -27.4389,\n",
            "         -15.5541, -10.6955, -17.0489, -16.1542, -16.5880, -25.6216, -21.5782,\n",
            "         -14.5389, -14.0039, -11.8009, -18.5816, -15.3688, -15.5578, -24.4739,\n",
            "         -18.0623, -13.5018, -16.1832, -12.7204, -16.1822, -19.6384, -23.7250,\n",
            "         -17.5455, -11.5749, -16.6993, -19.6394, -15.4243, -20.6668, -10.8788,\n",
            "         -15.4208, -20.3946, -23.8201, -13.0697, -25.9037, -10.0929, -24.4646,\n",
            "         -18.8339, -15.9587, -21.5988,  -8.0955, -14.6347, -10.9597, -17.0278,\n",
            "         -17.5939, -14.0294, -14.2591, -18.1815, -16.2783, -16.4032, -20.5532,\n",
            "         -11.1474, -13.6456, -12.3192, -21.3883, -17.6276, -17.0268, -18.4957,\n",
            "         -25.7130, -19.3802, -19.8469, -16.2697, -21.6818, -15.8875, -10.2894,\n",
            "         -11.2845, -19.7531, -21.3763, -12.1792, -12.1515, -24.1812, -12.4205,\n",
            "         -15.4911, -21.2518, -23.2343, -12.0333, -15.2721, -18.1831, -18.6290,\n",
            "         -23.3457, -23.0989, -19.8455, -12.8608, -17.6643, -25.2234, -10.7652,\n",
            "          -5.2249, -14.0865, -13.7475, -24.7986, -16.1094, -21.7108, -17.2076,\n",
            "         -18.7979, -16.3995, -19.4971, -20.7421,  -9.5758, -14.8357, -15.5980,\n",
            "         -20.5728, -20.1024, -15.2938, -12.0744, -12.8212, -21.7869, -22.8968,\n",
            "         -22.8996, -16.6844, -17.3542, -13.7899, -15.0645, -26.9778, -11.3700,\n",
            "         -20.6598, -17.5310, -16.2188, -20.1315, -20.8866, -22.6597, -22.4490,\n",
            "         -21.8842, -22.0500, -10.4903, -17.4504, -21.1966, -16.5957, -18.4127,\n",
            "         -16.6731, -17.8827, -17.4627, -20.8654, -17.8977, -20.5722, -18.2939,\n",
            "         -17.2860, -18.5968, -24.8550, -22.5479, -20.8884, -27.0730, -15.6808,\n",
            "         -21.4957, -20.6615, -24.7405, -18.8668, -15.4017, -18.0452, -14.1336,\n",
            "         -19.9478, -20.3465, -18.0894, -21.1732, -17.5950, -16.7475, -18.0412,\n",
            "         -15.3054, -17.4519, -17.8539, -14.2116, -17.6290, -16.2213, -20.3518,\n",
            "         -18.1175, -15.7646, -10.5806, -26.3740, -18.5603, -14.6609, -15.6628,\n",
            "         -26.2578, -17.7527, -18.5519, -16.8766, -17.2392, -16.6916, -17.6583,\n",
            "         -18.5341, -16.3356, -11.0621, -27.7475, -17.9106, -21.0537, -24.3985,\n",
            "         -18.4423, -23.5543, -24.6558, -14.9774, -18.7873, -25.3906, -24.3002,\n",
            "         -20.6249, -20.7308, -22.5276, -20.5828, -17.1104, -16.9385, -19.0911,\n",
            "         -12.8826, -12.1915, -13.5524, -24.3339, -30.8027, -15.2226, -19.6663,\n",
            "         -17.9389, -16.2324, -14.5380, -22.0506, -18.8858, -16.6712, -16.2542,\n",
            "         -23.4956, -19.5774, -16.7962, -29.4097, -26.4849, -19.5434, -15.6776,\n",
            "         -12.5098, -19.2924, -15.3467, -17.5967, -17.3115, -14.5545, -19.0265,\n",
            "         -19.4897, -21.3581, -17.3709, -19.1803, -25.2453, -11.2557, -20.3880,\n",
            "         -19.3985, -16.3734, -22.4301, -17.7821, -24.6979, -15.5453, -17.2647,\n",
            "         -16.1031, -16.5467, -15.8396, -20.1696, -16.0401, -16.1470, -21.7353,\n",
            "         -15.0733, -22.4930, -20.3150, -12.5048, -12.0463, -12.9658, -23.5663,\n",
            "         -15.5706, -16.8997, -18.4933, -19.3056, -23.3087, -18.8582, -24.5453,\n",
            "         -10.8009, -21.8245, -16.4933, -14.2242, -14.3145, -15.8501, -16.8915,\n",
            "         -19.8344, -13.9156, -11.2734, -23.6156, -19.4729, -13.2930, -19.4030,\n",
            "         -18.9803, -15.4110, -28.5086, -14.7453, -22.2455, -17.2762, -20.9740,\n",
            "         -15.8587, -14.6739, -18.1832, -16.5933, -21.1262, -18.1358, -16.6862,\n",
            "         -15.4535, -24.1313, -12.7440, -17.4422, -17.4222, -18.6029, -13.7757,\n",
            "         -14.1092, -20.9240, -18.0254, -16.9435, -15.2585, -19.9587, -18.4888,\n",
            "         -15.1575, -17.2500, -19.4397, -16.4064, -12.8809, -15.6953, -25.2245,\n",
            "         -14.9755, -16.8322, -20.5467, -13.3979, -17.7750, -18.7799, -19.3376,\n",
            "         -21.0309, -19.0943, -15.7533, -16.6785, -17.8241, -15.7590, -18.6417,\n",
            "         -20.4819, -15.2583, -13.9657, -14.7226, -16.1467, -17.0214, -23.7408,\n",
            "         -23.5550, -13.2842, -22.2572, -20.5832, -25.9514, -20.6039, -16.9428,\n",
            "         -17.3221, -15.8961, -15.1827, -19.5947, -16.2495, -22.7305, -13.2169,\n",
            "         -16.2274, -24.0311, -17.1379, -17.5091, -14.1425, -22.1141, -17.8566,\n",
            "         -26.8810, -16.9221, -16.0457, -16.6016, -21.3285, -18.9580, -23.1803]])\n",
            "\n",
            "tgt:\t tensor([175, 193, 265,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t2.548463\n",
            "pred:\t tensor([[-21.0493, -21.1587, -12.1589,  -6.4123, -13.7648,  -5.3315, -10.8313,\n",
            "          -8.1828, -13.5605, -10.8113, -18.8157,  -3.5411, -19.8889, -16.8849,\n",
            "          -9.7696, -15.5236, -15.9002, -18.0091, -23.0020, -13.8721, -14.0998,\n",
            "         -16.3457, -20.9180, -26.3543, -12.9898, -15.5352, -21.1926, -20.1399,\n",
            "         -22.0575, -22.1694, -14.2047, -19.2353, -14.3825, -11.2598, -15.3993,\n",
            "          -3.3180, -13.8649, -12.4165, -12.5611, -19.7760, -14.8664, -12.5628,\n",
            "         -17.7714, -15.9615, -19.6557, -16.8621, -15.5829, -16.8142,  -7.6388,\n",
            "         -16.8212, -15.8574, -13.2168, -16.6842, -19.0764, -16.9575, -19.7503,\n",
            "         -16.2990, -13.3102, -15.8453, -15.0222, -15.7892, -19.3458, -20.2350,\n",
            "         -17.3003, -11.6286, -15.7806, -14.1510, -16.6297, -20.6497, -17.8876,\n",
            "         -11.7282, -13.1116, -12.4127, -16.6653, -15.8599, -18.5563, -22.9550,\n",
            "         -15.6266, -12.5464, -12.5056, -10.2603, -15.2366, -19.2538, -22.2801,\n",
            "         -16.1736, -14.0529, -15.6815, -15.9741, -14.4966, -26.3844, -10.2385,\n",
            "         -17.2087, -19.5576, -22.6183, -12.0053, -23.5263, -11.6346, -28.4697,\n",
            "         -17.5698, -12.4326, -20.2167,  -9.0926, -11.2806, -12.3538, -17.1711,\n",
            "         -14.5821, -10.8858, -10.6773, -18.6477, -16.5612, -14.5625, -22.5107,\n",
            "         -11.0875, -16.5858, -15.5058, -17.3583, -16.3551, -15.8869, -18.1099,\n",
            "         -24.4619, -17.3261, -20.3041, -14.9450, -17.2616, -14.1087, -11.8360,\n",
            "         -12.3792, -20.6432, -19.5534, -11.1595, -11.6946, -18.4412, -12.0362,\n",
            "         -17.4824, -18.8057, -17.8374, -12.4946, -12.5811, -18.3875, -20.3462,\n",
            "         -20.0024, -23.7808, -21.9377, -13.2671, -13.5488, -27.8872, -11.0909,\n",
            "          -6.5585, -13.8998, -14.3042, -21.2234, -14.7010, -18.8361, -13.1924,\n",
            "         -20.4285, -17.2060, -16.8946, -17.9294, -10.0090, -14.7873, -18.0009,\n",
            "         -18.7273, -17.1866, -16.1159,  -9.7022, -10.7954, -21.8275, -27.3727,\n",
            "         -26.0949, -14.7632, -21.0593, -13.5073, -11.5965, -30.6162, -12.5004,\n",
            "         -24.1862, -23.4467, -17.2970, -21.5509, -21.5981, -23.5568, -23.0397,\n",
            "         -16.4525, -21.9205, -15.6633, -17.0531, -19.9504, -19.0749, -18.6577,\n",
            "         -14.3113, -23.4857, -14.0014, -19.0342, -18.0052, -20.7527, -18.0845,\n",
            "         -16.3806, -17.2987, -22.0259, -24.3721, -18.4800, -27.2989, -17.2750,\n",
            "         -20.7133, -24.7378, -26.9018, -16.8279, -18.3293, -16.8364, -17.4884,\n",
            "         -22.8746, -21.5339, -21.9492, -15.0491, -19.0755, -20.7232, -17.3058,\n",
            "         -13.8728, -20.6519, -16.4724, -11.3881, -18.5816, -18.1209, -15.8818,\n",
            "         -19.4347, -12.8454, -13.1886, -27.3349, -15.3492, -12.3594, -12.2951,\n",
            "         -26.2176, -14.4071, -16.2649, -13.5323, -14.8892, -16.1764, -16.2295,\n",
            "         -18.7722, -13.8896, -15.9442, -24.7331, -16.5643, -16.2622, -20.6003,\n",
            "         -16.4956, -23.5429, -22.5536, -12.5813, -19.3382, -20.8157, -21.2445,\n",
            "         -22.2449, -21.7521, -19.8746, -19.6445, -15.1443, -15.2028, -21.1168,\n",
            "         -12.8168, -13.9639, -11.4438, -23.6525, -25.9693, -19.0092, -25.8848,\n",
            "         -21.2719, -17.8677, -14.5165, -26.0137, -18.0795, -19.1712, -15.7175,\n",
            "         -22.5128, -19.1654, -17.2163, -25.0337, -23.9814, -20.4466, -15.4277,\n",
            "         -16.6831, -20.3936, -18.7143, -16.1715, -20.3812, -19.4611, -19.8247,\n",
            "         -23.0263, -21.1090, -14.1750, -22.4126, -24.5586, -10.4930, -21.5521,\n",
            "         -24.8685, -18.7814, -19.9088, -16.0122, -21.1291, -12.4989, -14.7351,\n",
            "         -17.7613, -20.3434, -12.6670, -13.0516, -18.4271, -17.4127, -17.1170,\n",
            "         -15.0206, -17.7644, -19.4523, -16.2483, -13.6798, -11.8168, -22.4816,\n",
            "         -16.6672, -17.1777, -16.2737, -16.1953, -23.3015, -18.5342, -27.6680,\n",
            "         -14.5927, -22.0342, -18.2359, -21.6797, -14.8544, -12.0165, -18.7421,\n",
            "         -15.2317, -17.8628, -10.5699, -21.5075, -17.8779, -14.0882, -17.9489,\n",
            "         -24.9642, -20.5596, -22.8377, -15.8984, -22.0067, -20.8030, -21.9868,\n",
            "         -17.3144, -13.3461, -20.0120, -12.8111, -25.0942, -19.6354, -17.3994,\n",
            "         -14.9838, -27.7439, -14.5267, -15.1750, -19.2734, -22.0991, -14.0326,\n",
            "         -10.8047, -26.2777, -12.3387, -16.3594, -15.5439, -21.6099, -20.1616,\n",
            "         -19.1235, -20.4143, -20.2849, -14.8103, -15.0523, -22.0211, -23.2603,\n",
            "         -14.3819, -17.2276, -20.5027, -12.4647, -19.8206, -15.9732, -20.3414,\n",
            "         -18.4101, -15.4296, -17.3310, -16.1542, -20.4621, -17.7713, -17.9593,\n",
            "         -20.7641, -18.4536, -18.9504, -18.2180, -18.4796, -18.2263, -25.2620,\n",
            "         -25.0686, -17.4533, -21.0064, -21.1854, -23.6143, -18.4016, -14.9629,\n",
            "         -24.5180, -14.3487, -14.7676, -20.0815, -18.7849, -20.1795, -15.3267,\n",
            "         -11.0195, -21.8254, -16.9601, -16.3572, -18.3476, -20.7682, -19.0293,\n",
            "         -27.5156, -13.6565, -12.8979, -16.6656, -18.2138, -19.2728, -21.6916]])\n",
            "\n",
            "tgt:\t tensor([ 14,  79, 277,  41, 331,  11,   2,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t2.887683\n",
            "pred:\t tensor([[-20.9236, -20.9581, -10.9486,  -7.2237, -12.0853,  -2.7677, -11.3632,\n",
            "          -8.8026, -12.1050, -10.0371, -19.8193,  -4.7657, -14.5785, -15.3946,\n",
            "         -12.0883, -17.2826, -16.4497, -21.1031, -21.3910, -14.5853, -17.4161,\n",
            "         -14.6051, -20.1314, -23.2643, -13.9739, -13.7546, -22.2671, -22.0420,\n",
            "         -21.2046, -22.5130, -13.7079, -21.7625, -12.7050, -11.0432, -13.7881,\n",
            "          -3.0472, -15.3557, -11.4604, -14.4968, -19.8442, -12.8083,  -8.7174,\n",
            "         -16.4588, -16.3536, -22.2743, -14.8978, -14.7759, -18.9575,  -7.6612,\n",
            "         -14.2448, -18.8111, -12.0060, -18.9551, -16.3117, -16.8560, -16.8544,\n",
            "         -15.8485, -13.8151, -14.4065, -12.3439, -15.3430, -12.4114, -18.6521,\n",
            "         -13.7280, -13.2902, -13.2800, -17.7101, -20.8447, -20.3045, -17.2609,\n",
            "          -9.2527, -16.8459, -14.5372, -14.8266, -16.7747, -17.5352, -24.7632,\n",
            "         -17.0133, -13.1450, -11.7788,  -8.5293, -14.3601, -16.2615, -24.2725,\n",
            "         -16.1169, -14.1722, -15.5717, -15.3554, -16.9244, -28.0054,  -9.4156,\n",
            "         -19.9992, -18.7151, -23.6807, -15.3705, -23.8142, -11.8928, -23.2730,\n",
            "         -12.8345, -14.3192, -19.4338, -11.4579, -13.7059, -11.6239, -15.9282,\n",
            "         -17.8238, -14.0696,  -9.9021, -16.4553, -17.1635, -21.8184, -25.3302,\n",
            "         -11.9023, -14.4720, -15.8018, -16.9792, -16.6087, -18.3611, -15.9379,\n",
            "         -29.0452, -17.3284, -16.2133, -12.4122, -14.5881, -14.7849, -10.1119,\n",
            "         -13.9671, -18.6877, -21.2651, -10.1121,  -8.7833, -19.5825, -10.9355,\n",
            "         -16.7427, -18.6261, -17.9057, -12.3252, -13.4665, -19.9752, -20.7225,\n",
            "         -23.5164, -25.3503, -17.2373, -11.2996, -13.6694, -28.4302, -10.2260,\n",
            "          -6.2448, -12.5116, -16.5021, -18.1546, -15.1879, -16.4792, -12.6893,\n",
            "         -18.4717, -16.0258, -13.1010, -15.3615,  -7.7463, -12.6811, -18.5334,\n",
            "         -18.1585, -17.9393, -15.4473,  -8.2548, -12.7395, -19.4193, -26.0969,\n",
            "         -25.6537, -17.5757, -25.0388,  -9.9992, -12.0014, -31.9307, -11.6896,\n",
            "         -27.1609, -24.1457, -16.9459, -21.1770, -24.7323, -22.4643, -19.8978,\n",
            "         -16.7008, -23.3542, -17.1142, -19.8624, -21.8981, -20.4131, -21.4600,\n",
            "         -14.9379, -24.5550, -10.8911, -22.3478, -17.4959, -19.7396, -18.1377,\n",
            "         -14.7134, -18.5433, -25.2875, -25.8301, -17.6788, -28.6042, -18.6914,\n",
            "         -19.8219, -25.3176, -25.5218, -21.4587, -17.3088, -16.5341, -16.1946,\n",
            "         -22.0364, -22.6004, -20.6310, -17.8385, -23.7346, -21.5586, -16.4506,\n",
            "         -14.2488, -20.0284, -17.2780, -15.0190, -16.3909, -19.3354, -19.2340,\n",
            "         -15.3010, -11.8103, -11.8677, -25.3983, -14.8309, -12.2732, -11.5224,\n",
            "         -24.2134, -18.6278, -17.1124, -13.9835, -12.6115, -15.2454, -12.8944,\n",
            "         -19.0606, -13.1330, -14.2735, -26.2862, -16.9511, -15.8164, -19.1728,\n",
            "         -12.3476, -19.6385, -22.9691, -13.2258, -18.3922, -22.6766, -20.7867,\n",
            "         -20.5583, -21.9487, -15.6266, -22.6429, -19.3766, -15.0657, -23.5394,\n",
            "         -10.3060, -13.8016, -12.5988, -21.1549, -24.5353, -17.8026, -26.9506,\n",
            "         -20.6487, -16.8107, -12.9963, -20.5252, -17.1663, -16.7653, -16.5336,\n",
            "         -24.2905, -17.6192, -17.7602, -27.5497, -27.4537, -24.1820, -16.9687,\n",
            "         -15.9830, -21.6416, -20.4686, -18.2092, -21.8367, -13.6529, -18.5076,\n",
            "         -27.0074, -21.9273, -16.4490, -24.3530, -26.0338, -11.6153, -21.7476,\n",
            "         -27.4231, -15.5379, -23.6376, -15.4478, -22.2589, -12.7906, -18.1187,\n",
            "         -15.8161, -17.8998, -12.5541, -13.1500, -16.1173, -20.7239, -19.7607,\n",
            "         -15.4517, -18.5401, -20.4460, -15.4412, -13.4039, -14.6610, -23.9150,\n",
            "         -15.6494, -15.3288, -14.4407, -10.9438, -19.3692, -20.5058, -23.8465,\n",
            "         -13.2005, -19.5237, -20.6937, -19.7533, -14.0105, -13.0913, -13.0960,\n",
            "         -18.5532, -19.1251, -14.8225, -18.2525, -16.9664, -16.0282, -19.4582,\n",
            "         -25.7848, -22.8551, -25.7245, -13.0873, -24.5252, -19.3653, -20.9950,\n",
            "         -15.0949, -12.9082, -13.7975,  -9.0645, -27.9284, -16.1455, -16.1828,\n",
            "         -16.3139, -30.0578, -13.6186, -16.9865, -16.4085, -22.2755, -15.3259,\n",
            "         -10.9637, -29.2904, -19.4437, -17.6395, -13.1766, -24.5362, -23.1512,\n",
            "         -16.6634, -17.2017, -23.1444, -19.5986, -15.6223, -19.4411, -24.5250,\n",
            "         -13.6009, -15.9004, -16.7741, -14.1492, -18.6285, -13.9658, -21.3778,\n",
            "         -18.7470, -20.0259, -21.6005, -17.8763, -21.3328, -18.7125, -16.4304,\n",
            "         -16.4218, -15.3002, -19.1926, -18.9297, -22.8265, -19.1281, -25.5657,\n",
            "         -24.9998, -16.4359, -18.5342, -17.9831, -18.0978, -17.3233, -14.5186,\n",
            "         -25.9456, -14.6354, -16.4775, -21.0450, -19.2278, -21.5597, -14.6868,\n",
            "         -10.6139, -21.4591, -14.7081, -20.1097, -18.0153, -24.2318, -23.8574,\n",
            "         -23.8919, -13.0840, -18.4723, -15.1174, -18.5806, -16.5557, -27.1594]])\n",
            "\n",
            "tgt:\t tensor([ 62, 107,  14, 171, 191,  11,   2,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t2.492487\n",
            "pred:\t tensor([[-21.5310, -21.7375, -10.6703,  -6.8357, -13.4527,  -4.4743, -10.9739,\n",
            "          -3.9792, -13.1687, -12.5572, -18.7600,  -7.4815, -16.0963, -13.7197,\n",
            "         -11.0554, -16.7671, -16.0509, -21.8604, -22.4528, -16.2117, -11.0015,\n",
            "         -12.4037, -17.1377, -23.4464, -11.8443, -15.9362, -22.7093, -22.3901,\n",
            "         -22.0000, -23.2661, -13.1931, -20.8070, -10.0453, -10.1739, -13.9566,\n",
            "          -4.2802, -10.8713, -12.7018, -13.2903, -19.7368, -13.5080, -11.5606,\n",
            "         -19.5741, -15.0402, -21.0609, -12.1972, -16.9969, -19.4528,  -9.1062,\n",
            "         -14.1250, -17.4459, -16.5667, -17.8051, -15.5017, -13.0722, -15.2935,\n",
            "         -19.0597, -14.5418, -17.3732, -14.0962, -13.5792, -18.1176, -21.2175,\n",
            "         -15.4762, -14.4010, -13.1804, -16.2878, -15.5336, -20.4628, -15.6989,\n",
            "         -12.5922, -16.2392, -13.6250, -13.3338, -15.4394, -12.0656, -20.7381,\n",
            "         -19.0107, -14.3989, -13.5881, -10.8659, -14.7246, -16.5229, -24.0338,\n",
            "         -15.1651, -14.7547, -18.0268, -19.0659, -18.6302, -25.7872, -13.6236,\n",
            "         -18.0246, -20.7447, -22.6537, -14.8983, -22.9932, -11.1910, -21.1349,\n",
            "         -17.4065, -14.0819, -15.7225, -11.1765, -15.9966, -10.9613, -17.1399,\n",
            "         -19.0907, -14.3644, -14.6046, -19.5007, -19.2689, -16.7588, -23.0345,\n",
            "         -11.4495, -12.1584, -14.9748, -20.0317, -20.1330, -16.1858, -18.5906,\n",
            "         -24.8928, -22.0766, -15.9897,  -9.1569, -16.8707, -12.6969,  -9.7569,\n",
            "         -10.3057, -17.8548, -20.2414, -10.3389,  -8.7485, -18.6729,  -8.6690,\n",
            "         -12.7672, -21.4856, -20.4404, -13.4631, -16.0122, -19.1910, -19.2114,\n",
            "         -24.3233, -26.8589, -14.8738,  -9.9377, -13.0423, -28.2371, -11.3978,\n",
            "          -6.7373, -13.3646, -15.0056, -19.1905, -14.9608, -19.5274, -14.5092,\n",
            "         -19.6200, -17.2658, -18.7321, -21.5488, -11.7125, -12.9677, -16.4393,\n",
            "         -21.8461, -22.9384, -12.8162, -13.2055, -11.4426, -22.4016, -26.9971,\n",
            "         -24.0319, -16.1896, -22.7457, -13.1844, -12.2772, -30.0209, -10.6914,\n",
            "         -25.1739, -23.3585, -16.3471, -21.4715, -21.9785, -26.4723, -21.2941,\n",
            "         -19.2378, -21.6964, -14.1890, -22.1779, -20.0101, -17.9313, -18.7038,\n",
            "         -14.3090, -23.2358, -15.9410, -23.5125, -22.4929, -22.1116, -15.0854,\n",
            "         -16.1040, -17.8664, -24.2945, -22.3710, -19.2713, -29.4950, -18.3263,\n",
            "         -26.2843, -22.6892, -26.8026, -21.7528, -14.8327, -20.2228, -20.7483,\n",
            "         -21.5316, -21.4215, -22.0883, -18.2015, -20.7786, -18.6941, -17.7493,\n",
            "         -17.2950, -18.1019, -18.0994, -14.9607, -17.2757, -16.3885, -18.4850,\n",
            "         -15.0963, -12.8540,  -9.1963, -25.8666, -18.0246, -10.6504, -12.1614,\n",
            "         -25.7648, -18.0109, -18.2315, -15.0087, -15.6210, -16.6821, -15.9386,\n",
            "         -17.8626, -13.7113, -13.4262, -25.0545, -20.8354, -16.6123, -18.3334,\n",
            "         -15.5391, -22.2395, -22.5953, -16.1545, -17.7066, -23.2727, -22.9987,\n",
            "         -21.2467, -21.1543, -22.7925, -20.2804, -15.9976, -17.4760, -24.2276,\n",
            "         -14.3035, -10.9616, -10.5766, -23.2229, -27.4675, -17.0561, -25.9886,\n",
            "         -21.3500, -19.6793, -17.8053, -24.2429, -17.1145, -20.1113, -22.9263,\n",
            "         -26.6553, -19.8587, -15.5791, -25.0399, -26.4433, -22.9887, -17.4080,\n",
            "         -17.5372, -24.1703, -17.4779, -15.6459, -21.0754, -11.4501, -21.4893,\n",
            "         -24.8726, -25.2487, -17.2717, -25.7982, -28.5078, -12.6114, -20.8137,\n",
            "         -23.2261, -18.8404, -22.7638, -15.6933, -21.2950, -15.9021, -16.0781,\n",
            "         -14.7032, -14.9502, -13.9600, -19.5449, -17.0998, -17.5394, -22.3269,\n",
            "         -14.6902, -23.4972, -21.5129, -15.4919, -11.8214, -13.3132, -26.4416,\n",
            "         -15.2910, -14.4872, -15.5787, -15.6197, -24.2177, -20.6024, -27.3221,\n",
            "         -12.5382, -21.7062, -16.9233, -17.8520, -16.0734, -15.3896, -16.5344,\n",
            "         -16.4084, -17.3611, -15.6512, -22.1982, -17.9968, -13.8549, -24.1607,\n",
            "         -23.0608, -19.4199, -27.4824, -12.1641, -23.8481, -20.7319, -21.9023,\n",
            "         -16.1700, -13.1358, -18.5103, -11.3045, -23.7603, -15.5285, -17.3011,\n",
            "         -16.0790, -30.3085, -13.7697, -17.9670, -15.6242, -22.5080, -10.1386,\n",
            "         -15.4120, -28.1584, -16.5921, -17.4187, -16.5512, -21.3623, -16.3085,\n",
            "         -18.1598, -17.4527, -21.9438, -22.3787, -12.4512, -20.3563, -21.8674,\n",
            "         -15.1316, -17.4529, -17.5766, -13.6723, -16.8721, -15.4277, -21.1479,\n",
            "         -24.1510, -18.4574, -21.3814, -17.5768, -20.5433, -17.4852, -15.5628,\n",
            "         -20.7689, -16.1944, -13.2043, -18.7253, -23.9241, -16.3347, -26.1636,\n",
            "         -24.1772, -16.9384, -20.9755, -21.5733, -26.7620, -22.6935, -16.9651,\n",
            "         -19.5503, -15.6529, -16.1379, -21.2627, -15.8045, -21.3922, -16.2513,\n",
            "         -13.5167, -23.9427, -14.5903, -19.2279, -17.7276, -21.1418, -19.8084,\n",
            "         -28.1262, -16.9498, -15.2291, -18.0232, -23.4436, -15.1313, -22.4799]])\n",
            "\n",
            "tgt:\t tensor([  3, 207,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t2.853506\n",
            "pred:\t tensor([[-21.9803, -22.2223, -12.5079,  -5.0540, -14.3926,  -3.4202, -14.8095,\n",
            "          -8.7271, -12.5014, -12.3996, -19.6405,  -2.9516, -14.8005, -18.5352,\n",
            "          -9.4816, -15.7623, -16.8644, -19.9513, -21.9411, -15.5098, -14.7118,\n",
            "         -19.4765, -16.9148, -22.2586, -10.8039, -15.7936, -24.9290, -22.1311,\n",
            "         -20.3029, -23.0262, -15.8513, -22.4882, -11.3471, -13.4360, -16.2683,\n",
            "          -3.5082, -11.0753, -12.7924, -14.5727, -22.1420, -14.1691, -16.6387,\n",
            "         -16.3009, -17.1390, -20.2231, -17.2518, -15.0618, -18.0187, -10.6171,\n",
            "         -16.3193, -16.3378, -16.7493, -19.3726, -17.5803, -18.3865, -17.7523,\n",
            "         -20.7353, -13.1868, -20.5483, -14.6949, -16.4481, -19.7107, -21.1762,\n",
            "         -16.5119, -15.4031, -19.3488, -13.2754, -20.2238, -19.8610, -18.8549,\n",
            "         -12.6777, -17.7180, -14.6841, -14.3594, -17.5902, -13.6882, -21.1794,\n",
            "         -17.6397, -16.1502, -16.6401, -12.0967, -15.5808, -19.4318, -26.3445,\n",
            "         -17.5458, -18.0445, -15.8243, -18.2382, -16.3108, -26.7271, -14.3421,\n",
            "         -17.6682, -20.9167, -22.4934, -15.6118, -24.9801, -14.8316, -21.6447,\n",
            "         -18.0093, -15.1371, -19.3288, -11.2275, -15.7015, -11.6020, -19.5400,\n",
            "         -18.8771, -11.7073, -14.4817, -18.0103, -15.6582, -20.0123, -21.9555,\n",
            "         -12.6865, -15.9159, -18.8211, -22.8533, -16.0892, -15.8985, -16.6710,\n",
            "         -26.2413, -18.1928, -19.4221, -17.0892, -17.6829, -15.6340, -12.0344,\n",
            "         -13.8665, -16.1991, -22.4918, -12.7894, -12.0818, -20.0529, -10.9945,\n",
            "         -12.4768, -21.9077, -20.7993, -11.2804, -17.6356, -22.1420, -21.5703,\n",
            "         -23.6952, -26.1349, -17.6975, -10.9262, -15.1364, -31.1688, -12.8057,\n",
            "         -10.2834, -14.2443, -15.9651, -21.1727, -16.3361, -19.9585, -16.4595,\n",
            "         -21.2344, -17.8959, -17.5092, -16.4485, -13.7979, -13.4537, -18.1032,\n",
            "         -21.0667, -19.7171, -15.5675, -12.8508, -11.4248, -23.5838, -29.8473,\n",
            "         -28.5686, -18.5281, -24.6800, -14.2071, -14.5176, -32.5389, -13.4608,\n",
            "         -24.6593, -25.0737, -16.2790, -25.6674, -22.0855, -21.9617, -19.2123,\n",
            "         -19.4098, -23.3164, -17.1900, -20.9270, -19.6414, -18.3553, -18.1751,\n",
            "         -14.8145, -22.8608, -15.2029, -21.3134, -20.4345, -21.7369, -17.7582,\n",
            "         -19.6307, -20.0237, -23.2120, -22.0307, -24.1105, -28.6874, -18.7661,\n",
            "         -23.9924, -24.7077, -27.6919, -20.3458, -14.4774, -17.9398, -19.7574,\n",
            "         -19.6231, -21.5014, -20.8138, -15.5142, -20.0082, -19.9236, -13.9649,\n",
            "         -14.8948, -17.5010, -17.7632, -13.3597, -17.7564, -13.3944, -17.2746,\n",
            "         -18.5536, -12.9465, -10.5665, -24.3379, -22.4130,  -8.6560, -14.9602,\n",
            "         -26.4506, -18.4714, -16.7227, -14.1367, -15.4638, -17.2025, -18.4649,\n",
            "         -18.3244, -15.8974, -15.0924, -25.7176, -17.5869, -15.3455, -23.0392,\n",
            "         -17.5690, -24.8774, -26.2198, -13.1014, -17.9311, -19.7473, -22.6641,\n",
            "         -23.8007, -22.2436, -21.8682, -23.1350, -17.9156, -16.8407, -25.1447,\n",
            "         -14.7348, -14.3371, -12.8650, -20.5436, -27.7340, -18.9997, -27.4945,\n",
            "         -21.9249, -19.3875, -14.4471, -21.6947, -14.3665, -20.3245, -22.5460,\n",
            "         -25.4386, -22.2819, -16.4149, -23.8787, -28.4195, -22.9970, -16.6211,\n",
            "         -20.5086, -19.9694, -18.6976, -18.1424, -18.0866, -16.2072, -20.3846,\n",
            "         -22.2138, -22.8435, -16.7548, -22.2830, -26.4595,  -9.6117, -22.7064,\n",
            "         -24.2181, -18.9012, -22.2261, -17.1052, -19.0136, -14.6859, -17.1669,\n",
            "         -15.9655, -19.2308, -17.9056, -16.4665, -15.9245, -20.2171, -19.5910,\n",
            "         -18.0560, -21.5977, -21.6896, -13.5045, -16.9717, -13.6757, -30.2830,\n",
            "         -17.2062, -20.2565, -16.9414, -17.0431, -24.2822, -20.6224, -25.2549,\n",
            "         -12.6085, -23.1368, -20.1572, -18.4540, -13.6625, -13.1920, -21.7889,\n",
            "         -15.4445, -17.6713, -17.4735, -21.1818, -20.5211, -16.2440, -20.7740,\n",
            "         -25.1480, -20.6274, -24.8651, -17.6850, -24.7512, -18.8374, -23.5227,\n",
            "         -15.9840, -14.1989, -19.5583, -17.1974, -23.3083, -15.0733, -19.2964,\n",
            "         -16.1072, -28.8215, -16.3007, -17.9116, -16.6229, -20.4932, -13.9441,\n",
            "         -14.4632, -26.3831, -16.3067, -16.8381, -14.5506, -24.2128, -18.9852,\n",
            "         -20.1435, -21.4454, -23.2831, -19.9004, -12.7511, -18.7442, -21.5321,\n",
            "         -14.4366, -21.3766, -20.8284, -17.8248, -18.6578, -14.8276, -18.4001,\n",
            "         -24.1623, -17.7995, -21.2463, -16.5112, -20.2749, -20.3626, -17.4980,\n",
            "         -19.7127, -17.1130, -15.0183, -19.5278, -23.7326, -16.9768, -25.0412,\n",
            "         -22.4948, -16.2155, -22.4495, -18.5005, -21.9363, -22.3736, -16.9530,\n",
            "         -21.9074, -16.2585, -16.3642, -22.4737, -19.1641, -22.6524, -18.3331,\n",
            "         -13.5833, -22.6294, -12.3157, -17.8805, -15.7494, -22.3253, -21.0951,\n",
            "         -29.6056, -16.9574, -14.4652, -15.0571, -24.2805, -18.9692, -20.3201]])\n",
            "\n",
            "tgt:\t tensor([14, 28, 75,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t2.411434\n",
            "pred:\t tensor([[-22.1847, -22.3725, -11.0590,  -7.8490, -15.3984,  -7.9702, -12.2946,\n",
            "          -6.9527, -12.3582,  -9.8708, -21.8634,  -7.1911, -18.6922, -15.2717,\n",
            "         -10.9339, -16.8548, -16.1886, -21.0835, -21.0714, -15.5314, -15.4767,\n",
            "         -17.8901, -18.3008, -23.1096, -10.0004, -15.5358, -23.2733, -23.3850,\n",
            "         -24.0828, -23.9065, -18.5700, -21.5737, -12.7981, -12.7248, -14.5697,\n",
            "          -2.7559, -11.5730, -12.0431, -13.4864, -18.2253, -13.5600, -13.1700,\n",
            "         -19.2588, -16.0861, -19.5632, -14.0845, -17.4219, -18.8667,  -9.8562,\n",
            "         -13.0381, -15.9582, -17.8046, -16.5618, -18.7117, -18.9829, -19.1856,\n",
            "         -22.3844, -18.9995, -17.0548, -13.2917, -14.2167, -19.8531, -23.4092,\n",
            "         -16.2931, -13.6112, -15.9384, -16.5463, -17.1248, -21.5645, -18.2279,\n",
            "         -11.8510, -15.4459, -14.2842, -16.9921, -15.2203, -15.0148, -22.7563,\n",
            "         -16.6169, -13.3439, -13.6948, -10.5389, -16.3706, -17.9584, -22.7368,\n",
            "         -16.9059, -16.2279, -16.6351, -18.8456, -18.3478, -24.3758, -11.4298,\n",
            "         -18.6672, -21.8907, -24.7292, -13.6401, -25.9601, -10.3623, -22.2859,\n",
            "         -17.6687, -14.0234, -17.7142,  -9.8010, -14.5164, -13.1548, -18.7048,\n",
            "         -17.7530, -11.9379, -13.3751, -18.3687, -16.2130, -19.4752, -21.4236,\n",
            "         -12.8255, -13.1504, -16.4761, -19.8141, -19.5444, -17.9252, -18.7715,\n",
            "         -26.1029, -18.8889, -16.9730, -14.2596, -17.1771, -13.7085, -10.6583,\n",
            "         -12.2330, -18.2824, -21.2867, -11.4743,  -9.8076, -20.5254, -12.2485,\n",
            "         -16.8861, -23.4761, -20.1046, -12.0384, -17.7767, -19.6056, -20.4996,\n",
            "         -23.8513, -25.1561, -20.7171, -12.9777, -15.7530, -29.9048, -13.2098,\n",
            "          -6.7306, -15.2472, -13.2137, -21.6718, -18.4074, -20.1796, -17.8006,\n",
            "         -22.0187, -16.5514, -16.9269, -20.6926, -11.4938, -11.5152, -19.6670,\n",
            "         -21.9743, -20.3254, -12.4231, -11.8217,  -9.7410, -20.8928, -27.0319,\n",
            "         -24.0616, -18.0158, -23.4356, -12.7292, -12.9871, -30.6031, -12.9398,\n",
            "         -25.8884, -21.1725, -15.1079, -23.5516, -21.0654, -27.4686, -22.6853,\n",
            "         -20.9782, -22.9821, -12.6830, -19.1639, -21.3281, -20.0424, -19.4517,\n",
            "         -14.7717, -19.6654, -18.5115, -25.1940, -20.9293, -22.3115, -19.1153,\n",
            "         -19.5892, -18.6151, -23.8299, -25.2685, -20.8844, -29.6595, -18.6090,\n",
            "         -22.9857, -23.3829, -30.4917, -22.5665, -15.9651, -20.6065, -15.4190,\n",
            "         -21.6465, -22.2586, -21.9095, -19.3530, -20.1022, -19.4253, -17.0967,\n",
            "         -14.2078, -18.6430, -21.8060, -14.1127, -17.7448, -17.6308, -20.6265,\n",
            "         -18.5051, -15.8012, -12.2060, -26.7497, -18.8549, -14.8625, -14.1002,\n",
            "         -28.7396, -16.5951, -19.0842, -15.3939, -16.5474, -17.4634, -16.1344,\n",
            "         -20.1499, -15.7464, -11.4378, -25.2331, -21.1123, -16.7907, -19.6370,\n",
            "         -17.4711, -25.3540, -25.0049, -13.1384, -17.6689, -26.6599, -21.9231,\n",
            "         -22.9941, -22.1866, -21.9990, -23.3427, -17.2857, -18.6093, -24.5814,\n",
            "         -12.1840, -14.5727, -11.1898, -23.1860, -27.2417, -14.4834, -22.7743,\n",
            "         -20.3249, -16.1665, -17.2189, -24.4015, -18.1795, -21.2402, -17.6828,\n",
            "         -22.2604, -20.4992, -17.6987, -27.7139, -29.3749, -22.7944, -17.3902,\n",
            "         -17.1247, -21.1073, -17.5668, -21.7084, -22.2235, -15.4183, -22.9162,\n",
            "         -26.0430, -27.0430, -18.6276, -22.2879, -28.6871,  -9.5058, -22.7031,\n",
            "         -23.5495, -20.8512, -26.1174, -18.1098, -25.0593, -15.3254, -18.6280,\n",
            "         -15.0314, -16.6170, -12.9495, -20.6144, -15.7169, -18.4335, -18.6993,\n",
            "         -16.0271, -19.8989, -25.4385, -15.2024, -10.7434, -15.7333, -27.7413,\n",
            "         -15.6182, -18.6060, -19.1674, -17.0094, -23.5165, -20.3834, -25.6163,\n",
            "         -14.8188, -19.9463, -19.2580, -19.3134, -17.5348, -17.0177, -20.3278,\n",
            "         -17.5727, -17.5888, -15.7736, -20.2153, -17.1396, -13.2254, -21.2668,\n",
            "         -22.6032, -19.4516, -27.5576, -11.4326, -25.7492, -21.1257, -22.7300,\n",
            "         -16.8086, -18.5759, -18.9387, -17.6621, -24.4336, -19.2590, -16.7198,\n",
            "         -16.0989, -29.2954, -16.8309, -20.6938, -18.3725, -20.4511, -14.6788,\n",
            "         -13.0735, -28.5932, -16.4444, -18.7541, -15.1506, -22.9251, -18.0588,\n",
            "         -17.9535, -18.7647, -20.3405, -16.9412, -13.1660, -19.1741, -23.7122,\n",
            "         -15.3810, -16.8277, -23.4245, -18.4305, -18.1514, -17.1508, -20.0624,\n",
            "         -22.8336, -18.6674, -21.1728, -17.7042, -18.4557, -19.3930, -14.9058,\n",
            "         -19.0804, -16.2748, -19.2264, -17.0179, -20.9168, -18.1334, -25.3965,\n",
            "         -29.9861, -15.5880, -26.5853, -19.0507, -25.4612, -19.5785, -16.6743,\n",
            "         -22.2337, -16.4890, -14.1699, -20.7108, -19.6799, -25.1430, -13.0407,\n",
            "         -18.1926, -29.2214, -17.2359, -21.4236, -17.5555, -21.0750, -21.1033,\n",
            "         -31.5250, -16.7168, -15.6913, -12.0243, -20.2268, -23.1716, -24.3437]])\n",
            "\n",
            "tgt:\t tensor([155, 254,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t3.157995\n",
            "pred:\t tensor([[-22.1820, -22.4298, -11.6110,  -8.1451, -13.4133,  -7.2769, -12.5240,\n",
            "          -3.6796, -15.1208, -11.6136, -20.1775,  -6.3110, -20.1325, -17.4116,\n",
            "         -10.4486, -15.7394, -16.7313, -21.4458, -21.6781, -15.3469, -13.5730,\n",
            "         -15.8270, -19.3149, -24.2999, -11.5470, -16.5214, -22.9082, -22.7568,\n",
            "         -21.7899, -23.5922, -17.2594, -20.5358, -11.6943, -12.4230, -17.2100,\n",
            "          -3.2747, -11.9664, -11.6267, -11.9000, -22.7664, -14.5335, -12.9088,\n",
            "         -19.3964, -17.7263, -20.4652, -13.2787, -16.2514, -19.6584, -12.0053,\n",
            "         -14.5833, -18.5447, -17.1798, -16.4062, -16.5075, -16.6715, -17.1042,\n",
            "         -20.7699, -17.8754, -18.6297, -15.5413, -16.0808, -18.4020, -22.2955,\n",
            "         -16.0304, -11.6872, -13.7383, -17.4902, -15.7981, -22.9822, -19.1911,\n",
            "         -11.9531, -15.9529, -13.4378, -15.8565, -15.9312, -15.7025, -22.5790,\n",
            "         -15.2852, -14.2257, -12.1156, -10.7667, -15.6457, -18.8692, -22.2921,\n",
            "         -20.2103, -15.7550, -16.1102, -18.6992, -16.0862, -26.5893, -14.2063,\n",
            "         -18.1433, -19.7079, -23.2480, -14.7901, -24.5051,  -9.5619, -22.2586,\n",
            "         -18.8688, -13.5303, -22.1483, -12.2880, -16.0454, -12.1071, -17.5820,\n",
            "         -20.5962, -12.3464, -12.9709, -18.8171, -18.7144, -24.7617, -21.8155,\n",
            "         -10.6156, -14.6329, -16.7233, -19.5086, -18.8632, -15.7751, -20.6144,\n",
            "         -26.3048, -20.9953, -15.0476, -15.2857, -15.9517, -15.7353,  -9.5314,\n",
            "         -12.3280, -20.5056, -21.0531, -15.7881,  -8.9440, -17.8670, -12.1226,\n",
            "         -15.7927, -23.6496, -22.1762, -12.2365, -20.6662, -20.2757, -21.3221,\n",
            "         -23.2798, -24.4548, -17.8161, -11.2651, -13.9389, -30.9437, -11.5341,\n",
            "          -6.9985, -15.5866, -13.4815, -21.2164, -16.7681, -20.2582, -15.8213,\n",
            "         -17.9396, -18.7505, -18.3446, -19.4819, -11.7649, -13.2463, -16.8118,\n",
            "         -21.8678, -18.7326, -12.8226, -12.2403, -12.1537, -21.2648, -27.6006,\n",
            "         -27.7922, -18.7233, -23.7615, -13.2895, -12.0391, -30.9699, -12.4589,\n",
            "         -25.2098, -20.4977, -18.3045, -25.0335, -21.1064, -27.4211, -23.3253,\n",
            "         -20.7963, -22.7407, -13.2233, -19.6891, -22.3076, -19.1625, -20.7694,\n",
            "         -14.3250, -22.8915, -18.1614, -25.7995, -20.8858, -20.6187, -18.7334,\n",
            "         -20.0323, -19.7425, -25.1296, -21.2551, -23.0283, -32.8104, -18.0891,\n",
            "         -22.8589, -24.5707, -28.0873, -21.5927, -16.1709, -20.7592, -18.7933,\n",
            "         -21.2337, -21.1153, -19.0001, -18.4490, -21.0720, -20.0564, -15.9216,\n",
            "         -16.3982, -18.7738, -17.9052, -16.4212, -17.2198, -19.9773, -18.7947,\n",
            "         -21.2991, -16.8965, -11.8216, -27.5204, -18.9351, -13.7263, -11.8245,\n",
            "         -26.4294, -17.9998, -17.9316, -15.6299, -16.1033, -14.1609, -16.0475,\n",
            "         -16.1594, -15.7924, -13.5485, -24.4990, -20.7062, -18.6068, -21.8990,\n",
            "         -17.9150, -23.6864, -27.9042, -15.0638, -17.7395, -24.6105, -22.4650,\n",
            "         -21.3630, -20.5119, -22.5263, -21.8828, -14.3626, -17.9134, -26.5665,\n",
            "         -13.8975, -14.2982, -13.9657, -24.5046, -28.1722, -18.1120, -24.5659,\n",
            "         -19.0448, -18.9794, -19.3117, -24.6643, -14.3941, -18.6539, -18.9518,\n",
            "         -24.1690, -19.5975, -16.9934, -23.4765, -27.5164, -22.9396, -15.9679,\n",
            "         -16.2703, -21.8018, -18.9249, -19.4262, -17.7380, -17.8439, -21.2223,\n",
            "         -24.5838, -22.5916, -17.5993, -22.9311, -29.2304, -11.3123, -22.7551,\n",
            "         -24.5635, -20.0780, -22.6443, -18.5133, -22.0415, -12.6232, -19.6740,\n",
            "         -15.4706, -18.9261, -20.3403, -18.4546, -18.1906, -17.8159, -19.4465,\n",
            "         -17.6095, -21.2948, -22.7738, -14.4793, -13.0501, -16.4938, -26.4816,\n",
            "         -18.0244, -16.8707, -24.1695, -14.9633, -25.0066, -19.5132, -25.9974,\n",
            "         -13.7515, -20.2416, -18.8527, -15.5114, -16.7099, -14.5940, -18.9988,\n",
            "         -20.3046, -17.4374, -11.8654, -23.7696, -19.5039, -15.3511, -21.3848,\n",
            "         -23.3498, -21.7065, -27.3678, -14.8868, -25.1724, -18.5460, -22.2201,\n",
            "         -15.6635, -14.7831, -18.6748, -16.3595, -24.9216, -16.1084, -14.0353,\n",
            "         -16.8436, -30.9807, -17.2261, -22.8497, -16.4868, -21.6105, -14.2551,\n",
            "         -13.8581, -26.6142, -18.5831, -14.7283, -14.6780, -23.4922, -14.9690,\n",
            "         -17.5485, -21.6946, -22.3138, -16.1822, -13.0129, -19.8803, -23.5088,\n",
            "         -16.1369, -18.9097, -21.0460, -13.7434, -16.9990, -16.0790, -22.4551,\n",
            "         -21.5575, -19.6466, -19.5683, -20.8721, -16.9100, -17.1433, -17.3225,\n",
            "         -16.8598, -15.0889, -18.7376, -19.3157, -22.9929, -16.2545, -27.9488,\n",
            "         -30.1760, -16.5883, -24.4938, -25.4967, -21.3356, -21.8439, -16.1648,\n",
            "         -21.2470, -13.3418, -17.3452, -20.1655, -17.9653, -23.6746, -14.9845,\n",
            "         -17.0336, -25.1993, -15.8081, -17.6101, -14.5041, -22.7279, -19.0846,\n",
            "         -30.4007, -18.0915, -19.0989, -14.6815, -23.0237, -17.1128, -20.7648]])\n",
            "\n",
            "tgt:\t tensor([ 92, 341,  86,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t2.879318\n",
            "pred:\t tensor([[-22.5311, -22.7234, -11.9638,  -6.7441, -15.1684,  -7.2084, -11.6927,\n",
            "          -6.3038, -13.6670, -11.8699, -20.6351,  -7.5564, -16.2826, -20.2068,\n",
            "         -11.0261, -17.4426, -17.5833, -19.1735, -23.3509, -14.2655, -15.5283,\n",
            "         -15.7331, -17.7895, -24.8636, -12.7632, -14.6407, -24.9158, -22.2443,\n",
            "         -23.6199, -25.7439, -19.1121, -23.1922, -13.0115, -13.5740, -16.6966,\n",
            "          -3.9462, -11.4387, -10.6148, -11.8208, -20.6206, -12.3343, -13.4244,\n",
            "         -15.4643, -16.0634, -22.9875, -14.7869, -15.7102, -18.4073, -12.1293,\n",
            "         -13.4726, -16.6602, -17.3123, -16.8299, -19.2363, -17.0166, -18.2051,\n",
            "         -22.6849, -16.0163, -18.5552, -18.1367, -16.2192, -21.1808, -25.4597,\n",
            "         -17.1763, -11.5170, -18.0019, -19.2540, -18.7140, -23.0433, -21.3368,\n",
            "         -11.9124, -20.8081, -12.3621, -16.5064, -16.1095, -14.0625, -22.3563,\n",
            "         -17.7094, -12.3857, -14.4243, -11.4957, -14.8202, -19.4332, -26.0682,\n",
            "         -18.8280, -13.9404, -19.5770, -21.2843, -17.5531, -24.9473, -12.8441,\n",
            "         -18.1719, -21.6909, -25.8345, -15.8204, -25.1912, -12.0295, -27.2936,\n",
            "         -21.1572, -16.3893, -19.5019, -11.5080, -13.1273, -12.2567, -19.3875,\n",
            "         -18.4451, -11.0032, -15.7227, -17.9526, -18.0062, -21.0197, -23.2925,\n",
            "         -11.5617, -13.7917, -16.0196, -20.2712, -20.8111, -17.3918, -21.7805,\n",
            "         -26.9579, -20.9578, -17.7053, -15.6013, -19.4498, -16.5794, -12.7642,\n",
            "         -13.5693, -18.3027, -19.2518, -13.1158, -10.3090, -20.8141, -11.7169,\n",
            "         -12.9119, -25.8420, -20.5935, -11.3552, -15.0636, -22.1944, -17.4981,\n",
            "         -23.6208, -25.0477, -17.8710, -10.6829, -14.2395, -27.9044, -11.7215,\n",
            "          -7.2927, -14.8610, -14.5857, -26.1289, -18.8911, -20.8061, -17.2635,\n",
            "         -20.1697, -18.3324, -18.0228, -20.2019, -11.4883, -15.7752, -17.3895,\n",
            "         -20.7164, -18.9363, -14.1154, -12.1586, -11.7509, -23.7100, -29.3052,\n",
            "         -24.2358, -16.9924, -23.4892, -14.1578, -13.5364, -30.8950, -12.2435,\n",
            "         -24.3991, -21.7945, -16.9883, -23.8093, -20.1320, -22.3446, -23.5192,\n",
            "         -22.5174, -20.3825, -13.8390, -19.6424, -19.7518, -20.8179, -18.4410,\n",
            "         -14.9322, -20.8348, -16.5558, -19.7449, -20.5678, -23.0626, -20.3724,\n",
            "         -17.8413, -20.3744, -23.1262, -23.4868, -23.9002, -29.7901, -18.8969,\n",
            "         -22.7030, -24.4255, -28.0465, -19.1659, -15.5307, -21.6008, -20.4348,\n",
            "         -19.5891, -21.3873, -20.4311, -16.4392, -20.2789, -18.0417, -17.9772,\n",
            "         -17.2272, -20.2513, -19.3555, -12.7923, -15.2045, -16.7272, -15.6770,\n",
            "         -20.1318, -15.4758, -10.4670, -25.4338, -18.9017, -16.1934, -16.8737,\n",
            "         -24.2681, -16.8977, -23.2076, -17.5156, -19.2024, -17.9966, -17.3247,\n",
            "         -20.0976, -16.5227, -15.1438, -26.4234, -22.7456, -16.3263, -22.1717,\n",
            "         -20.6885, -27.5536, -25.7039, -14.6906, -18.4862, -23.8392, -24.9981,\n",
            "         -19.1345, -22.9462, -23.7353, -20.2757, -16.2532, -18.6093, -24.1920,\n",
            "         -16.3716, -13.4802, -12.4146, -24.6405, -30.1624, -18.3503, -24.3478,\n",
            "         -21.0081, -17.8482, -18.9273, -23.6149, -15.9713, -20.3998, -19.2696,\n",
            "         -22.7080, -20.5358, -17.8765, -27.5799, -27.7082, -22.5822, -16.7118,\n",
            "         -19.2095, -20.4847, -17.7404, -16.4816, -19.8145, -14.2759, -24.1559,\n",
            "         -23.8625, -24.2955, -17.8981, -24.0996, -23.4713, -14.8709, -22.8319,\n",
            "         -24.3893, -17.2586, -25.8658, -18.6522, -24.1564, -14.8398, -17.7474,\n",
            "         -15.1251, -19.4838, -15.6316, -21.0024, -15.0113, -20.9909, -21.8872,\n",
            "         -16.7458, -20.4123, -22.3306, -15.9794, -13.8295, -14.0798, -25.3065,\n",
            "         -15.6134, -15.7608, -19.0541, -15.9161, -22.9172, -21.0154, -25.6500,\n",
            "         -12.7288, -21.8060, -17.6502, -18.6297, -16.5642, -18.1261, -19.7296,\n",
            "         -19.3009, -18.1805, -14.3174, -24.1390, -19.9963, -16.0173, -23.1560,\n",
            "         -23.4399, -21.3545, -27.0830, -14.3351, -26.9994, -19.4297, -22.9580,\n",
            "         -17.5963, -12.0165, -19.5828, -18.9982, -24.2985, -15.7190, -16.4326,\n",
            "         -17.1114, -26.7262, -14.7286, -18.1863, -18.1050, -19.5361, -13.6207,\n",
            "         -13.0391, -27.8530, -19.9006, -18.6098, -17.2690, -26.8536, -21.5721,\n",
            "         -16.9837, -23.4533, -24.1975, -19.1993, -12.6147, -20.0226, -24.6312,\n",
            "         -17.3211, -20.2935, -20.9434, -13.8924, -19.1661, -19.4706, -21.7503,\n",
            "         -22.3852, -19.2142, -18.7090, -18.9274, -20.9272, -17.0474, -17.3823,\n",
            "         -20.2256, -14.9384, -16.2734, -18.1193, -22.5541, -16.7224, -26.2457,\n",
            "         -28.1616, -15.2242, -21.2766, -22.0643, -26.1095, -23.3604, -17.7605,\n",
            "         -18.2977, -15.2965, -16.0833, -21.9140, -18.3048, -23.4171, -19.3924,\n",
            "         -13.5561, -25.1719, -15.6437, -17.1546, -18.5347, -21.5851, -20.0936,\n",
            "         -31.0600, -19.7197, -17.4018, -16.6849, -22.4940, -22.1282, -26.6863]])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjXBvNX-urJn"
      },
      "source": [
        "### LSTM Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md5DstyZurJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "96a95b70-ca50-4184-dc29-29478f07f5d0"
      },
      "source": [
        "plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n",
        "plt.title('Loss Curve of LSTM Attention')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of LSTM Attention')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioodyLC9urJq"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 70% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGfooV5kurJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae090b12-f166-4b8e-fd48-8753f3a39e19"
      },
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_lstm(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  \n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_lstm(lstm_net, train_iter, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['reste', 'à', 'unk', '.', ',', '!', 'en', 'en']\n",
            "\n",
            "tgt:\t ['reste', 'à', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', '!', 'unk', '!', '!', '!', '!', '!']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t [\"j'étais\", 'unk', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "tgt:\t [\"j'étais\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t [\"c'est\", 'unk', '.', 'unk', '.', 'en', 'en', 'en']\n",
            "\n",
            "tgt:\t [\"c'est\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', \"l'ai\", 'unk', '.', 'unk', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['je', \"l'ai\", 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.8510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmfQ5KM7urJs"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Recurrent Neural Networks can capture long-range, variable-length sequential information, but updating the current state relies on the previous states. Thus it cannot be parallelized across the entire sequence. In contrast, CNNs are easy to parallelize but they cannot capture sequential dependency within variable-length sequences and their receptive field is limited. Transformers resolve this dilemna by being able to capture long-range dependencies while also being easy to parallelize.\n",
        "\n",
        "Transformers consist of several different components and we will walk you through each of them. The original paper can be found [here](https://arxiv.org/pdf/1706.03762.pdf). [Here](http://jalammar.github.io/illustrated-transformer/) is a very informative blog about transformers that should also be a good reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhPWtYtFurJt"
      },
      "source": [
        "**Multi-Head Self-Attention**\n",
        "\n",
        "Multi-head self-attention is an extension of the dot-product attention we've previously implemented. The \"self-attention\" part means that the query, key, and value all come from the same sequence. For a sentence, this means that we are looking at how each word pays attention to other words in the same sentence. The \"multi-head\" part means instead of only having one attention map, we can have multiple. This means that for a given word in the sentence, it can pay attention to different parts of the sentence.\n",
        "\n",
        "The steps in the multi-head attention can be summarzied by the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTTsDlwTurJt"
      },
      "source": [
        "   1. The multi-head self-attention takes the initinal query $Q$, key $K$, and value $V$ as input. Note that, if not provided specifically, usually these are set to the same input embeddings $X=Q=K=V$ initially.\n",
        "   \n",
        "   1. Then, a linear projection is applied to $Q,K,V$ sepearately for each head $i=1,\\dots,h$. \n",
        "      $$\n",
        "   Q_i = QW^{Q}_i, K_i = KW^{K}_i, V_i = VW^{V}_i, i \\in [0, \\dots, h-1],\n",
        "   $$\n",
        "   \n",
        "   where $W^Q_i \\in \\mathcal{R}^{d_{model} \\times d_k}, W^K_i \\in \\mathcal{R}^{d_{model} \\times d_k}\\text{, and } W^V_i \\in \\mathcal{R}^{d_{model} \\times d_v}$. Note that these weight matrices are the only trainable parameters in the transformer.\n",
        "     \n",
        "   1. Apply the scaled dot-product attention to each of these projected set of queries, keys, and values:\n",
        "   $$\n",
        "   \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_iK^T_i}{\\sqrt{d_k}})V_i\n",
        "   $$\n",
        "   \n",
        "   1. Concatenate all the heads together and project it with another learned linear projections: \n",
        "   \n",
        "   $$\n",
        "   \\text{O} = \\text{Concate(head}_1, \\dots, \\text{head}_h) \\\\\n",
        "   \\text{MultiHead}(Q, K, V) = \\text{O}W^o, \\hspace{10mm} \\text{where } W^o \\in \\mathcal{R}^{{hd_v} \\times d_{model}}\n",
        "   $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5j6y3pZurJu"
      },
      "source": [
        "A good visualization from the above referenced [blog](http://jalammar.github.io/illustrated-transformer/) is shown below. Transformer stacks several multi-head attention modules together. For the first multi-head layer, the input is from the dataset, so an additional embedding layer is needed to project the input sequence into the appropriate dimensions. For subsequent layers, the output from the layer previous layer is directly used as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV_wYZYBurJu"
      },
      "source": [
        "<div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f22637507515b0f0dc58316cd5df365c",
          "grade": false,
          "grade_id": "cell-41f2921301bf3992",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "z4RcyYNeurJv"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, dk, num_heads,  **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, the same d_model in paper, feature dimension of query/key/values\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      num_heads: int, number of heads used for this MultiHeadAttention\n",
        "    \"\"\"\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = DotProductAttention()\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the linear mappings for the query, key, and values.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.d_model = d_model\n",
        "    self.d_k =dk\n",
        "    \n",
        "    self.q_linear = nn.Linear(d_model, num_heads*dk)\n",
        "    self.v_linear = nn.Linear(d_model, num_heads*dk)\n",
        "    self.k_linear = nn.Linear(d_model, num_heads*dk)\n",
        "    self.out = nn.Linear( num_heads * dk ,d_model)\n",
        "    \n",
        "    self.attention=DotProductAttention()\n",
        "    # END OF YOUR CODE\n",
        " \n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, T, d_q)\n",
        "      key: tensor of size (B, m, d_k)\n",
        "      value: tensor of size (B, m, d_v)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, T is length of sequence, d_q, d_k, and d_v are the feature dimensions of query,\n",
        "      key, and value. In the paper, d_q = d_k = d_v.\n",
        "\n",
        "    Outputs:\n",
        "      attention\n",
        "      \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of MultiHeadAttention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    \n",
        "    B, T_k, d_model = key.shape\n",
        "    T_q = query.shape[1]\n",
        "    T_v = value.shape[1]\n",
        "\n",
        "    num_heads = self.num_heads\n",
        "    d_k = self.d_k\n",
        "\n",
        "    q = self.q_linear(query)\n",
        "    k = self.k_linear(key)\n",
        "    v = self.v_linear(value)\n",
        "\n",
        "    valider_length = valid_length.repeat(num_heads)\n",
        "\n",
        "    k_reshape = torch.reshape(k,(B, T_k, num_heads, d_k))\n",
        "    k_reshape = k_reshape.permute(0, 2, 1, 3)\n",
        "    k_reshape = torch.reshape(k,(B * num_heads, T_k, d_k))\n",
        "\n",
        "    q_reshape = torch.reshape(q,(B, T_q, num_heads, d_k))\n",
        "    q_reshape = q_reshape.permute(0, 2, 1, 3)\n",
        "    q_reshape = torch.reshape(q,(B * num_heads, T_q, d_k))\n",
        "\n",
        "    v_reshape = torch.reshape(v,(B, T_v, num_heads, d_k))\n",
        "    v_reshape = v_reshape.permute(0, 2, 1, 3)\n",
        "    v_reshape = torch.reshape(v,(B * num_heads, T_v, d_k))\n",
        "\n",
        "    att = self.attention(q_reshape, k_reshape, v_reshape, valider_length)\n",
        "\n",
        "    att_reshaped = torch.reshape(att, (B, num_heads, T_q, d_k))\n",
        "    att_reshaped = att_reshaped.permute(0, 2, 1, 3)\n",
        "    att_reshaped = torch.reshape(att_reshaped, (B, T_q, num_heads*d_k))\n",
        "    \n",
        "    attention = self.out(att_reshaped)\n",
        "    # END OF YOUR CODE\n",
        "    return attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec3fhKWsurJv"
      },
      "source": [
        "Implement the MultiHeadAttention class below:\n",
        " - Complete the __init__() function, where the linear mappings for query, key, and values should be created.\n",
        " - Complete the forward() function, where the multi-head attention is performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWvFpRI6urJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c13a05-a7ce-4d77-8f8d-27dec0947106"
      },
      "source": [
        "cell = MultiHeadAttention(5, 90, 9)\n",
        "X = torch.ones((2, 4, 5))\n",
        "valid_len = torch.tensor([2, 3])\n",
        "cell(X, X, X, valid_len).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdFNtIkGurJz"
      },
      "source": [
        "### Position-wise Feed-Forward Network\n",
        "\n",
        "Another key component in the Transformer block is the position-wise feed-forward network (FFN). It's called position-wise FFN because the linear mapping is applied to each position separately and identically. For example, for an embedded input of size $N \\times T \\times D_{in}$, there are $N*T$ vectors of dimension $D_{in}$. If we apply a one layer position-wise FFN with weights of size $D_{in} \\times D_{out}$. The linear projection will be applied to each of the $N*T$ vectors separately and identically. Thus, the output would have size $N \\times T \\times D_{out}$. Another way to think about this is that this is the same as a 1x1 convolution mapping from $D_{in}$ channels to $D_{out}$ channels.\n",
        "\n",
        "Transformers stack two layers of position-wise FFN together, with a ReLU activation in between:\n",
        "\n",
        "$$\n",
        "\\text{PositionWiseFFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "Complete the class PositionWiseFFN:\n",
        "\n",
        "- Complete the __init__() function, where two position-wise FFN should be created.\n",
        "- Complete the forward() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "144103b4a6258c267c6a327e33851042",
          "grade": false,
          "grade_id": "cell-39ca6d33ccf1a8f5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-_oiC7qYurJ0"
      },
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "  def __init__(self, input_size, ffn_l1_size, ffn_l2_size):\n",
        "    super(PositionWiseFFN, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of the input\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the feed forward network for PositionWiseFFN\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.W_1 = nn.Linear( input_size,ffn_l1_size)\n",
        "    self.W_2 = nn.Linear( ffn_l1_size,ffn_l2_size)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      X: tensor of size (N, T, D_in)\n",
        "    Output:\n",
        "      o: tensor of size (N, T, D_out)\n",
        "    \"\"\"\n",
        "    o = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass of PositionWiseFFN\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    temp=F.relu(self.W_1(X))\n",
        "    o = self.W_2(temp)\n",
        "    # END OF YOUR CODE\n",
        "    return o\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TMqvkmHurJ2"
      },
      "source": [
        "Check your result. Expected output\n",
        "\n",
        "```\n",
        "[[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
        "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
        "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUlEkiN9urJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd00706c-ddf0-47d5-fbc0-dc62635990d7"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "ffn = PositionWiseFFN(4, 4, 8)\n",
        "ffn(torch.ones((2, 3, 4)))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
              "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
              "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkfUnd3urJ8"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Replacing RNNs with the multi-head attention layer and applying the position-wise feed-forward network makes the computation parallelizable since these modules compute the output of each item in the sequence independently. However, since every item is processed in parallel, there is no notion of ordering of the sequence. For an input sentence, this means that the transformer doesn't know the ordering of the words in the sentence. For most tasks, this ordering is very important. To address this, transformers propose adding a positional encoding to each input that corresponds to the position in the sequence. This means that we take the position of each word in the sentence (eg. 0, 1, 2, etc...) and map it to some $d_{model}$-dimensional embedding. We then add this embedding with every input item so that the input is not position-aware. Transformers use the following sinusoidal positional encoding:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "PE_{(pos, 2i)} &= sin(pos / 10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} &= cos(pos / 10000^{2i/d_{model}}) \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "An example borrowed from this [blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) can give an ituition how this positional encoding works. Suppose you want to encode the number from $0$ to $8$ using binary encoding, the result would like this:\n",
        "$$\n",
        "\\begin{align*}\n",
        "0: && 0  0  0 \\\\\n",
        "1: && 0  0  1 \\\\\n",
        "2: && 0  1  0 \\\\\n",
        "3: && 0  1  1 \\\\\n",
        "4: && 1  0  0 \\\\\n",
        "5: && 1  0  1 \\\\\n",
        "6: && 1  1  0 \\\\\n",
        "7: && 1  1  1 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "Note the frequency of ones in each digit is different. Thus, words at different locations will have different embedding features (digits in the example). The figure below visualized a position encoding matrix of dimension $\\mathcal{R}^{50 \\times 128}$\n",
        "\n",
        "The forward pass of the positional encoding should add the positional embedding to the input:\n",
        "\n",
        "`y = x + positional_encoding(x)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pV5fWGhurJ8"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY9s_PQsurJ8"
      },
      "source": [
        "Complete the class PositionalEncoding:\n",
        "- Complete the __init__() function, where the tensor $PE$ should be created.\n",
        "- Complete the forward() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6f724034b91387df0b62a10fb7268858",
          "grade": false,
          "grade_id": "cell-fc6933701b990269",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cACZvxgGurJ9"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, dim, device, max_len=1000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      dim: feature dimension of the positional encoding\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize positional encoding. You should create `self.pe`\n",
        "    # here according to the definition above. The positional encoding should\n",
        "    # support up to position `max_len`.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.device=device\n",
        "    pe = torch.zeros(max_len, dim)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-np.log(10000.0) / dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "    # END OF YOUR CODE\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D_in)\n",
        "    Output:\n",
        "      Y: tensor of the same size of X\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for positional encoding. After getting the positional\n",
        "    # encoding with regards to the time dimension, add it to the input X.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    encoded = self.pe[:X.size(1)].transpose(0,1).to(device=self.device)\n",
        "\n",
        "    \n",
        "    #tested with arbitrary zeros arrays in the same shape of X, encoded, still errored\n",
        "\n",
        "    #now just gonna check to make sure they're not the same\n",
        "\n",
        "    wouldFail = False\n",
        "\n",
        "    if(encoded.dim() == X.dim()):\n",
        "      for i in range(1, encoded.dim()):\n",
        "        if(X.size(i) != encoded.size(i)):\n",
        "          wouldFail = True\n",
        "      \n",
        "    else:\n",
        "      wouldFail = True\n",
        "\n",
        "    if wouldFail:\n",
        "      raise ValueError(\"Stopped the addition before it could crash your cuda\")\n",
        "    else:\n",
        "      Y = X + encoded\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChUijJyyurKA"
      },
      "source": [
        "Check your result. Expected output\n",
        "\n",
        "```\n",
        "[[1., 2., 1., 2., 1., 2., 1., 2., 1., 2.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hp83OSLurKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b723d815-7d24-4711-deaf-d9553a49dcc8"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "pe = PositionalEncoding(10, device)\n",
        "pe(torch.ones((2, 1, 10), device=device))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 1., 2., 1., 2., 1., 2., 1., 2.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO0nsV2SurKD"
      },
      "source": [
        "### Add and Norm\n",
        "\n",
        "Transformers use a residual connection followed by a layer normalization layer to connect the inputs and outputs of other layers. To be specific, an \"add and norm\" layer is appended after each multi-head attention layer and the position-wise FFN layer. *The code for AddNorm Layer is given as below.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ8iPO-ourKD"
      },
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, dropout, embedding_size):\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(embedding_size)\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        return self.norm(self.dropout(Y) + X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFY6huNqurKF"
      },
      "source": [
        "### Encoder and Decoder\n",
        "\n",
        "The following figure gives a simple example of how the Transformer is built on these components introduced above. It's easy to see that the encoder of the Transformer consists of several identical encoder blocks, and so does the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU3Rneh_urKG"
      },
      "source": [
        "<div>\n",
        "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h583BASVurKG"
      },
      "source": [
        "Complete the forward() function for the EncoderBlock and DecoderBlock. Note that, for the decoder, when applying self-attention, the sequential queries **cannot** attend to those at later time steps. For example, in a sequence, the query entry at time step 5 can only observe the first 5 entries. You can use `valid_length` to enforce this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a74c7336c9f0f3646b41cd064e1ebadf",
          "grade": false,
          "grade_id": "cell-544ed4c7777ee925",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Dk3B40g_urKH"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads, dropout):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      \n",
        "    \"\"\"\n",
        "    self.attention = MultiHeadAttention(d_model, d_k, num_heads)\n",
        "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
        "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
        "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
        "\n",
        "  def forward(self, X, valid_length):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      valid_length: tensor of size (N), valid lengths for each sequence\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the EncoderBlock. Use the figure above\n",
        "    # for guidance:\n",
        "    # attention -> add+norm -> feed forward -> add+norm\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    X=X.to(\"cuda\")\n",
        "    valid_length=valid_length.to(\"cuda\")\n",
        "    Y = self.addnorm_1(self.attention(X, X, X, valid_length), self.attention(X, X, X, valid_length))\n",
        "    \n",
        "    Y=self.addnorm_2(self.ffn(Y), self.ffn(Y))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "77dbfb80f87e8bc1beb98efaa7c01e61",
          "grade": false,
          "grade_id": "cell-8d2fafea68ccbc9f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ijjFs9ryurKJ"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads,\n",
        "             dropout, **kwargs):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      \n",
        "    \"\"\"\n",
        "    self.attention_1 = MultiHeadAttention(d_model, d_k, num_heads)\n",
        "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
        "    self.attention_2 = MultiHeadAttention(d_model, d_model, num_heads)\n",
        "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
        "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
        "    self.addnorm_3 = AddNorm(dropout, d_model)\n",
        "\n",
        "  def forward(self, X, **kwargs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      **kwargs: other arguments you think is necessary for implementation\n",
        "    Outputs:\n",
        "      Y: tensor of size (N, T, D_out)\n",
        "      \n",
        "      Feel free to output variables if necessary.\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the DecoderBlock. Use the figure above\n",
        "    # for guidance:\n",
        "    # self attention -> add+norm -> enc-dec attention -> add+norm -> feed forward -> add+norm\n",
        "    # for the first attention layer, make sure to construct a `valid_length` that\n",
        "    # ensures each element cannot attend to later elements in the sequence.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    enc_valid_length = kwargs['enc_valid_len']\n",
        "    enc_outputs = kwargs['enc_outputs']\n",
        "    #print(\"got to the decoder block\")\n",
        "    atten1 = self.attention_1(X, X, X, enc_valid_length)\n",
        "    addnorm1 = self.addnorm_1(X, atten1)\n",
        "    atten2 = self.attention_2(addnorm1, enc_outputs, enc_outputs, enc_valid_length)\n",
        "    addnorm2 = self.addnorm_2(addnorm1, atten2)\n",
        "    Y=self.addnorm_3(self.ffn(addnorm2),self.ffn(addnorm2))\n",
        "    \n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClQJ7nakurKK"
      },
      "source": [
        "### Transformer  Implementation\n",
        "\n",
        "By stacking two encoder blocks and two decoder blocks, build the Transformer using the above components. \n",
        "\n",
        "- Implement the Encoder of Transformer:\n",
        " - Complete the __init__() function with a word embedding layer and several EncoderBlocks.\n",
        " - Complete the forward() function\n",
        "- Implement the Decoder of Transformer\n",
        " - Complete the __init__() function\n",
        " - Complete the forward() function\n",
        "- Implement the Transformer\n",
        " - Complete the forward() function\n",
        " - Complete the predict() function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ae68e2f2cb0af15282eeaa8ff420a15c",
          "grade": false,
          "grade_id": "cell-33857dd68f665a2b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "nwZP0NuqurKL"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
        "               num_heads, num_layers, dropout, device):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      num_layers: number of encoder blocks\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement init() function for TransformerEncoder. See forward() notes\n",
        "    # for more details.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, device)\n",
        "    self.blks = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.blks.add_module(\"block\"+str(i),\n",
        "            EncoderBlock(d_model, num_layers, ffn_l1_size, ffn_l2_size, num_heads, \n",
        "                         dropout))\n",
        "    # END OF YOUR CODE\n",
        "    \n",
        "  def forward(self, X, valid_length):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the TransformerEncoder\n",
        "    # First, use an embedding so each element in X is d_model (hint: use nn.Embedding)\n",
        "    # Then, apply the positional embedding to each element\n",
        "    # Lastly, pass the resulting input into num_layers of EncoderBlocks\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    embedded = self.embedding(X.long())\n",
        "    X = self.pos_encoding(embedded * (self.d_model**.5))\n",
        "\n",
        "    for blk in self.blks:\n",
        "      X = blk(X, valid_length)\n",
        "    # END OF YOUR CODE\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "765b53867f3968948b51990b9c69f80f",
          "grade": false,
          "grade_id": "cell-eefed3b5de8d2f3d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "CNYdzjF7urKN"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
        "             num_heads, num_layers, dropout, device):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      num_layers: number of decoder blocks\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement init() function for TransformerDecoder\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, device)\n",
        "    self.blks = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "        self.blks.add_module(\"block\"+str(i),\n",
        "            DecoderBlock(d_model, num_layers, ffn_l1_size, ffn_l2_size, num_heads,\n",
        "                         dropout))\n",
        "    self.dense = nn.Linear(d_model, vocab_size)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      valid_length: tensor of size (N,), valid lengths for each sequence\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the TransformerDecoder. This will look\n",
        "    # very similar to the TransformerEncoder.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    enc_outputs,enc_valid_len=state\n",
        "    embedded = self.embedding(X.long()) #make it a long because it wants it to be for some reason\n",
        "    \n",
        "    X = self.pos_encoding(embedded *(self.d_model**.5))\n",
        "   \n",
        "    for blk in self.blks:\n",
        "        X = blk(X,enc_outputs=enc_outputs,enc_valid_len=enc_valid_len)\n",
        "    # END OF YOUR CODE\n",
        "    return self.dense(X), X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "68446581beb8f1b29dcb65023d32bcdf",
          "grade": false,
          "grade_id": "cell-1a06f1a890069bc5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_hH-JsmlurKQ"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
        "  def __init__(self, encoder, decoder, **kwargs):\n",
        "    super(Transformer, self).__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
        "    \"\"\"Forward function\"\"\"\n",
        "    loss = 0\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass of transformer\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    word_embedded = self.encoder(src_array, src_valid_len)\n",
        "    denseX , X = self.decoder(tgt_array, (word_embedded,tgt_valid_len))\n",
        "\n",
        "    for t in range(MAX_LEN+1):\n",
        "      \n",
        "      pred = denseX[:,t]\n",
        "\n",
        "      logsoft = F.log_softmax(pred)\n",
        "      nullloss = F.nll_loss(logsoft, tgt_array[:,t+1], ignore_index=0)\n",
        "      loss = loss + nullloss\n",
        "    # END OF YOUR CODE\n",
        "    return loss, pred\n",
        "\n",
        "  def predict(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement predict() of transformer\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    preds = []\n",
        "    word_embedded = self.encoder(src_array, src_valid_len)\n",
        "    \n",
        "    src = tgt_array[:, 0].unsqueeze(1)\n",
        "\n",
        "    for t in range(MAX_LEN+1):\n",
        "      denseX, X = self.decoder(src, (word_embedded,tgt_valid_len))\n",
        "\n",
        "      pred = denseX.argmax(dim=2)\n",
        "\n",
        "      src = torch.cat([src, pred], dim=0)\n",
        "      preds.append(pred)\n",
        "\n",
        "    pred = torch.cat(preds, dim=1)\n",
        "    # END OF YOUR CODE\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi46ZLe0urKS"
      },
      "source": [
        "Find a good learning rate for training this model. Feel free to tune other hyperparameters as well as long as your best model is saved in `transformer_net`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cdb66470a63f5f46080f7af308a72e99",
          "grade": false,
          "grade_id": "cell-535ea1c455d3fe85",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "B8b8Lhx6urKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b7fccb-6972-4007-9434-83b6ff5b66b1"
      },
      "source": [
        "def train_transformer(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "\n",
        "# hyper-params: feel free to modify the values and numbers of hyper-params \n",
        "\n",
        "# training\n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "#transformer hp\n",
        "d_model = 512\n",
        "ffn_l1_size = 2048\n",
        "ffn_l2_size = d_model\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `transformer_net` variable. Feel free to tune other hyperparameters\n",
        "# as well.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "lr = 0.00001\n",
        "# END OF YOUR CODE\n",
        "epochs = 12\n",
        "device = torch.device('cuda:0') # cuda:0 if you have gpu\n",
        "\n",
        "encoder = TransformerEncoder(vocab_eng.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
        "                             num_heads, num_layers, dropout, device=device)\n",
        "decoder = TransformerDecoder(vocab_fra.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
        "                             num_heads, num_layers, dropout, device=device)\n",
        "\n",
        "transformer_net = Transformer(encoder, decoder)\n",
        "transformer_loss_list = train_transformer(transformer_net, train_iter, lr, epochs, device)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 0 / 1872\tLoss:\t38.742046\n",
            "pred:\t tensor([-0.3781, -0.1587, -0.7206, -1.0226,  0.3627,  0.2787,  0.3608,  0.1971,\n",
            "         0.5054,  0.4903, -1.2768, -0.1827,  0.9892, -0.5534, -0.0101, -0.2190,\n",
            "         0.6070,  0.4100,  0.1721,  0.1258, -0.2674,  0.0052, -0.5844, -0.4673,\n",
            "         0.4848, -0.0097, -0.7945, -0.3389, -0.0803,  0.0821, -0.1950,  0.1691,\n",
            "        -0.6096, -0.5713, -1.7820,  0.1410,  0.2091,  0.4456,  0.4066,  0.0384,\n",
            "         0.7517,  0.4985, -0.2673, -0.8662,  0.5885,  0.0867, -0.6348, -0.3200,\n",
            "        -0.1258, -0.6295,  0.4339,  0.4579,  0.4009, -0.0391,  0.7571,  0.8017,\n",
            "         0.0464, -0.8940, -0.4169, -0.6217, -0.8781,  0.7436,  0.1407,  0.5292,\n",
            "         0.3341, -0.1593, -0.8674,  0.9463, -0.4035,  1.0590, -0.1420,  0.6159,\n",
            "         0.0377,  1.6513,  0.3273, -0.2668, -0.2711, -0.4839, -0.1859, -0.5771,\n",
            "         0.4772,  0.3435, -0.1662, -0.4716, -0.3170, -0.1542,  0.5798,  0.2035,\n",
            "        -0.4392, -0.8757, -0.5735, -0.3196,  0.5784,  0.2362,  1.1977,  0.7416,\n",
            "        -0.1570,  0.1270,  0.0553, -0.4868, -0.6899, -0.5261,  0.0872,  0.9624,\n",
            "        -0.9459, -0.3230, -0.1967, -0.1951,  0.2184, -0.5793, -0.1676, -0.6999,\n",
            "        -0.0491,  0.5395, -0.0621, -1.0278,  0.2924,  0.5039,  0.6810,  0.7726,\n",
            "         0.8207,  0.1225, -0.3918, -0.2183,  0.1849, -0.1857, -0.0257, -1.7395,\n",
            "        -0.8413, -0.0993,  0.2032,  0.1821, -0.7192,  0.1770,  0.3224, -0.8500,\n",
            "        -0.3663, -0.4202,  0.5835,  0.3468,  0.1626,  0.4938, -0.5332,  0.6121,\n",
            "        -0.2086, -0.6866, -0.6630, -0.1565, -0.4424, -0.9768,  0.1481, -0.7772,\n",
            "         0.0907,  0.4170,  0.0158,  0.6030,  1.0836,  0.1310, -0.2337,  0.1145,\n",
            "        -0.0953, -0.4949, -0.1021, -0.3661,  0.5832, -0.2808, -0.2736, -0.5252,\n",
            "        -0.4588,  0.3953,  0.2861,  0.6047, -0.1373, -0.1960,  1.5755, -0.0546,\n",
            "        -0.9127,  0.1693, -0.4132, -0.0033,  0.1676, -0.4911,  0.6131,  0.5049,\n",
            "         0.1988, -0.1097, -0.8513, -0.7972, -0.2963, -0.4011, -0.1643,  0.2162,\n",
            "        -0.9103, -0.1951,  0.0139, -0.2545,  0.2537, -0.3350, -0.1500,  1.1429,\n",
            "         0.3793, -0.4219,  0.1494,  0.5796,  0.6497,  0.6754, -0.1817,  0.0567,\n",
            "         1.2070, -0.9344,  0.9095, -0.7890,  0.6651,  0.6866, -0.8393, -0.6208,\n",
            "         1.2480, -0.2489,  0.9010,  0.5092, -0.8725, -0.8162, -0.1650,  0.2464,\n",
            "         0.4547,  1.7606,  0.0625,  0.1716, -0.7343, -0.7287, -0.6406,  1.3019,\n",
            "        -0.6647,  0.1240, -0.3437,  0.0096, -0.6270, -0.6352,  0.0335,  0.5376,\n",
            "        -1.0047,  0.2224,  0.5960,  0.2550,  0.6303,  0.1914, -0.9873,  0.4672,\n",
            "        -1.1142, -0.7239, -0.6461,  0.2480,  0.4846, -1.3281,  0.3428,  0.8112,\n",
            "         0.9940, -1.0506, -0.9764, -0.4612, -0.6383,  0.4965,  0.3516, -0.4225,\n",
            "         0.1054,  0.9695, -0.3969,  0.2655,  0.8535, -0.5144, -0.2624, -0.1733,\n",
            "        -0.3803,  0.0164, -0.3783, -0.3773,  1.5379,  0.4914,  0.2653,  0.4744,\n",
            "         0.4941, -0.3735,  0.3630, -0.5034,  0.5046,  0.3010,  0.1909,  0.0400,\n",
            "         0.3593, -0.0136,  0.0315, -0.8163, -0.4250,  0.3653, -0.3122,  0.3807,\n",
            "         0.1213, -0.0618,  1.0638, -0.5723,  0.1553, -0.0881,  0.6488,  0.3993,\n",
            "         0.3616, -0.8764, -0.0640, -0.5157, -0.0241,  0.0476, -0.1364, -1.3659,\n",
            "        -1.2000,  0.2203, -0.0776,  0.2244, -1.0084,  0.6301, -0.6427, -0.5130,\n",
            "         0.8342,  0.5127, -0.0723,  0.6712, -0.1321,  0.4309, -0.0392,  0.6382,\n",
            "         1.6761,  0.0953,  0.1212, -0.7864,  0.5482, -0.6256,  0.2194, -0.3555,\n",
            "        -0.0373,  0.5015, -0.7237, -0.9769,  0.3143,  0.2990,  0.0306, -0.2535,\n",
            "        -0.2451,  0.6575,  0.0829,  0.2756,  0.3467,  0.0257, -0.5140,  0.1383,\n",
            "         0.4875,  0.9996, -0.0362, -0.2421, -0.1137,  0.2133,  0.2228,  0.4872,\n",
            "         0.0694,  0.4612,  0.1186, -0.6459,  0.3877, -0.3419, -0.3102,  0.8810,\n",
            "        -0.4191,  0.1720,  0.6357, -0.9951,  0.0691, -0.0120, -0.9492, -0.5348,\n",
            "        -0.2349, -0.1964, -0.2638,  0.5863, -0.7569, -0.0020, -0.3441,  0.1360,\n",
            "         0.2387,  0.3047,  0.6828, -0.1085, -0.3037,  0.3666, -0.3726, -0.2997,\n",
            "        -0.3932, -1.6392, -0.3277, -1.0942, -0.0927, -0.6039, -0.0329,  0.3818,\n",
            "         0.2762,  1.2220,  0.6465,  0.8743,  1.4720,  0.2910, -0.4951, -0.4826,\n",
            "         0.3881,  0.9092, -0.3639,  0.2390,  0.0179,  0.6657, -0.8490, -0.0421,\n",
            "        -0.2578,  0.2834,  0.1904,  0.6602])\n",
            "\n",
            "tgt:\t tensor([ 14,  28, 207,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 156 / 1872\tLoss:\t12.958678\n",
            "pred:\t tensor([-1.3110e+00, -1.1903e-01,  8.5385e-01,  3.7223e+00, -1.8846e-01,\n",
            "         3.1211e+00,  1.6529e-03, -3.6146e-01, -9.0392e-02,  7.9918e-01,\n",
            "        -1.9424e-01,  3.0050e+00, -2.6664e-01, -5.1832e-02,  3.8199e-01,\n",
            "         1.0824e+00, -7.7242e-01,  1.5717e-01,  3.2984e-01, -4.2731e-01,\n",
            "        -1.6566e-01,  3.5592e-02, -5.0734e-01, -9.8334e-01,  3.4069e+00,\n",
            "        -1.3355e-01, -4.7360e-01, -6.9547e-01,  1.3311e+00, -9.0016e-02,\n",
            "        -1.2796e-01, -4.4161e-01, -7.4139e-01,  7.4525e-01,  5.3715e-01,\n",
            "         1.1658e+00,  8.6492e-01,  1.2799e+00,  6.5497e-01, -1.9091e-01,\n",
            "         6.6283e-01,  1.5972e+00,  2.5961e-01, -5.1673e-01, -3.0996e-01,\n",
            "         5.9411e-01, -8.5394e-01, -6.8715e-01,  1.2163e+00, -2.6095e-01,\n",
            "        -4.7669e-01,  2.1998e-01,  1.0585e+00, -4.7741e-01, -3.2376e-03,\n",
            "        -1.5018e-01, -6.4194e-02, -5.4880e-01, -2.3980e-01, -4.7886e-01,\n",
            "        -7.0739e-01, -7.8698e-01,  1.5582e-01, -1.0667e+00,  5.9440e-01,\n",
            "        -5.3420e-01,  2.5398e-01,  7.3253e-01,  6.8890e-01,  1.1528e-01,\n",
            "         1.7557e-01, -2.5956e-01,  1.2746e+00,  6.9158e-01,  8.8839e-01,\n",
            "         9.4824e-01,  3.7317e-01, -1.0325e-01,  8.9737e-02,  6.9930e-01,\n",
            "        -4.8088e-01,  7.5388e-02, -9.0290e-02,  1.0475e-02,  7.2677e-02,\n",
            "         7.0555e-01, -3.3078e-01, -7.9864e-01, -2.1344e-01,  7.3233e-01,\n",
            "         5.4105e-01,  2.4511e-02,  1.4537e-01, -5.8639e-01,  3.0857e-02,\n",
            "        -7.2490e-02,  1.4884e-01, -7.3962e-01,  2.8999e-01, -3.2720e-01,\n",
            "        -8.1335e-01, -7.7486e-01, -4.8173e-01, -5.9446e-01, -4.9765e-01,\n",
            "        -2.6137e-01, -2.6942e-01, -5.5499e-01,  2.0190e+00, -5.5214e-01,\n",
            "        -2.3874e-01, -8.4539e-01, -2.2963e-01,  6.4992e-01,  5.0114e-01,\n",
            "        -5.6760e-01,  5.9758e-01,  2.1276e-01, -3.0902e-01, -2.2473e-01,\n",
            "         6.3541e-01, -1.0490e-01,  5.9905e-01, -8.9165e-01, -8.4196e-02,\n",
            "         8.0210e-01, -5.1689e-01,  3.4104e-02,  1.1785e-01, -5.1832e-01,\n",
            "        -4.7465e-02, -1.6960e-01, -5.2923e-01,  8.6375e-02,  2.4492e-01,\n",
            "         7.4738e-01,  8.4759e-02, -4.0155e-01,  2.8013e-01, -6.1838e-01,\n",
            "        -1.0631e-01, -6.6594e-01, -8.1812e-01,  8.1112e-01, -4.8235e-01,\n",
            "        -7.3351e-01, -1.3848e-01, -1.1933e-01,  9.4146e-01, -9.3069e-02,\n",
            "        -4.0864e-01, -1.3433e-01,  2.0061e-01, -1.0218e-03, -3.7902e-02,\n",
            "        -7.7435e-02, -5.2876e-01, -1.1404e-01, -6.4174e-01, -4.5637e-01,\n",
            "         8.1426e-01,  2.7810e-01, -8.6765e-01,  3.9917e-01,  2.0561e-01,\n",
            "        -2.5794e-01, -3.8079e-01,  1.8797e-01,  1.1458e-01, -4.9568e-01,\n",
            "        -2.9358e-01,  1.2562e+00,  6.6966e-01, -8.5830e-01, -6.3057e-01,\n",
            "        -2.8057e-01,  7.3248e-02, -2.0319e-02, -3.9777e-01,  1.9773e-02,\n",
            "         1.1094e-01, -4.4935e-01, -8.1208e-01, -1.0082e+00,  3.3941e-01,\n",
            "        -7.9714e-01, -3.3344e-01, -6.8711e-01,  2.1320e-01,  7.9419e-01,\n",
            "         4.6794e-04,  1.3265e+00, -1.6279e-01, -1.0737e+00, -3.2562e-01,\n",
            "        -5.1899e-01, -1.3524e+00, -7.4390e-01,  5.0896e-01,  6.0683e-01,\n",
            "        -6.8534e-01,  3.6645e-01,  4.7178e-01,  9.6189e-01, -1.6062e-01,\n",
            "         4.9334e-01, -6.7828e-02,  1.0995e+00,  7.3439e-01, -1.0942e+00,\n",
            "         1.7477e-01, -3.9197e-01,  2.5843e-01,  6.6573e-01,  6.5193e-01,\n",
            "         3.3551e-01, -6.4652e-01,  9.8577e-02, -2.9074e-01,  8.1916e-01,\n",
            "        -7.2832e-01, -1.5040e-01, -7.3630e-01, -6.7014e-01, -2.1661e-01,\n",
            "         1.3087e+00,  7.6013e-01, -2.4022e-01,  3.0998e-01,  6.1474e-01,\n",
            "         7.0209e-02,  6.0530e-01, -2.9904e-01, -6.4518e-01, -4.9827e-01,\n",
            "        -2.2015e-01, -6.9806e-01,  5.5954e-02, -4.8806e-01, -6.6280e-01,\n",
            "        -1.2151e+00, -1.8041e-01, -7.6501e-02, -3.3161e-01,  2.6391e-01,\n",
            "        -8.9269e-01, -1.0907e+00,  1.4188e+00, -5.1603e-01, -6.1970e-01,\n",
            "        -3.6629e-04, -1.2614e-01,  5.4145e-01, -7.5711e-01,  7.3180e-01,\n",
            "        -8.0700e-02, -1.1314e-02, -3.6894e-03, -9.2143e-01, -5.9037e-01,\n",
            "        -1.6761e-01,  8.2231e-01, -2.1352e-01,  1.2578e-02,  8.4882e-02,\n",
            "        -4.8466e-01, -5.5726e-01, -2.4048e-01, -1.2917e+00, -4.9140e-01,\n",
            "        -9.2968e-01, -6.4398e-01, -2.2195e-01, -6.5099e-01, -2.1064e-01,\n",
            "        -4.7188e-01, -9.4403e-02,  5.4976e-01,  1.2198e-01, -4.4029e-01,\n",
            "         5.6067e-01,  2.4293e-01,  3.6535e-01, -1.0420e-01,  2.0942e-01,\n",
            "        -1.1570e+00,  6.3061e-03, -5.4497e-01, -7.1476e-01,  5.3488e-01,\n",
            "         5.4206e-02, -4.3169e-01,  1.7172e-01, -1.3195e-01, -6.4694e-01,\n",
            "         5.2112e-01, -7.1678e-01, -4.3402e-01, -4.2405e-02, -5.2177e-01,\n",
            "         4.6207e-01,  6.1418e-01,  1.8699e-01, -3.9079e-01, -1.1644e-01,\n",
            "        -5.1977e-01,  2.4940e-01,  4.6191e-01,  3.7838e-01,  5.7355e-02,\n",
            "        -4.3664e-02,  5.1920e-02,  2.1992e-01,  1.2814e-01, -3.0681e-01,\n",
            "        -1.6029e-01,  8.4576e-02,  9.9917e-02,  1.0302e-01, -1.4885e+00,\n",
            "        -8.3095e-02,  1.6883e-01, -7.2118e-01,  3.3574e-03, -2.3649e-02,\n",
            "        -4.2000e-01,  3.9814e-03,  1.2450e-01, -6.9088e-02, -2.8927e-01,\n",
            "        -2.2947e-01, -6.6788e-01, -6.2178e-01, -7.5202e-01,  9.2243e-01,\n",
            "        -5.2898e-01,  4.2780e-01,  3.2166e-01,  1.0447e-02, -7.2648e-01,\n",
            "         2.3521e-01,  7.3979e-01,  5.8720e-01,  2.2630e-01, -8.9971e-02,\n",
            "         4.6371e-01, -6.1937e-01,  1.1340e-01, -1.1034e+00, -4.1447e-01,\n",
            "         2.7115e-01, -2.0433e-02,  2.3989e-01, -2.9392e-02,  4.0437e-01,\n",
            "        -5.9597e-01, -1.0598e+00,  3.3249e-01, -9.2599e-01, -4.0104e-01,\n",
            "        -2.1449e-01,  2.6771e-01,  5.3957e-02,  8.8582e-02, -7.2812e-01,\n",
            "        -8.2378e-01, -8.4168e-01,  6.0393e-01, -5.0918e-01,  7.9128e-01,\n",
            "        -3.5728e-02, -1.8610e-01, -4.2307e-03, -1.6574e-01, -8.8898e-01,\n",
            "        -4.1588e-02, -9.9677e-01, -8.9939e-01, -3.7354e-01, -2.8507e-01,\n",
            "        -8.8976e-01, -1.1646e+00, -6.9261e-02, -5.1046e-01, -3.5638e-01,\n",
            "        -5.7527e-02, -3.3132e-02,  3.6974e-03, -1.3111e+00, -4.8750e-01,\n",
            "        -4.5805e-01, -3.7499e-01, -8.4524e-01, -5.8795e-01,  8.2873e-02,\n",
            "        -6.1813e-01, -9.4812e-01, -1.0837e+00, -9.9848e-02, -3.5343e-01,\n",
            "        -1.0111e+00,  1.5430e-01, -4.4755e-01, -5.4185e-01,  1.0787e-01,\n",
            "        -6.3936e-01, -2.1174e-01,  4.7642e-02,  3.1974e-01,  9.0874e-02,\n",
            "         2.2346e-01, -5.1502e-01, -6.7949e-01, -1.2432e+00, -7.6595e-01,\n",
            "         1.1267e-01, -5.8827e-01, -9.9248e-03, -6.6603e-01,  4.8327e-02])\n",
            "\n",
            "tgt:\t tensor([  3, 309,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 312 / 1872\tLoss:\t7.764785\n",
            "pred:\t tensor([-8.8032e-01,  2.5496e-02,  6.5548e-01,  8.3940e-03, -1.5080e-01,\n",
            "         4.6948e+00,  1.2478e+00,  2.5858e-01,  3.6576e-01,  1.6845e+00,\n",
            "         4.8085e-01,  2.2315e+00, -5.5409e-01, -5.6577e-01, -2.5644e-01,\n",
            "         1.4004e+00,  3.0601e-01, -1.3034e+00,  8.2327e-01, -3.3320e-01,\n",
            "         1.9417e-02, -1.7069e-01, -7.0452e-01,  3.9034e-01,  4.3515e+00,\n",
            "        -5.1536e-01, -6.0749e-01, -1.9494e-02, -9.8552e-01, -1.0034e-01,\n",
            "        -8.5953e-01, -1.1101e+00, -3.3930e-01, -5.0176e-01,  7.3276e-01,\n",
            "         4.6293e-01, -2.9711e-01,  3.0668e-01,  3.8819e-01, -2.2671e-01,\n",
            "         1.4785e+00,  1.4734e+00, -1.1746e-01, -5.7942e-01,  8.0093e-01,\n",
            "         9.1464e-01, -4.4201e-01, -6.8291e-01,  1.0449e+00, -4.7624e-01,\n",
            "         4.3372e-02, -2.8856e-02,  1.4111e+00, -1.8682e-01, -9.7523e-01,\n",
            "         7.3365e-01, -2.9381e-02,  4.3112e-02,  5.9777e-01,  3.0526e-01,\n",
            "        -7.8673e-01,  7.1276e-02, -8.9385e-02, -6.4517e-01,  2.8776e-01,\n",
            "         5.8789e-02, -1.2738e-01,  9.6219e-01, -5.7811e-01,  2.1576e-01,\n",
            "         6.0937e-01, -1.1536e-01,  1.0282e+00, -3.6287e-02,  8.9383e-01,\n",
            "         3.1393e-01,  7.5071e-01, -5.0103e-01, -8.3751e-02,  7.9830e-01,\n",
            "        -2.1310e-01,  1.1437e-01, -4.2279e-01,  1.2687e+00, -3.2105e-01,\n",
            "         1.1008e+00,  5.2100e-01, -2.3705e-01,  5.3479e-01,  7.4096e-01,\n",
            "         4.2496e-01, -5.6584e-01,  1.7270e-01, -7.5834e-01,  4.8810e-01,\n",
            "         1.0164e-01, -8.1815e-02, -4.6485e-02,  1.0905e-01, -1.1922e+00,\n",
            "         1.7678e-02, -4.0695e-01,  2.4625e-01, -5.8151e-01, -1.0031e+00,\n",
            "        -5.3557e-01, -8.5683e-01, -7.6501e-01,  1.4359e+00,  8.3586e-01,\n",
            "        -3.4995e-01, -3.3924e-01, -2.2614e-01,  7.3278e-01,  1.9109e-01,\n",
            "         2.2392e-01, -4.0217e-01,  2.6416e-01, -1.2164e-01,  3.5006e-01,\n",
            "        -5.4188e-03, -3.7241e-01,  1.2501e+00, -6.8431e-01, -4.6017e-01,\n",
            "         8.5024e-01,  6.3876e-02, -3.4590e-01,  7.0712e-01, -3.9523e-01,\n",
            "        -4.2774e-01,  2.2030e-01, -7.3854e-01, -1.3413e-01, -3.4428e-01,\n",
            "        -1.1159e-01, -4.9159e-01, -1.7163e+00, -4.9567e-02, -1.6925e-01,\n",
            "         4.6226e-02, -6.7926e-01, -2.1072e-01,  8.9491e-01, -3.5937e-01,\n",
            "        -3.1390e-01, -2.7562e-02,  7.1737e-01,  6.2889e-01,  1.9978e-01,\n",
            "         3.6050e-01,  3.1733e-02, -6.1980e-01, -1.7365e-01,  7.1707e-01,\n",
            "        -2.8871e-01, -3.3606e-01,  1.1836e-01, -1.0165e-01,  2.8008e-01,\n",
            "         2.9710e-01, -4.1588e-01, -6.6799e-01, -3.8514e-01,  1.2833e+00,\n",
            "        -9.2731e-01,  7.6189e-02,  3.8745e-02, -3.0408e-01, -3.1475e-01,\n",
            "        -3.9553e-02,  3.4608e-01,  6.0859e-01, -5.2039e-01, -6.6246e-01,\n",
            "         1.4104e-01,  3.6793e-01,  2.9405e-01, -7.1085e-01,  4.3476e-01,\n",
            "        -3.0612e-01, -3.3989e-01,  2.6932e-01, -6.4268e-01,  5.2511e-01,\n",
            "         2.3631e-01,  7.2472e-02, -7.4944e-01,  6.5650e-01,  1.2248e+00,\n",
            "        -4.9025e-01, -8.0625e-01, -6.8322e-01, -4.8205e-01,  8.5223e-01,\n",
            "        -4.6397e-02, -7.4509e-01, -5.4580e-01,  1.4860e-01,  9.6183e-01,\n",
            "        -2.7879e-01,  8.5248e-02,  1.0363e+00,  1.3480e+00, -7.7668e-02,\n",
            "         7.2916e-02, -7.6771e-01,  4.6473e-01,  1.0830e-01, -1.0913e+00,\n",
            "        -6.4620e-01, -3.6699e-01,  2.5989e-01,  9.8673e-03,  8.9618e-01,\n",
            "        -5.4018e-02,  1.1820e-01, -2.1327e-01,  1.4071e-01,  1.2950e+00,\n",
            "        -8.5700e-01, -6.1880e-01, -2.2297e-01, -5.9652e-01,  3.5172e-01,\n",
            "         1.0200e-01,  4.4031e-01, -1.0769e+00,  2.0744e-01,  2.2339e-01,\n",
            "        -3.1593e-01, -1.3404e-01, -8.2526e-01,  5.6105e-02, -3.8244e-01,\n",
            "        -1.1080e+00, -7.8454e-01,  1.0772e-01, -3.5505e-01, -6.3485e-01,\n",
            "        -1.0800e+00, -7.3662e-01,  8.8051e-02, -4.5121e-01, -9.9992e-02,\n",
            "        -7.2428e-01, -5.8928e-01,  5.2176e-01, -4.5410e-01, -4.3245e-01,\n",
            "        -7.6397e-01,  4.4863e-01,  4.9338e-01, -1.2304e+00,  6.2698e-01,\n",
            "         7.1222e-01, -2.9459e-01,  8.8073e-02, -1.3787e+00, -3.0130e-02,\n",
            "        -2.3182e-01, -3.0650e-01,  6.9963e-02, -6.2941e-01, -4.2228e-01,\n",
            "         1.9098e-01, -6.9807e-01,  1.7767e-01, -6.7263e-01, -5.0597e-01,\n",
            "        -5.4961e-01,  1.8876e-01, -5.0410e-01, -6.4309e-01, -4.1029e-01,\n",
            "        -1.9384e-03,  9.2114e-02, -2.4403e-01, -7.7192e-01, -6.9574e-01,\n",
            "        -2.2204e-01, -4.7807e-01,  1.5266e+00, -2.4151e-01,  7.9245e-01,\n",
            "        -1.3811e+00, -4.1855e-02,  9.8953e-01, -5.3246e-01, -3.2345e-01,\n",
            "        -6.6899e-01, -1.4890e-01,  1.3610e+00, -6.1030e-01, -3.6705e-01,\n",
            "         6.7640e-02, -8.4815e-01, -2.4799e-01,  4.4545e-01, -1.2180e+00,\n",
            "        -1.3272e-01,  4.4262e-01,  1.3598e-01, -8.7605e-02, -7.8497e-02,\n",
            "         1.3510e+00, -7.0584e-02,  1.0114e+00,  1.1710e+00,  2.3123e-02,\n",
            "        -4.2301e-01, -1.1407e-01,  1.0206e-02,  4.2541e-01,  6.6515e-02,\n",
            "        -2.8138e-01, -3.9354e-01, -2.5552e-01,  7.9356e-01, -4.0344e-01,\n",
            "        -1.3406e-01,  7.9078e-01, -1.4297e-01,  5.0900e-01,  1.7812e-01,\n",
            "        -7.1472e-01, -4.8466e-02, -5.3362e-01,  3.7866e-01, -5.8357e-01,\n",
            "        -6.5884e-01,  4.4319e-01,  1.9372e-01,  1.5127e-01,  4.5963e-01,\n",
            "         7.3638e-02,  2.9571e-01, -4.9870e-01,  2.7859e-01, -8.7412e-01,\n",
            "        -4.8129e-02, -1.8250e-01,  5.9336e-01, -8.6408e-02, -3.6497e-01,\n",
            "        -1.0680e-02, -9.9044e-01,  1.4875e-01, -3.2867e-01, -4.6389e-01,\n",
            "        -1.3117e-01, -7.2118e-01, -4.0291e-01,  7.4918e-02, -5.1215e-01,\n",
            "        -6.8057e-01, -2.2784e-01, -8.0926e-01, -5.8712e-01,  2.6807e-02,\n",
            "        -9.3055e-02, -9.5678e-01, -8.8584e-01,  7.0322e-02, -4.4758e-01,\n",
            "        -2.3697e-01,  4.0479e-01,  8.9196e-01, -6.1992e-01,  7.5336e-01,\n",
            "        -3.2694e-01,  1.1681e-01, -3.9073e-01, -6.2517e-01, -1.6735e-01,\n",
            "         1.5605e-01, -1.1289e+00, -4.0043e-01, -1.0332e-02,  1.0740e-01,\n",
            "        -5.3199e-01, -5.8347e-01,  1.7848e-01, -7.6007e-01, -5.1519e-01,\n",
            "        -1.5390e-01,  6.4478e-01,  3.0009e-01, -5.9020e-01, -9.6300e-01,\n",
            "        -3.1186e-03, -6.8280e-01, -9.5485e-01, -2.9722e-01, -6.3239e-01,\n",
            "         9.7844e-03, -2.9857e-03, -6.5274e-01, -1.0049e+00, -2.0298e-02,\n",
            "        -6.2932e-01, -9.6810e-02, -1.0027e+00, -1.3291e-01,  1.3022e-01,\n",
            "        -1.8377e-01, -6.9826e-01, -1.3241e-01,  6.6592e-01, -6.4461e-01,\n",
            "        -3.2185e-02, -4.3717e-01,  9.9939e-02, -6.1748e-01, -1.0593e-01,\n",
            "         6.1064e-01, -2.4407e-01, -4.3157e-01, -9.9745e-01, -6.7832e-02])\n",
            "\n",
            "tgt:\t tensor([266,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 468 / 1872\tLoss:\t7.424311\n",
            "pred:\t tensor([-3.7425e-01, -6.5555e-01,  6.7507e-01, -2.1212e+00, -1.2872e+00,\n",
            "         3.5687e+00,  7.1925e-01, -2.1737e-01,  1.3007e-01,  1.1669e+00,\n",
            "        -2.5776e-01,  6.2524e+00, -1.0340e+00, -1.3620e+00,  6.8011e-01,\n",
            "         1.2281e+00, -1.6387e-01, -2.3766e-01,  7.5415e-01, -2.9735e-01,\n",
            "        -2.4474e-01, -6.8139e-02, -9.4323e-01,  5.0074e-01,  2.1318e+00,\n",
            "        -1.7366e-01, -8.6990e-01, -5.5321e-01,  1.2378e+00, -9.7489e-01,\n",
            "        -9.1237e-01, -1.3836e-01,  5.5207e-01, -2.5476e-01,  3.2443e-01,\n",
            "         5.2312e-01, -9.4483e-01,  3.4827e-01, -5.6307e-01, -9.3773e-01,\n",
            "         8.7230e-01,  3.7350e-01, -5.1334e-01, -5.6244e-02,  1.8043e-01,\n",
            "        -2.8244e-02, -5.2737e-01, -4.2675e-01,  8.0206e-01,  6.8505e-01,\n",
            "         5.8784e-01, -9.1328e-01,  1.0683e+00, -4.3388e-01, -1.4543e-02,\n",
            "         6.1674e-01,  4.1898e-01, -4.7574e-01,  2.7147e-01,  3.1324e-01,\n",
            "        -1.6548e-01, -3.0044e-01, -1.7846e-01, -9.7486e-01, -4.9656e-01,\n",
            "        -6.0942e-01, -5.6542e-01,  1.5481e+00,  1.5315e-01,  2.2584e-01,\n",
            "         5.6583e-01,  4.3246e-01, -3.0241e-01, -1.1401e-01,  3.7798e-01,\n",
            "         9.6474e-01,  4.3414e-01, -9.4622e-01, -7.4576e-01, -3.4531e-01,\n",
            "        -2.2205e-01, -6.6096e-01, -2.1375e-01,  9.7951e-01,  3.0632e-01,\n",
            "         6.1925e-01,  1.3103e-01, -1.5388e-01,  6.3911e-01, -1.8332e-01,\n",
            "        -2.9362e-02, -1.9259e-01,  5.5563e-01, -9.6910e-01,  1.0208e+00,\n",
            "         1.9144e-01,  1.8583e-01, -1.0806e+00, -6.5233e-01, -7.6860e-01,\n",
            "        -5.2184e-01, -3.7223e-01,  3.2235e-01, -2.2220e-01, -1.2188e+00,\n",
            "        -4.8410e-01, -2.8757e-01, -1.0287e-01,  1.0722e+00,  3.3925e-01,\n",
            "        -6.9171e-01, -2.3013e-01,  6.6135e-01,  1.2696e+00,  9.3817e-02,\n",
            "        -6.8980e-01, -2.7627e-01,  9.8187e-01, -9.2793e-01,  7.7342e-01,\n",
            "        -2.1876e-01, -7.1986e-01,  5.4637e-01,  3.3485e-02, -8.6547e-02,\n",
            "         3.0642e-01, -3.1048e-01,  1.1153e-01,  2.9684e-01, -5.3627e-02,\n",
            "        -8.5367e-01,  3.4775e-01, -6.7223e-01, -6.6835e-03,  1.9887e-01,\n",
            "        -4.1122e-02,  1.6861e-01, -5.5181e-01, -3.6170e-01,  1.6443e-03,\n",
            "        -4.3208e-01, -5.0317e-01,  3.7756e-01,  7.5175e-02, -6.6995e-01,\n",
            "        -3.0975e-01,  2.8095e-01, -9.9362e-01, -1.1902e-01, -3.3790e-01,\n",
            "         6.7913e-01,  3.1686e-01, -8.8360e-02, -4.5499e-01,  2.8629e-01,\n",
            "         1.7206e-01, -2.7741e-01, -7.5864e-01, -2.1553e-01, -1.6657e-01,\n",
            "         4.2887e-02, -6.7784e-01, -4.0193e-01, -2.4043e-01,  1.1658e+00,\n",
            "        -1.1819e+00,  5.8927e-01,  6.4944e-01, -6.3602e-02, -7.3862e-01,\n",
            "        -1.4656e-01,  1.5492e+00,  5.4757e-02, -9.8512e-01, -4.4384e-01,\n",
            "        -3.0401e-01, -1.2842e+00,  1.6458e-01, -7.8315e-01,  5.6655e-01,\n",
            "        -3.4665e-01, -1.8486e-01,  4.9400e-01, -5.3689e-01,  5.9248e-01,\n",
            "        -6.9360e-01,  2.5625e-01, -4.9527e-01,  1.0056e-01,  1.1619e-01,\n",
            "        -2.6792e-02, -2.8078e-01, -5.7080e-01, -1.2442e+00,  8.3926e-01,\n",
            "         6.4477e-01, -8.5118e-01, -1.7119e-01,  1.9125e-01,  1.5685e+00,\n",
            "        -7.8626e-01,  2.2601e-01,  1.0158e-01,  4.9426e-01, -2.6223e-01,\n",
            "         6.7646e-02, -5.2410e-01,  2.0812e-01,  4.1881e-01, -8.6327e-01,\n",
            "         3.0806e-01,  5.2494e-01, -3.0874e-01, -3.2493e-01, -6.5405e-02,\n",
            "         2.7900e-01,  3.0081e-01, -5.2199e-01, -6.7675e-01,  8.8638e-01,\n",
            "        -7.9992e-01, -2.6991e-01, -2.6991e-01, -1.7171e-01,  4.6191e-01,\n",
            "         1.5131e-01,  9.5413e-01,  7.0661e-01,  2.4138e-01,  7.2581e-01,\n",
            "        -3.3260e-02, -5.1114e-01, -3.4387e-01, -4.5334e-01,  3.1763e-02,\n",
            "        -2.6838e-01, -3.4690e-01, -1.0322e+00,  4.2287e-01, -4.5314e-02,\n",
            "        -7.4488e-01, -2.0981e-01,  7.9226e-03,  4.3592e-01, -5.4992e-01,\n",
            "        -3.8549e-01,  2.4928e-01,  1.4888e-01, -5.1980e-01, -1.2371e-01,\n",
            "         1.3101e-02,  1.7618e-01, -3.1215e-01, -1.5295e+00,  1.2427e+00,\n",
            "        -9.1996e-02, -3.2356e-01, -7.9621e-01, -1.1784e+00, -5.0114e-01,\n",
            "         1.0661e-01,  2.0210e-01, -5.2020e-01, -5.0859e-01, -5.7408e-01,\n",
            "        -1.1587e-01, -9.5757e-01, -3.9009e-01, -9.0701e-02, -1.9722e-01,\n",
            "        -6.7465e-01, -6.9159e-01, -2.2436e-01,  4.5512e-01, -1.0969e-01,\n",
            "         2.5386e-01,  4.6411e-01, -1.4770e-01, -5.9781e-01, -3.0928e-01,\n",
            "        -5.8814e-02,  3.8859e-01,  9.8390e-01, -1.0797e+00, -3.1079e-01,\n",
            "        -9.5172e-02,  1.3486e-01,  2.0104e-01, -9.9600e-01, -1.1706e+00,\n",
            "        -4.2499e-01,  3.9766e-01,  2.6203e-01, -1.6313e-01,  7.2666e-01,\n",
            "        -8.7687e-01, -9.4428e-01,  2.1403e-01, -6.1807e-01, -9.8276e-02,\n",
            "         9.2947e-01,  1.3946e+00,  4.8911e-01, -3.2505e-01,  5.9744e-02,\n",
            "         5.2339e-01,  4.6922e-01,  8.5169e-01,  1.4876e+00,  4.6281e-01,\n",
            "        -1.6878e-01, -2.9314e-01, -7.9009e-01,  1.0308e+00, -8.9074e-01,\n",
            "        -1.4970e-01, -4.3203e-01,  8.0330e-01,  1.2652e+00,  3.1672e-01,\n",
            "         2.1098e-01,  1.5073e-02,  1.6242e-01,  1.1252e-02, -8.9676e-02,\n",
            "         1.2130e-01,  4.3965e-01,  5.2975e-01,  2.4656e-01,  4.0638e-01,\n",
            "        -5.2285e-01, -7.1616e-01, -3.9563e-01,  8.3728e-03,  7.9240e-03,\n",
            "        -2.8287e-01, -2.6660e-01, -1.6309e-01,  7.5940e-01, -4.9734e-01,\n",
            "         2.9004e-01,  2.4075e-02,  1.4085e+00, -3.7142e-01,  2.0046e-01,\n",
            "        -1.7286e-01, -1.7785e-01, -4.2305e-01, -1.0073e+00, -4.3814e-01,\n",
            "        -3.3522e-01,  2.5750e-01,  1.1139e-01, -1.5563e-01,  6.1949e-01,\n",
            "        -7.8507e-01, -1.4885e-01,  2.9699e-03, -1.5404e+00, -8.5837e-01,\n",
            "        -1.9375e-01, -8.4666e-01, -2.9697e-01, -6.2231e-02, -2.1696e-01,\n",
            "        -9.2343e-01, -7.7474e-03,  1.0061e+00, -1.0585e+00,  5.5078e-01,\n",
            "        -2.3479e-02, -3.6582e-01,  1.2080e-01,  3.1615e-01, -4.1700e-01,\n",
            "         6.0428e-01, -8.6260e-01, -6.0104e-01, -3.4020e-01,  2.0025e-01,\n",
            "        -7.9841e-01, -4.2969e-01,  5.7472e-01, -3.1376e-01,  8.9540e-01,\n",
            "        -1.7563e-02,  1.1426e+00, -2.0056e-01,  2.7155e-02,  1.1921e+00,\n",
            "         2.3651e-01, -8.3818e-02, -8.7466e-02, -2.8167e-01, -4.9518e-02,\n",
            "        -5.2014e-01, -2.7998e-01, -5.6539e-02, -9.7120e-01, -8.7221e-02,\n",
            "        -1.5849e-01, -2.9754e-01,  8.1104e-03,  1.0749e-01,  8.2435e-01,\n",
            "         4.4084e-01, -4.8776e-01, -4.3518e-01,  1.0992e+00, -7.1407e-01,\n",
            "        -8.6794e-01, -4.0795e-02,  6.8047e-02, -1.1961e-01,  5.0935e-01,\n",
            "         5.3087e-01, -7.5401e-01, -1.7599e-01, -9.3200e-01, -4.8573e-01])\n",
            "\n",
            "tgt:\t tensor([ 95, 120, 114, 207,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 624 / 1872\tLoss:\t5.391657\n",
            "pred:\t tensor([-9.3944e-01, -1.5289e+00, -2.9371e-01, -1.7163e+00, -1.9367e+00,\n",
            "         4.7044e+00,  1.3706e-01,  1.0695e-01, -2.6174e-01,  1.5489e+00,\n",
            "         2.5329e-01,  5.2319e+00, -6.2311e-01, -1.4862e+00,  3.7697e-01,\n",
            "         2.2804e+00, -2.3546e-02,  3.4121e-01,  1.7512e+00, -4.8850e-01,\n",
            "        -8.3016e-02,  3.5350e-01, -3.9557e-01,  8.3279e-01,  5.4733e-01,\n",
            "        -4.0179e-01, -5.4506e-01, -5.9888e-01,  4.0389e-02, -4.7421e-01,\n",
            "        -5.9878e-01, -4.2414e-01,  2.4250e-01,  2.6141e-01,  4.6798e-01,\n",
            "        -5.0834e-01, -1.1263e+00,  3.8717e-01, -2.9500e-01, -4.4507e-01,\n",
            "         1.4040e+00,  1.1069e+00, -8.5717e-01,  6.6226e-01,  7.0681e-01,\n",
            "        -3.4003e-01,  5.3720e-01, -2.6656e-01,  1.1980e+00, -6.8829e-02,\n",
            "         2.7287e-01, -2.2553e-01,  8.9406e-01,  5.7030e-01,  2.5308e-01,\n",
            "         4.5042e-01, -1.3068e+00, -1.2656e-01,  3.1492e-01,  3.4029e-01,\n",
            "        -4.4986e-01,  2.9177e-01,  5.5791e-01, -7.2929e-01,  2.1057e-01,\n",
            "        -1.1213e+00,  1.9888e-01,  1.5110e+00,  2.4816e-01,  2.2612e-01,\n",
            "        -2.2161e-02, -3.7665e-01, -3.1880e-01,  1.9461e-01, -1.3099e-01,\n",
            "         4.7173e-01, -2.7011e-01, -6.7485e-01, -2.4790e-01, -6.3843e-01,\n",
            "         1.0738e-01,  7.1528e-01,  2.5998e-02,  3.8403e-01,  2.8866e-01,\n",
            "         8.2964e-01, -5.1219e-01, -8.2968e-01,  1.8692e-01, -4.1779e-01,\n",
            "        -6.9676e-02, -3.0869e-01,  7.0009e-01, -1.4305e+00,  1.1150e+00,\n",
            "         3.5665e-01, -3.6278e-01,  7.6263e-02, -6.8944e-01, -1.1253e+00,\n",
            "        -4.9980e-01, -4.0044e-01, -4.0084e-01, -2.7036e-01,  4.2787e-02,\n",
            "        -1.2437e+00,  1.5907e-01,  1.3790e-01,  1.5033e+00, -3.9901e-01,\n",
            "        -1.3434e-01, -7.6291e-01,  7.2176e-01,  1.0610e+00, -5.7801e-01,\n",
            "         2.8361e-01,  5.4143e-01,  8.0673e-01, -3.1864e-01,  1.6028e-01,\n",
            "         3.0445e-02, -2.4014e-01,  3.2003e-01, -4.4036e-01,  3.0938e-01,\n",
            "        -1.0930e-01, -7.2650e-01, -5.5010e-01,  6.1601e-01, -4.9734e-01,\n",
            "        -2.9971e-01, -3.7057e-01, -5.7685e-01, -4.2790e-01,  1.7662e-01,\n",
            "         3.5138e-01, -1.3761e-01, -1.1323e+00, -5.3487e-01, -2.5215e-01,\n",
            "        -3.0416e-01, -1.6210e-02,  5.8834e-01,  5.1812e-02, -5.5800e-01,\n",
            "        -5.1860e-01,  4.7569e-01, -1.6228e-01, -1.6060e-01, -4.1459e-02,\n",
            "         9.9503e-01, -3.5276e-01, -7.5892e-01, -5.3259e-01,  6.3095e-01,\n",
            "         7.5144e-01,  6.7744e-01, -1.1176e+00, -1.0521e+00,  1.3342e-01,\n",
            "        -1.2210e-01,  7.3061e-01,  6.4597e-01,  4.4462e-02,  4.2055e-01,\n",
            "        -7.2883e-01,  3.6320e-01, -6.0493e-02, -5.9400e-01, -5.9088e-01,\n",
            "        -1.5688e-01,  1.7118e+00,  1.1089e-01, -6.7666e-01, -7.9352e-01,\n",
            "        -2.1381e-01, -1.7302e+00,  9.8309e-01, -7.3042e-01, -7.1320e-01,\n",
            "        -8.4305e-01, -3.2165e-01,  6.6325e-01, -3.0982e-01, -5.3473e-02,\n",
            "        -1.3320e+00, -1.0036e-01, -5.6913e-01,  7.0550e-01, -7.2223e-01,\n",
            "         1.1513e-01, -4.7205e-01, -3.8767e-01, -1.4219e+00,  6.8810e-01,\n",
            "        -1.3254e-01, -5.6412e-01, -9.6454e-02,  3.6584e-02,  8.2599e-01,\n",
            "        -5.3650e-01,  5.0083e-01,  1.8691e-01,  8.0557e-01,  7.8061e-02,\n",
            "         6.7443e-02, -5.8939e-01,  3.7550e-01,  5.7234e-01, -3.7037e-01,\n",
            "        -2.6949e-01,  6.2840e-02, -8.5736e-01, -1.1081e+00,  2.1374e-01,\n",
            "         1.9164e-01,  5.7074e-01,  1.4439e-01, -3.9822e-01,  7.2821e-01,\n",
            "        -4.8778e-01, -3.6194e-01,  3.0518e-01,  2.5463e-01,  3.5433e-01,\n",
            "        -8.9444e-01,  6.4810e-01,  1.2664e+00, -1.0451e+00,  1.9569e-01,\n",
            "        -2.8480e-01, -7.1011e-01, -4.7579e-01, -3.7758e-01,  1.1010e-02,\n",
            "        -9.3074e-01, -6.1976e-01, -8.4313e-01,  7.4220e-01, -4.1065e-01,\n",
            "        -8.0595e-01, -8.4057e-01, -5.3600e-01,  3.8814e-01, -1.1363e+00,\n",
            "         1.0490e-01, -3.3832e-01, -7.4115e-01, -2.3264e-01, -1.0534e+00,\n",
            "        -3.0534e-01, -4.6659e-01,  9.6918e-02, -1.4590e+00,  9.0999e-01,\n",
            "         1.2625e+00,  1.0162e-01, -1.0396e+00, -2.3541e-01,  5.5009e-01,\n",
            "         7.7113e-01, -2.4641e-01, -5.9608e-01, -1.2006e+00, -4.7707e-01,\n",
            "        -6.4009e-01, -1.2558e+00, -1.0862e+00, -1.8303e-01,  1.9466e-01,\n",
            "        -1.1861e+00, -3.9650e-01, -4.6015e-01,  1.5181e-01,  3.0820e-01,\n",
            "        -1.7943e-01,  5.7335e-01, -3.1409e-01, -4.0500e-01, -4.8034e-01,\n",
            "         3.4877e-01,  4.7069e-01,  4.8310e-01,  1.1168e-02, -1.9657e-01,\n",
            "        -2.3951e-01, -2.5637e-01,  2.1367e-01, -8.2412e-01, -2.9321e-01,\n",
            "        -1.1595e-01,  7.5388e-02,  8.1032e-01, -2.4733e-01,  2.0998e-01,\n",
            "         1.7002e-01, -9.5797e-01,  2.0111e-01,  6.1508e-02, -8.5457e-01,\n",
            "         6.0844e-01,  1.1597e+00,  5.6259e-01, -2.7526e-01, -6.3183e-01,\n",
            "        -8.7636e-02,  5.4918e-01, -1.9029e-01,  9.0060e-01,  1.9908e-01,\n",
            "        -5.9791e-01,  3.3166e-01, -2.6048e-01,  9.9817e-02, -1.1515e+00,\n",
            "        -1.0744e-02,  1.1247e-03,  4.1111e-01,  5.1038e-01,  1.0963e-01,\n",
            "         3.9067e-01, -1.1153e+00, -5.1139e-01,  1.5471e-02, -6.5541e-01,\n",
            "         1.3546e-01,  8.1809e-01, -5.8002e-01,  1.1158e+00, -3.2090e-01,\n",
            "        -5.9054e-01,  2.1863e-01, -6.2300e-01, -1.9980e-01,  2.3861e-01,\n",
            "        -8.6305e-02, -1.9617e-01,  3.1506e-01,  1.1552e+00, -3.8936e-01,\n",
            "        -1.1520e+00,  2.6651e-02,  3.8749e-01, -1.5944e-01,  5.9524e-02,\n",
            "        -4.5389e-01, -1.1434e+00,  2.8198e-01, -1.2396e+00,  2.0469e-01,\n",
            "         6.2820e-02,  2.1250e-02, -2.3375e-02, -4.5082e-01,  4.0468e-02,\n",
            "        -8.6628e-01, -2.0150e-01, -5.7019e-01, -4.4809e-01, -7.1088e-01,\n",
            "        -5.7248e-02, -6.0082e-01,  6.5220e-02, -4.4481e-01,  7.6697e-02,\n",
            "        -6.7586e-01, -2.7845e-01,  9.3832e-01, -1.0814e-02,  2.8552e-01,\n",
            "        -8.8319e-03, -9.8614e-01,  5.2372e-02,  2.0954e-01, -1.4543e+00,\n",
            "        -8.9485e-02, -2.2554e-01, -5.6882e-01, -3.9307e-01, -4.8082e-01,\n",
            "        -2.1866e-02, -4.7825e-01,  7.5367e-01, -2.8799e-01, -2.4411e-03,\n",
            "         1.1505e-01,  7.0465e-01, -4.9996e-01, -6.4565e-01, -1.2905e-01,\n",
            "        -6.3782e-01,  3.1372e-01,  4.1799e-01, -1.0514e-01,  2.4549e-01,\n",
            "         4.6016e-01,  2.6930e-01,  3.5455e-01, -4.1801e-01, -8.7082e-01,\n",
            "        -4.7173e-01, -5.4605e-01, -5.9699e-02, -1.4067e-01,  2.2432e-01,\n",
            "         5.2729e-01,  1.1664e-01,  1.5685e-01,  2.9554e-01, -3.4253e-01,\n",
            "        -9.3203e-02,  8.3257e-02, -1.9568e-01,  3.0808e-01,  3.0918e-01,\n",
            "        -1.6180e-01, -3.3859e-01, -1.0094e+00, -1.5683e+00, -8.7536e-01])\n",
            "\n",
            "tgt:\t tensor([156, 357, 254,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 780 / 1872\tLoss:\t6.127857\n",
            "pred:\t tensor([-1.4004e+00, -2.0100e-01, -1.3512e+00,  3.6893e-01,  4.6717e-01,\n",
            "         3.2536e+00,  1.8842e+00,  7.0633e-01,  5.6024e-01,  5.8550e-01,\n",
            "        -2.1572e-01,  2.6536e+00,  6.5656e-01, -7.0716e-01,  1.6212e+00,\n",
            "         4.3315e-01,  1.5099e-01, -3.4321e-01,  7.5085e-01, -3.9365e-01,\n",
            "        -5.1862e-01, -5.3218e-01, -1.1153e-01, -5.4685e-01,  1.7643e+00,\n",
            "        -6.9072e-01, -2.2745e-02,  7.3574e-01, -3.6237e-01,  7.4627e-01,\n",
            "        -5.4001e-01,  1.0628e-01,  1.3318e-02, -8.8047e-01,  1.2597e+00,\n",
            "        -1.2335e+00, -1.2666e+00, -2.4009e+00,  4.8986e-01,  7.8276e-02,\n",
            "         6.1200e-01, -8.3486e-01, -1.0415e+00, -3.6134e-02,  1.2684e+00,\n",
            "         1.1152e+00, -6.8628e-01, -6.0162e-01,  6.5807e-01,  4.2812e-01,\n",
            "         1.8675e-01, -6.4278e-01,  2.1345e+00,  1.1322e+00,  4.8362e-01,\n",
            "        -7.5604e-01,  8.7775e-02,  3.0273e-02,  6.4475e-01,  9.8477e-01,\n",
            "        -1.0217e+00,  2.2674e-01,  2.6334e-01, -5.4311e-01, -3.4589e-01,\n",
            "         6.3491e-02, -3.9440e-01,  1.1193e+00, -1.5838e+00,  2.6111e-01,\n",
            "        -1.0211e+00, -7.4703e-01, -7.0589e-01, -2.2256e-01, -6.8590e-01,\n",
            "        -9.0908e-01,  1.2594e+00,  2.0662e-01, -8.1975e-01, -2.1227e+00,\n",
            "        -3.8014e-01, -4.2176e-01, -5.1717e-01,  2.3761e-03,  7.2016e-01,\n",
            "         1.2011e+00,  7.1317e-01,  2.8231e-01,  1.6642e+00, -3.7789e-01,\n",
            "        -3.7867e-01, -6.2454e-01,  1.6654e-02, -1.5220e+00,  7.6237e-01,\n",
            "         8.2138e-01, -4.0534e-01, -1.0241e+00, -4.1754e-01,  1.0332e-01,\n",
            "        -1.2387e+00,  1.5837e-01, -9.9468e-01, -6.9405e-01, -1.0252e+00,\n",
            "        -6.3595e-01, -4.6461e-01, -7.8970e-01, -7.1107e-01,  2.1852e-03,\n",
            "        -2.9410e-01, -2.4165e-01, -8.0609e-02,  9.1308e-01,  8.7129e-02,\n",
            "        -9.4430e-01, -1.9493e+00,  4.4737e-01, -1.3412e-01,  2.2170e-01,\n",
            "        -1.7153e-01, -4.3415e-01,  1.1923e+00,  1.5483e-02,  3.4061e-01,\n",
            "        -7.0042e-02, -4.5475e-01, -7.4345e-02, -9.4824e-02, -5.9501e-01,\n",
            "        -1.1035e+00,  1.6080e-01,  2.6402e-01, -7.7165e-01, -2.3893e-01,\n",
            "        -1.0124e-02,  2.0273e-01, -1.7090e+00,  3.6143e-01,  5.1032e-01,\n",
            "        -9.9235e-02, -1.5669e-01, -8.8078e-01, -1.2668e-01, -1.6499e-01,\n",
            "        -2.9364e-01,  5.3075e-01, -1.4227e+00,  1.6582e+00, -3.9675e-01,\n",
            "         5.9996e-01, -2.7746e-01, -1.9967e-01, -1.8920e-01,  8.8569e-01,\n",
            "         5.6373e-01, -3.0694e-01, -7.8973e-01,  5.2228e-01,  3.1191e-01,\n",
            "        -5.5079e-02, -1.0580e+00,  7.6883e-02,  3.0320e-01,  9.8087e-01,\n",
            "        -9.2200e-01, -9.6232e-01,  4.3338e-01, -5.9319e-01, -1.0511e+00,\n",
            "        -9.3586e-02, -1.9367e+00,  4.1808e-01, -8.5669e-01, -1.9246e-01,\n",
            "         8.8494e-01, -7.0443e-01, -6.0947e-01, -6.7116e-01,  9.3073e-02,\n",
            "        -3.6792e-01, -5.0464e-01,  3.1400e-01,  3.8514e-01, -4.8819e-01,\n",
            "         6.3733e-01,  7.6265e-01, -5.0009e-01,  2.8123e-01,  3.7692e-01,\n",
            "        -1.5941e-02, -1.2137e+00,  9.6270e-03, -1.0514e+00,  9.5268e-01,\n",
            "        -4.7885e-01, -3.1943e-01, -9.7227e-01, -7.2167e-01,  7.8040e-01,\n",
            "         1.0094e-01, -2.7582e-01, -8.8185e-01,  5.6055e-01, -3.2055e-01,\n",
            "        -5.7114e-01, -3.9324e-01, -7.9934e-01,  6.4869e-01, -2.1936e-01,\n",
            "         5.5826e-02, -7.4021e-01,  7.0213e-01, -2.6079e-01,  3.4102e-01,\n",
            "        -1.0652e+00, -2.0502e-01,  9.3474e-01,  1.0772e-01,  1.2634e+00,\n",
            "         4.1587e-02, -7.0977e-02, -3.0551e-01, -5.8693e-02,  7.7947e-02,\n",
            "         3.5774e-01,  1.4676e+00, -1.7832e-02,  6.5720e-01,  8.2560e-01,\n",
            "         6.6345e-01, -1.1154e+00, -3.5616e-01,  8.8770e-01, -1.0151e+00,\n",
            "        -7.5373e-01, -1.2151e+00, -2.9390e-01, -3.9311e-01,  3.5641e-01,\n",
            "        -6.0280e-01,  1.6103e-01, -1.4357e-01,  4.1513e-01,  1.2875e-01,\n",
            "        -6.7542e-01, -3.4952e-01, -5.4986e-01, -7.4690e-01,  9.0420e-01,\n",
            "        -1.3995e+00, -5.0541e-01,  4.3452e-01, -1.2494e-01, -3.1989e-01,\n",
            "         4.6012e-01, -3.7036e-01,  5.2906e-02, -4.5358e-01,  5.9819e-01,\n",
            "         2.6347e-01, -1.0001e+00, -4.0176e-02, -6.8011e-01,  5.0807e-01,\n",
            "        -2.8821e-01, -1.6091e+00,  4.2400e-02, -2.1657e-01,  3.0632e-01,\n",
            "         6.2457e-01,  2.2112e-01,  1.9416e-01,  7.5358e-01, -6.9665e-02,\n",
            "         1.3665e+00,  7.3071e-01, -9.5702e-01,  1.3451e-01, -1.7768e-01,\n",
            "         4.4279e-01, -1.2094e-01,  1.7047e+00, -5.5362e-01, -1.8708e-01,\n",
            "        -1.1118e+00, -1.0906e+00, -5.7917e-02, -9.8671e-01, -1.9926e-01,\n",
            "         1.4795e-01, -2.6451e-01,  5.5839e-01, -3.4598e-01, -5.3991e-01,\n",
            "        -7.4136e-01, -4.8430e-01,  1.4841e-02,  8.3524e-01, -7.2720e-01,\n",
            "        -1.2096e-01,  5.0228e-01,  3.1623e-01,  9.8487e-01, -4.3766e-01,\n",
            "         1.1260e+00,  1.2261e+00, -3.9663e-01,  5.8087e-01, -1.0185e-01,\n",
            "        -2.4215e-01,  4.7426e-02, -1.1560e-01,  6.1773e-01, -7.4272e-01,\n",
            "        -1.0559e+00,  5.4460e-01,  9.9809e-01,  6.7803e-01,  6.3559e-01,\n",
            "         1.5565e-02,  9.7174e-01,  3.3940e-01,  7.9590e-01, -1.2062e-01,\n",
            "        -9.6884e-01,  8.2052e-01,  5.6240e-01,  5.0792e-01,  1.4165e-01,\n",
            "        -1.1746e-02,  5.9047e-01, -1.8546e-01, -3.7954e-01,  6.0350e-01,\n",
            "        -2.3194e-01, -1.3050e-01,  1.3590e-03, -6.7250e-01, -5.9883e-01,\n",
            "        -6.8751e-01, -1.2879e+00, -7.5866e-01, -6.3482e-01,  4.4705e-01,\n",
            "         7.9428e-01, -1.1588e-01, -2.9659e-01,  1.7481e-01,  8.7449e-01,\n",
            "        -1.6543e-01, -8.2043e-01,  2.4659e-01,  4.7277e-01, -5.0527e-01,\n",
            "         5.5393e-01,  8.9449e-01, -1.5708e+00, -1.7500e+00, -8.3389e-01,\n",
            "        -1.3255e-01,  5.7323e-01, -9.8235e-02, -7.3971e-01, -1.7631e-02,\n",
            "        -8.3242e-01,  1.1276e+00,  1.0205e+00, -9.2705e-01,  3.5409e-01,\n",
            "         3.1024e-01, -1.9707e-02,  5.9027e-01,  4.9962e-01, -5.1287e-01,\n",
            "         1.1984e+00, -1.0197e+00,  3.4918e-01,  7.4602e-01,  4.1323e-01,\n",
            "        -5.5292e-01, -4.4358e-01,  3.8454e-01, -3.2964e-01, -8.5423e-01,\n",
            "        -1.0143e-01, -1.6649e-01, -4.0081e-01, -6.3593e-01, -5.2830e-01,\n",
            "         2.6473e-01,  3.5701e-01, -4.8930e-01, -2.5129e-01,  2.5031e-01,\n",
            "         4.5259e-01,  5.1625e-02,  4.0669e-02,  6.7132e-01, -3.2067e-01,\n",
            "         5.1640e-01, -1.0767e+00, -8.2574e-01, -9.8610e-03,  6.4351e-01,\n",
            "         7.0863e-02, -4.4722e-01, -1.0311e+00, -7.5123e-02, -8.6031e-01,\n",
            "         1.0539e-01, -4.0564e-01, -5.1975e-01,  2.2409e-01,  4.6101e-01,\n",
            "         8.2416e-01, -3.7207e-01, -7.7355e-01, -1.4520e+00, -6.2640e-01])\n",
            "\n",
            "tgt:\t tensor([308, 242,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 936 / 1872\tLoss:\t4.159782\n",
            "pred:\t tensor([-2.0467e-01, -7.0458e-01,  9.4379e-02, -2.6774e+00, -6.7966e-01,\n",
            "         1.5827e+00,  1.2630e-01,  4.6718e-01,  1.1669e-01,  2.2731e-01,\n",
            "        -3.4761e-01,  5.0433e+00, -1.0876e+00, -1.0990e+00,  3.2499e+00,\n",
            "        -1.5721e-01,  5.1324e-01, -5.8358e-02,  7.5295e-01, -7.1516e-01,\n",
            "         4.9075e-02,  6.2614e-01, -1.3326e+00,  5.6248e-01, -3.2205e-03,\n",
            "        -7.5272e-02, -4.1465e-01,  2.3961e-03,  2.0480e+00, -1.0633e+00,\n",
            "         2.1322e-01, -1.9875e-01,  7.8803e-01,  1.3594e-01, -8.5258e-02,\n",
            "        -1.1820e+00, -6.8254e-01,  1.1594e+00,  2.1735e-01, -9.8560e-01,\n",
            "         1.3685e+00,  3.8752e-01, -9.3846e-01,  9.1342e-01,  9.6823e-01,\n",
            "         3.5842e-01, -8.9028e-01, -1.3138e+00,  4.2246e-01, -4.6776e-01,\n",
            "         4.6649e-01, -6.8876e-01,  3.4336e+00, -5.7714e-01,  7.6809e-01,\n",
            "         8.2629e-01,  4.0240e-01, -3.2008e-01,  2.0011e-01,  3.2257e-01,\n",
            "        -2.1986e-01, -2.7066e-01, -2.1304e-01, -6.5559e-01,  3.8710e-01,\n",
            "        -4.1923e-01,  6.9951e-02,  1.2710e+00, -4.4782e-01,  2.6102e-01,\n",
            "         2.7950e-01,  4.9376e-01, -1.6899e+00, -2.5049e-01, -4.9129e-01,\n",
            "         5.7667e-01, -6.5640e-01, -1.3831e+00, -9.6738e-01, -1.3014e+00,\n",
            "         7.4556e-01, -5.2710e-02, -4.0062e-01,  7.3204e-01, -1.0735e+00,\n",
            "        -1.4912e-01,  2.0850e-02, -5.5164e-01,  1.6858e-01,  1.5736e-01,\n",
            "         4.1407e-01, -1.4075e-01,  1.6083e+00, -2.7631e-01,  1.9621e-01,\n",
            "         1.3462e+00, -1.9094e-02, -8.7056e-01, -6.2547e-01, -1.5400e+00,\n",
            "        -3.8068e-01, -7.1156e-02,  6.7782e-01,  2.0356e-01,  1.5940e-01,\n",
            "        -5.5952e-01, -6.2212e-01,  7.7703e-01, -3.9509e-01, -3.1374e-01,\n",
            "         1.4523e-02, -4.4893e-01,  1.1126e+00,  2.4524e-01,  1.0097e-01,\n",
            "        -5.4218e-01, -2.3303e-01, -1.0208e-01, -6.3539e-01,  2.6636e-01,\n",
            "        -1.1608e-01,  1.1226e-02,  7.5075e-01,  3.8731e-01, -5.7606e-01,\n",
            "         2.1522e-01, -6.6977e-01,  4.6786e-01,  3.1578e-01, -2.2683e-01,\n",
            "        -1.0034e+00, -6.3176e-01, -5.3479e-01, -4.8346e-01, -1.8241e-01,\n",
            "         6.4499e-01, -5.1944e-01, -2.0415e-01, -3.5163e-01, -1.3493e+00,\n",
            "         6.8706e-01, -1.2832e+00,  4.1158e-01, -7.3184e-01, -7.1511e-01,\n",
            "        -1.0485e+00,  4.4037e-01, -4.3091e-01, -4.7955e-01, -7.5833e-01,\n",
            "         8.7393e-01,  7.7764e-01,  3.2025e-02, -8.1519e-01,  3.0793e-01,\n",
            "        -1.6321e-01, -1.0066e+00, -7.7634e-01,  1.0207e+00,  6.1499e-02,\n",
            "         5.0858e-01, -2.6039e-01,  2.5227e-01,  6.5873e-01,  1.0962e-01,\n",
            "        -1.1375e+00,  3.5441e-01,  9.4966e-01, -2.6252e-01, -3.1778e-01,\n",
            "         2.4849e-01,  3.1985e+00, -1.0022e+00, -7.7457e-01, -5.3101e-01,\n",
            "         2.3542e-01, -3.8600e-01, -2.5365e-01, -7.1725e-01, -5.2388e-01,\n",
            "        -7.6156e-01,  6.3229e-01,  3.3812e-01, -8.8340e-01,  6.4062e-01,\n",
            "         3.2581e-02, -3.8453e-01, -3.4400e-01,  3.9989e-01,  3.2981e-01,\n",
            "         6.1576e-02, -9.8508e-01,  3.3906e-01, -7.1048e-01,  6.8411e-01,\n",
            "         9.7101e-01,  3.4928e-01, -2.8853e-01,  4.3909e-01,  1.1083e+00,\n",
            "        -1.0735e+00,  3.5678e-01, -1.7622e-01,  6.4152e-01, -1.2143e+00,\n",
            "         1.0796e+00, -9.7789e-01,  4.1134e-01,  3.2858e-01, -1.1491e+00,\n",
            "        -1.2262e-01,  1.8417e-01, -5.2328e-02, -8.4326e-01,  6.3581e-01,\n",
            "        -5.5054e-01,  7.8869e-01,  3.4188e-01, -6.3573e-01,  1.2153e+00,\n",
            "        -3.5827e-02,  2.0473e-01,  3.1140e-01,  3.8636e-01,  7.6358e-02,\n",
            "         5.1484e-01, -1.1422e-01,  9.6548e-02,  7.9685e-02,  5.0507e-01,\n",
            "         3.7545e-01, -8.9859e-01,  3.4692e-01, -1.1725e-02, -1.8568e-01,\n",
            "         6.6851e-02, -3.5727e-01, -3.3835e-01,  2.0590e-01,  8.9804e-02,\n",
            "        -4.0333e-01, -9.1579e-01,  2.9724e-01, -8.7101e-01, -2.7715e-01,\n",
            "         2.8239e-01, -4.1439e-02, -3.1051e-01,  1.9870e-01, -5.9763e-01,\n",
            "        -1.2380e+00, -1.8577e-01,  3.4537e-01, -1.1649e+00, -4.6395e-01,\n",
            "        -1.5200e-01, -1.1267e-01,  2.8734e-01, -1.1320e+00,  8.2347e-01,\n",
            "        -6.1627e-01, -5.7647e-02, -6.3318e-02, -4.8680e-01, -9.0725e-01,\n",
            "        -5.1315e-01, -9.7358e-01, -6.7503e-01, -5.8396e-01, -4.5383e-01,\n",
            "        -6.7426e-01,  1.8451e-01, -8.0930e-01,  1.2758e-01,  3.2951e-01,\n",
            "        -1.9858e-01, -1.5296e-01, -1.4251e-01, -3.0349e-01,  1.6411e-01,\n",
            "        -9.5801e-02,  8.5678e-02,  1.2404e+00, -8.2231e-01, -1.5254e-01,\n",
            "         7.8177e-01,  8.8417e-01,  1.6701e-01, -8.6188e-01, -3.6982e-01,\n",
            "        -7.0948e-01, -4.0602e-02,  1.7069e-01, -1.5195e-01,  7.5715e-01,\n",
            "        -8.0829e-01,  1.4566e-01,  2.9106e-01,  2.5835e-01,  3.7270e-01,\n",
            "        -7.9168e-01,  4.7298e-01,  2.3687e-01,  2.3203e-01, -2.2206e-01,\n",
            "        -4.1554e-02,  5.8441e-01,  2.3412e-01,  1.1374e+00, -3.9992e-02,\n",
            "         8.0042e-01,  1.0876e+00, -7.0046e-01,  7.5677e-01, -1.0972e+00,\n",
            "        -7.1476e-01, -1.0984e+00, -8.6044e-02,  1.3361e+00, -4.8443e-01,\n",
            "        -2.5701e-01,  4.6236e-01,  8.8233e-01,  6.0423e-02, -4.1173e-01,\n",
            "        -1.2155e-01,  5.1433e-02,  1.0104e+00, -3.7366e-02,  6.3431e-01,\n",
            "        -1.1588e+00, -2.6409e-01, -3.8665e-01, -3.4000e-01,  1.0977e-01,\n",
            "        -4.5972e-01, -3.8316e-01,  1.0004e+00,  4.0344e-01, -3.4160e-01,\n",
            "        -7.6446e-01,  4.3359e-01,  3.3133e-01, -3.2199e-01, -3.9903e-02,\n",
            "        -1.0262e+00, -1.2121e+00,  2.7174e-01, -1.8935e+00, -4.1708e-01,\n",
            "         5.9387e-01,  5.8813e-02,  5.3831e-01,  8.4059e-01,  6.0059e-02,\n",
            "        -3.1545e-01,  1.0012e+00, -7.7383e-01, -2.7865e-01, -9.7732e-01,\n",
            "        -1.0352e-01, -9.2767e-01,  2.6197e-01,  2.4876e-01, -4.4890e-01,\n",
            "        -5.1020e-01,  4.0513e-01,  4.6470e-01, -8.6974e-01,  1.8266e-01,\n",
            "        -6.5868e-01, -1.0511e+00,  1.6456e-01,  4.9994e-01, -1.4275e-01,\n",
            "         7.5169e-01, -1.1614e+00,  3.8833e-01, -5.8774e-01, -3.4631e-02,\n",
            "        -2.8740e-01,  2.2992e-01, -6.4410e-02, -7.4491e-01,  6.3048e-01,\n",
            "         2.2959e-01,  1.2584e+00, -3.3489e-01, -4.3877e-01, -2.3373e-02,\n",
            "         8.8825e-01, -9.2607e-01, -3.5918e-01, -1.0503e+00,  1.6557e-03,\n",
            "        -8.8239e-01, -4.0138e-01,  1.1850e-01, -1.0097e+00, -1.2380e-01,\n",
            "         9.4420e-02,  1.1085e-01,  4.4644e-02, -5.4838e-01,  5.3606e-02,\n",
            "         5.2855e-01,  3.2484e-02, -1.1981e+00,  8.9019e-01,  4.6041e-02,\n",
            "        -1.7836e-01, -6.9460e-01, -4.2931e-01, -1.1362e-01,  5.0360e-01,\n",
            "         1.0372e+00, -8.1500e-01,  2.2218e-01,  4.0168e-04, -5.4902e-01])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 120,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 1872\tLoss:\t3.986129\n",
            "pred:\t tensor([-7.8589e-01, -4.7732e-01, -8.4557e-01,  2.5897e+00, -3.0330e-02,\n",
            "         3.7015e+00,  1.3289e+00,  2.4604e-01, -3.5009e-01, -9.3890e-01,\n",
            "         3.9087e-01,  1.6422e+00,  3.9164e-01,  1.1989e-02,  2.4212e-01,\n",
            "         9.8807e-03,  1.2469e+00, -9.1142e-01,  1.5077e+00,  5.5651e-01,\n",
            "         2.2031e-01, -8.1273e-01, -1.8629e-01,  5.5851e-02,  1.3154e+00,\n",
            "        -7.2894e-01,  8.7507e-01, -1.0255e-01, -1.0114e+00, -1.0534e-02,\n",
            "        -4.8418e-01, -2.1475e-01, -4.7567e-01, -6.8045e-01,  1.5489e+00,\n",
            "        -2.5693e+00, -9.6201e-01, -1.7065e+00,  3.4907e-01,  2.0063e-01,\n",
            "        -4.9062e-01, -5.6813e-01, -7.0625e-01,  3.4592e-01,  1.2048e+00,\n",
            "         3.9751e-01, -8.3512e-01, -7.7163e-01, -2.1204e-01, -5.2039e-01,\n",
            "        -5.2996e-03, -4.5016e-01,  8.3032e-01,  1.0686e+00,  2.0497e-01,\n",
            "        -4.1370e-01, -1.7592e-01,  6.4515e-01, -2.1810e-02,  4.9130e-01,\n",
            "        -1.0360e+00,  3.3036e-01,  1.6814e-01, -9.4954e-01,  2.2324e-01,\n",
            "        -5.5471e-01, -8.2802e-01,  9.0822e-01, -2.4885e-01,  7.6983e-01,\n",
            "        -1.1988e+00,  3.9737e-01, -4.1837e-01,  3.7722e-01, -1.1369e-01,\n",
            "        -4.9931e-01,  1.3483e+00, -2.7187e-01, -2.1167e+00, -1.9755e+00,\n",
            "        -6.5464e-01, -4.0834e-01,  6.2391e-02, -6.1632e-01,  5.3523e-01,\n",
            "         1.1884e+00,  5.0824e-01,  5.6533e-02,  2.1452e+00,  4.6163e-01,\n",
            "        -1.0443e+00, -5.1576e-01,  3.5668e-01, -1.2552e+00,  1.1337e+00,\n",
            "         8.5009e-01,  1.9025e-01, -6.8606e-01, -4.5490e-01, -6.5060e-02,\n",
            "        -3.9966e-01,  4.1979e-01, -7.7361e-01, -8.7661e-01, -8.3357e-01,\n",
            "        -4.9255e-01, -2.0924e-01, -1.5657e+00, -2.0818e+00,  4.0094e-01,\n",
            "        -4.6070e-01, -1.1590e+00, -5.4770e-01,  1.2647e+00,  2.4061e-01,\n",
            "        -3.4445e-01, -8.1119e-01,  2.2616e-01,  4.0662e-02,  6.5877e-01,\n",
            "        -1.2033e+00, -4.6913e-01,  5.5699e-01,  1.0222e-01,  3.2381e-01,\n",
            "        -6.4766e-01,  4.2787e-01,  4.4994e-01,  1.6358e-01,  1.6879e-01,\n",
            "        -4.4431e-01,  6.7317e-02, -4.2893e-01, -2.6465e-01,  7.5255e-01,\n",
            "        -5.9360e-01, -2.5121e-02, -1.7945e+00,  6.3614e-01,  4.1787e-01,\n",
            "         4.3979e-01,  4.8349e-01, -1.0781e-01,  7.5694e-01, -4.7356e-02,\n",
            "        -1.1471e+00,  1.7782e-01, -6.6378e-01,  2.3584e+00, -2.3722e-01,\n",
            "         2.6299e-03, -2.8473e-01, -5.4710e-01, -2.7714e-01,  5.8721e-01,\n",
            "         5.8953e-01, -9.5612e-02, -8.2276e-01, -1.9156e-02,  6.0031e-01,\n",
            "         7.5271e-01, -9.1280e-01, -3.5507e-01, -2.7835e-01,  6.4470e-02,\n",
            "        -7.6519e-01,  1.7520e-01,  5.1011e-01,  1.8391e-01, -5.8635e-01,\n",
            "         7.0970e-03, -1.9910e+00,  6.6170e-01,  6.6106e-01,  1.8873e-02,\n",
            "        -2.4301e-01, -6.3206e-01, -1.1060e+00, -1.4829e-01,  1.1221e-01,\n",
            "         2.8066e-02, -9.2600e-01,  6.7941e-01,  6.1105e-01,  2.5539e-01,\n",
            "        -6.7020e-03,  6.0632e-01, -4.1118e-01,  5.0170e-01,  9.1498e-01,\n",
            "        -4.4089e-01,  5.4680e-01, -7.8128e-02, -6.5539e-01,  7.0827e-01,\n",
            "         4.3053e-01,  4.3190e-02, -8.2295e-01, -1.1227e+00, -5.0934e-02,\n",
            "        -3.0066e-02, -4.9241e-01,  6.5314e-01, -7.8115e-03,  1.8804e-01,\n",
            "        -2.0896e-01, -3.6600e-01, -6.6249e-01,  9.9284e-01, -4.4258e-01,\n",
            "         7.4623e-01, -1.3724e+00,  2.7040e-01,  3.1032e-02,  3.2899e-01,\n",
            "        -1.1060e+00, -5.7806e-01,  1.0313e+00,  2.6484e-02,  1.1401e+00,\n",
            "        -1.5815e-01, -7.6329e-01, -2.5650e-01, -2.1140e-01,  4.1303e-02,\n",
            "         4.8953e-01,  2.9765e+00,  5.6982e-01,  5.4940e-01,  1.3086e+00,\n",
            "         1.1590e+00, -1.0419e+00,  4.6785e-01, -4.0882e-01,  1.6295e-02,\n",
            "        -8.5084e-01, -2.7362e-01,  3.7080e-01,  8.3164e-02, -2.3660e-02,\n",
            "         1.0100e-01, -1.6313e-01,  4.8591e-02,  1.5186e+00,  3.7002e-01,\n",
            "        -8.4819e-01, -6.3101e-01, -2.3612e-01,  1.0378e-01,  2.6991e-01,\n",
            "        -1.5631e+00, -7.5469e-01,  1.3397e-01, -5.2854e-01,  2.3176e-01,\n",
            "         6.4322e-01,  7.7365e-02,  4.9403e-01, -6.7025e-01,  6.8401e-01,\n",
            "        -4.0172e-01, -1.2856e+00,  6.5232e-02, -5.6167e-01,  3.1319e-01,\n",
            "         7.4906e-02, -6.3336e-01, -5.8757e-02, -4.4611e-01, -3.5293e-01,\n",
            "         7.3153e-01,  5.8837e-01,  1.6317e-01,  2.3693e-02,  7.9872e-01,\n",
            "         9.9811e-01,  4.7975e-01, -2.4002e-02, -3.9440e-01,  5.3458e-01,\n",
            "        -4.5818e-01, -2.9674e-01,  9.6521e-01, -1.5335e-01, -3.2638e-01,\n",
            "        -1.3984e+00, -5.7387e-01,  2.6098e-01, -1.6578e-02,  5.6472e-02,\n",
            "         6.0499e-01,  9.7025e-01,  3.1418e-01, -7.2802e-01, -1.9009e-01,\n",
            "        -4.8772e-01, -4.1678e-01,  1.9258e-01,  8.3471e-01, -2.0475e-01,\n",
            "         4.3974e-02,  1.1212e+00,  2.9740e-01,  5.4578e-01, -3.0674e-01,\n",
            "         1.6336e+00,  4.2418e-01,  9.6889e-02,  5.0195e-01,  2.9821e-01,\n",
            "        -1.4795e-01, -2.5585e-01,  9.5673e-02,  2.8304e-01,  2.6618e-01,\n",
            "        -1.0716e+00,  3.5737e-01,  9.0694e-01,  8.6319e-01, -7.0828e-01,\n",
            "        -5.1317e-01,  8.6928e-02, -1.3715e-01,  5.6249e-01,  5.2915e-01,\n",
            "         5.3318e-02,  2.3080e-02,  4.0093e-01,  4.1421e-01, -2.9378e-01,\n",
            "         7.2603e-02,  5.3991e-01, -9.3450e-01, -1.1382e+00,  9.2072e-01,\n",
            "         9.2040e-01,  8.3489e-01, -8.1325e-01, -1.0465e+00, -2.6549e-01,\n",
            "         3.0481e-01, -1.3904e+00, -2.8809e-01, -1.0262e+00,  9.2353e-01,\n",
            "        -4.9439e-01,  1.1343e-01,  2.2770e-02,  1.3327e+00,  2.5004e-01,\n",
            "         8.3299e-01, -2.2828e-01,  2.0276e-01,  3.2392e-01, -1.3613e-01,\n",
            "         6.9154e-01, -2.0608e-01, -1.7972e+00, -1.0925e+00, -1.7320e-01,\n",
            "        -1.7877e-01,  7.6462e-01, -6.0570e-01,  6.2111e-01, -9.2683e-01,\n",
            "        -1.0130e+00,  6.4898e-01,  1.0649e+00, -4.3868e-01, -4.2344e-01,\n",
            "         5.0001e-01, -7.4356e-02,  2.6456e-01, -9.3836e-02, -1.9966e-01,\n",
            "         9.3680e-01, -2.0554e-01, -2.5405e-01, -3.6903e-01,  1.9961e-01,\n",
            "        -4.8312e-01,  7.2323e-01,  1.0639e-03, -3.0405e-01, -4.8514e-01,\n",
            "         2.8812e-02, -4.7345e-01, -2.0780e-01,  2.7335e-01,  1.7777e-01,\n",
            "         4.6808e-01,  1.0677e+00, -6.1899e-01,  1.0030e-01, -4.4325e-01,\n",
            "         3.9576e-01,  9.2423e-02,  1.4310e-01,  3.2760e-01, -4.6537e-01,\n",
            "         9.0373e-02, -6.9370e-01,  6.2907e-01, -1.0793e-01,  2.2750e-01,\n",
            "         2.5482e-01, -9.7017e-01, -1.0057e+00, -5.8741e-02, -1.9433e-01,\n",
            "         1.4650e+00,  6.5674e-01,  4.7885e-02, -1.5141e-01, -6.7673e-01,\n",
            "         9.6555e-01, -3.1258e-01, -3.0993e-01, -1.1183e+00, -5.3153e-01])\n",
            "\n",
            "tgt:\t tensor([ 6,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1248 / 1872\tLoss:\t3.967003\n",
            "pred:\t tensor([-0.8835, -0.1665, -0.8128,  2.5237, -0.3165,  2.3696,  2.3834, -0.0871,\n",
            "         0.5389, -0.0995, -0.4755,  0.5021,  0.0261,  0.1222,  0.3398, -0.7636,\n",
            "         0.9111, -1.0575,  1.0878,  0.4028,  0.0956, -0.4099, -0.1552, -0.3156,\n",
            "         2.6014, -1.4268, -0.0181, -0.5691,  0.0828,  0.5991, -0.4173,  1.5710,\n",
            "        -0.9533, -1.1925,  2.2666, -1.5205, -0.6632, -2.5066, -0.0909, -0.0745,\n",
            "        -0.3372, -0.2342, -0.2731, -0.0277,  1.4036,  0.9238, -1.8463, -0.0963,\n",
            "        -0.5791,  0.0666, -0.2209, -0.5223,  1.4541,  0.2708,  1.1284, -1.2405,\n",
            "         0.2714, -0.0613,  0.0851, -0.3581, -1.2418,  0.6810, -1.0909, -0.7210,\n",
            "         0.7547, -0.0502, -1.2557, -0.1041, -0.5607,  0.9224, -1.3894,  0.5395,\n",
            "         0.0149,  0.0543, -1.2342,  0.5598,  0.7675,  0.1784, -1.1723, -1.4678,\n",
            "        -0.8293, -0.4991,  0.6318,  0.1992,  0.2211,  0.8679,  0.2606,  0.3089,\n",
            "         1.9769,  1.3606, -0.5905, -0.8400, -0.2622, -1.0617,  0.5509, -0.3497,\n",
            "         0.0597,  0.7403, -0.4031, -0.4789, -1.5386,  0.1516,  0.1265, -0.8961,\n",
            "        -0.9107, -0.2851, -0.2610, -0.8591, -1.4000, -0.9827, -0.1592,  0.2750,\n",
            "        -1.2202,  1.9131,  0.4655,  0.2278, -1.6045,  0.0930,  0.0281, -0.0318,\n",
            "        -1.4806, -0.7564,  1.0095, -0.1574,  0.3182, -0.2908,  0.9391, -1.0406,\n",
            "         0.3581,  0.2981, -0.5032, -0.4870, -0.2778,  0.0964,  0.5530, -0.1308,\n",
            "        -0.1059, -0.2641,  0.2832,  0.6851,  0.1806, -0.1490, -0.6063, -0.3333,\n",
            "         0.3126, -0.3955,  0.0260, -1.2013,  2.3015, -0.2909,  0.5364,  0.5114,\n",
            "         0.0527, -0.0501,  0.0400, -0.2150, -0.7703, -0.2117,  0.0221,  0.2496,\n",
            "         0.5253, -0.5021, -0.8317, -0.3817,  0.9849, -0.5589,  0.0106,  1.1497,\n",
            "         1.1611,  0.3797,  1.3125, -2.6826, -0.5954,  0.6708,  0.0797,  1.5098,\n",
            "         1.2848, -0.6755, -0.7117, -0.3889, -0.2627, -0.9884,  0.3201,  0.1074,\n",
            "         0.1389,  0.1253,  0.4717, -0.6551,  0.3396,  1.3320,  0.5268,  0.6493,\n",
            "         0.2174, -0.0121,  0.1429, -0.5387, -0.6594, -0.6212, -0.5664,  0.0913,\n",
            "         0.1691,  0.5003,  0.9520,  1.0767,  0.4055, -0.2548, -0.3440, -0.1879,\n",
            "         1.2999, -0.0561, -0.1751, -0.1830, -0.0356, -0.5052,  0.2117, -1.4751,\n",
            "        -0.3329,  0.2406,  0.5771,  0.7869, -0.4038, -0.2631,  0.2486, -0.6342,\n",
            "         0.1963,  0.8021,  2.4137,  0.1005,  0.3331,  1.4776,  1.4947, -0.4169,\n",
            "        -0.8938, -0.1697,  0.1107, -0.8417, -1.0067,  0.3048, -0.1620, -0.8271,\n",
            "         0.8591, -0.1716, -0.1916,  0.1711, -0.3553, -0.3714, -0.8038,  0.1324,\n",
            "        -0.4026,  0.6719, -1.0478, -0.0346,  0.7703, -0.2776, -0.6843,  0.9688,\n",
            "        -0.7610, -0.3857,  0.1144,  0.7655, -0.0320, -0.4441, -0.2793, -0.5685,\n",
            "         0.9969,  0.4903, -0.5092,  0.4455, -0.9749,  0.1632,  0.3248,  0.2317,\n",
            "         0.2686, -0.4957, -0.2226, -0.2286,  0.4055,  0.5058,  0.3697, -0.3088,\n",
            "         0.4327, -0.3725,  1.8091,  0.1535,  0.1786, -1.4899,  0.0432,  0.1180,\n",
            "        -0.4121, -0.2468, -0.4029,  0.2775, -0.2617,  0.4613, -0.3635, -0.6082,\n",
            "        -0.5609,  0.1073,  1.6737, -0.5368, -0.2475, -0.1626,  0.8195,  0.8098,\n",
            "        -1.2670,  0.7340,  0.3604, -0.4145, -0.4364,  0.0158, -0.2773, -0.0243,\n",
            "         0.0705,  0.3949,  0.4805, -1.3138,  0.0536,  0.2366,  0.0934,  0.1662,\n",
            "        -0.3949,  0.5650,  0.5348,  0.4189, -0.1093, -0.7160, -0.0346,  0.6337,\n",
            "         0.2061, -0.1209, -0.5427,  0.1593, -0.3366, -0.9191,  0.5450,  0.0537,\n",
            "         1.1360, -0.1234, -0.6208,  0.0164,  0.1330, -0.9702, -1.4759, -0.7624,\n",
            "         0.5827,  1.2703,  0.3740,  0.6098,  1.5031,  0.8764,  0.7099, -0.1409,\n",
            "         0.1880,  1.0175, -0.2889,  0.8851,  0.0634, -1.6029, -0.9239,  0.3594,\n",
            "        -0.1808,  0.8937, -0.4750,  0.3964, -0.7812, -0.0948,  0.9863,  0.3762,\n",
            "         0.1914, -0.0256, -0.4544,  0.7962,  0.7581,  0.1495, -0.7909,  0.4466,\n",
            "        -0.9024, -0.4016, -0.5240,  0.4722,  0.0177, -0.2733,  0.2649, -0.4376,\n",
            "        -0.9009, -1.1610, -0.5915, -0.7951,  0.6909, -0.3572,  0.9356,  0.4327,\n",
            "        -0.2267, -0.0923, -0.3948, -0.3295, -0.4954,  0.5898,  1.2217, -0.5846,\n",
            "        -0.8379, -1.2771,  0.7538, -0.2567, -0.2864,  0.6259, -0.8266, -0.4461,\n",
            "        -0.1817, -0.4744,  0.8192, -0.4386, -0.9673, -0.4567, -0.3717,  0.2310,\n",
            "        -1.0543,  0.3807, -1.5663,  0.5712])\n",
            "\n",
            "tgt:\t tensor([176,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 1872\tLoss:\t2.724589\n",
            "pred:\t tensor([-0.3310, -0.7519, -1.7995,  2.0112, -0.1771,  0.8311,  1.8538, -0.7172,\n",
            "         0.1883, -1.3620, -0.9102,  1.5431,  0.6380,  0.0632, -0.1717, -0.7676,\n",
            "         1.4824, -0.6583,  1.0233,  0.5538,  0.0503,  0.6434,  0.5597,  0.9909,\n",
            "         2.0201, -1.3532,  0.2225, -0.4357,  1.0404,  1.3089, -0.1559,  0.3757,\n",
            "        -1.0708, -0.0381,  1.9974, -1.5075, -1.5098, -2.9434, -0.5867, -0.6119,\n",
            "         0.2388, -0.8862, -0.7549,  0.2751,  2.1432,  0.8738, -1.8633, -0.4041,\n",
            "         0.0302, -0.0602, -0.7015, -0.3257,  0.5555,  0.6469,  0.7854, -1.1092,\n",
            "        -0.4598,  0.1344, -0.6550,  0.1784, -1.3036,  0.5044, -0.7939, -0.2429,\n",
            "         0.4148, -0.2620, -1.0311,  0.1889, -0.2803,  0.8617, -1.6391, -0.4239,\n",
            "        -0.3652,  0.1854,  0.1047, -0.1961,  1.1106,  0.4990, -1.0033, -1.8651,\n",
            "        -1.4457, -0.3000,  0.5768, -0.0818,  0.8775,  0.5529,  0.3927,  0.2423,\n",
            "         2.8776,  1.3605, -1.0106, -0.7842, -0.2015, -0.9039,  0.6669, -0.9723,\n",
            "        -0.4843,  0.3297,  0.3476,  0.1669, -1.1977, -0.1691, -0.3014, -0.2981,\n",
            "        -0.6846, -0.2710, -0.5063, -1.6608, -2.1782,  0.0565, -0.4232,  0.2989,\n",
            "        -0.8761,  1.3846,  0.9763, -0.0320, -1.0976,  0.7270,  0.1224,  0.3064,\n",
            "        -1.0738, -0.7234, -0.0292,  0.5331,  0.8824, -1.2399,  1.1064, -0.3616,\n",
            "        -0.1924,  0.5972, -0.5151, -0.9790, -0.1458,  0.9375,  0.3971, -0.3625,\n",
            "         0.2906, -0.2096,  0.5196,  0.7099,  0.2774, -0.2179,  0.4340, -0.1652,\n",
            "         0.5982, -0.4812,  0.4522, -1.3536,  2.8054, -0.4060, -0.1016, -0.3409,\n",
            "        -0.3948,  0.0451,  0.5106, -0.8240, -0.3236, -0.2764, -0.2801,  0.5855,\n",
            "         0.4028, -0.6146, -0.4666, -0.6455,  0.4848,  0.3133, -0.3570,  1.0262,\n",
            "         0.9302,  0.0460,  0.7821, -1.0020,  0.1531,  0.8462,  0.2820,  1.2288,\n",
            "         0.1021, -0.8889,  0.1016, -0.3230,  0.4083, -1.0670,  0.5681, -0.3352,\n",
            "         0.5216,  0.2587,  0.6294, -0.4300,  0.2734,  0.9692,  0.5473,  0.2228,\n",
            "         0.0825,  0.2285,  0.4182, -0.0134,  0.4420, -0.0723, -0.6699,  0.7328,\n",
            "         0.0962,  1.1110,  0.2032,  1.3086,  0.2269,  0.0587, -0.7288, -0.8433,\n",
            "         1.3317,  0.6071, -0.0494, -0.5524,  0.1132, -0.0185,  0.3920, -1.6253,\n",
            "        -0.9183,  0.6588,  0.8938,  0.9079,  0.2867, -0.4123,  0.7288,  0.4378,\n",
            "        -0.1441,  1.7470,  2.6624,  0.2994,  0.5060,  0.8962,  1.5809, -0.8355,\n",
            "        -0.9317, -0.6890, -0.5372, -0.6098, -0.7928,  0.9685,  0.2370, -1.1499,\n",
            "         0.7821,  0.2791, -1.0958, -0.0851,  0.1973, -1.1439, -0.8447, -0.0723,\n",
            "        -0.5562,  1.1970, -0.7469, -0.5277,  0.2539, -0.1751, -0.3312,  0.4226,\n",
            "        -0.6261,  0.2340, -0.4208,  0.0148,  0.6695, -1.4274, -0.3627, -0.4353,\n",
            "         0.8778,  0.2953,  0.1031,  1.6151, -0.6135,  0.1597,  0.4673,  0.3885,\n",
            "         0.6669,  0.1254, -0.3859,  0.0239,  0.7548,  0.6945, -0.2085,  0.1786,\n",
            "         0.4520,  0.1230,  1.7533,  0.6974, -0.0355, -1.5356,  0.2105,  0.7141,\n",
            "        -0.5748,  1.0723,  0.7703,  0.3717,  0.4977,  0.3928,  0.0478, -0.1414,\n",
            "        -0.4351,  0.5458,  1.0836, -0.1311,  0.3033,  0.2521,  1.5765,  1.1451,\n",
            "        -0.5280,  1.1736, -0.2260, -0.1817,  0.2331, -0.0656,  0.4851, -0.2942,\n",
            "        -0.0731,  0.3738,  0.8763, -1.5088, -0.1295,  0.9992,  0.2775, -0.0420,\n",
            "        -1.1517, -0.0319, -0.0830,  0.9370,  0.3011, -0.4944,  0.2456,  0.7570,\n",
            "         0.1655,  0.0199,  0.0604,  0.5432, -0.2098, -0.7070,  0.7342,  0.4540,\n",
            "         0.4826, -0.2153, -1.4173, -0.4685,  0.3957, -1.7298, -0.4949, -0.3333,\n",
            "         0.7420,  0.5864,  0.7245,  0.0661,  0.7095,  0.9048,  0.6695,  0.3234,\n",
            "        -0.3907,  1.0805, -0.6089,  0.9350, -0.1103, -0.7147, -1.1400, -0.0226,\n",
            "         0.0215,  1.4404, -0.5883,  0.4202, -1.1521,  0.3684,  0.6691,  0.0086,\n",
            "         0.0852, -0.3237,  0.1283,  0.5133,  0.0986, -0.3537, -0.6676,  0.2781,\n",
            "        -0.5897, -0.1359,  0.0153,  0.3501,  0.5301, -0.4129,  0.5851, -0.5113,\n",
            "        -0.8415, -1.2449, -0.3802, -0.3938,  0.8688, -0.3304,  0.9991,  0.5700,\n",
            "        -0.4609, -0.1154,  0.1005,  0.6003, -0.1973,  0.4704,  0.9448, -0.4365,\n",
            "        -0.2379, -1.7224,  0.7891, -0.1900, -0.2560, -0.1327, -0.4504, -0.2519,\n",
            "         0.2014,  0.3817,  1.1756,  0.7611, -0.8075, -0.6014, -0.3298,  0.1325,\n",
            "        -0.7888,  0.4510, -1.4170, -0.0856])\n",
            "\n",
            "tgt:\t tensor([177,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 1560 / 1872\tLoss:\t4.262648\n",
            "pred:\t tensor([-0.7547, -0.6694, -1.4142, -0.4148, -2.2438,  4.5245, -0.0233,  2.2938,\n",
            "         0.1455, -0.1479, -0.6923,  3.4509, -0.7887, -0.4696,  1.6867, -0.5256,\n",
            "         0.9825, -0.0845,  0.1466, -0.1268,  0.6123, -0.0769, -1.0906,  0.3595,\n",
            "         0.6084, -0.6441,  0.0465,  0.6763,  1.5879, -0.2936, -0.4010, -0.5545,\n",
            "        -0.0822, -0.2535, -0.6997, -2.8060, -0.9685,  1.4014,  0.2005, -0.9931,\n",
            "         0.8636,  1.4633, -0.7952,  0.5739,  1.7597,  0.6885, -1.7511, -0.7090,\n",
            "        -0.6191, -0.8152,  0.3994, -0.8315,  3.0813, -0.8188,  0.4228, -0.1882,\n",
            "        -0.4286,  1.1828,  0.2911, -0.7377, -0.2561,  0.9678,  0.4202, -0.6807,\n",
            "        -0.6496,  0.0217,  0.4317,  0.8694, -0.6511,  0.7496,  1.4133,  0.9679,\n",
            "        -0.4069,  0.4877, -1.5583, -0.6357, -0.3026, -1.4458, -1.7113, -0.3473,\n",
            "         0.7086,  0.2330,  0.3976, -0.3096, -0.1133,  0.4349,  0.2533, -0.3827,\n",
            "        -0.5494,  0.3742, -1.0567, -0.6870,  0.4255, -0.9727,  0.8604,  0.8196,\n",
            "        -0.2772, -0.7780, -0.6274, -0.4123, -0.3465, -0.0873,  0.2402, -0.1082,\n",
            "         0.4202, -0.6455,  0.7447,  0.5815, -0.5543, -0.5970,  0.2219, -0.9199,\n",
            "         1.5802,  0.7515, -0.7475, -1.6559,  0.8286,  0.1413, -0.3492,  0.7282,\n",
            "        -0.9023, -0.8056, -0.1895,  0.6232, -0.2310,  0.5979, -1.0331,  1.1867,\n",
            "         0.5153, -0.2803, -1.0704, -0.4025, -0.6683, -0.5117, -0.2995, -0.3354,\n",
            "        -1.4696, -1.1030,  0.4498, -1.6624,  0.0787, -0.9517,  0.2417, -0.8154,\n",
            "        -0.7731, -0.7694, -0.5559, -1.5139,  0.7953, -0.1689,  0.0065,  0.7312,\n",
            "         0.0397, -1.2612,  1.0607, -0.6569, -1.3242, -0.7766,  0.4870,  0.2219,\n",
            "         0.9110, -0.3746,  0.5868,  0.3884, -0.1894, -0.8411, -0.0731,  0.0120,\n",
            "        -0.6546, -1.2196,  0.2905,  1.8652, -0.5521, -0.2978, -0.5247, -0.5144,\n",
            "         0.0397, -1.3645, -0.6362, -0.9991, -0.0945,  0.1036,  1.0664,  0.1440,\n",
            "         0.9170,  0.4841, -0.2040, -1.0592,  0.7339,  0.3330, -0.4987, -1.7396,\n",
            "        -0.0969, -1.2937,  1.2068,  0.5952,  0.4335, -0.1256,  0.3069,  0.4058,\n",
            "        -0.9140,  0.2577, -0.1752, -0.7758, -1.4749,  1.2265, -0.3483, -0.5320,\n",
            "         1.9941, -1.5141,  0.4011, -0.4111, -0.2093, -0.4012,  0.5697, -0.2398,\n",
            "         0.5701, -0.1687, -0.1550,  0.2739, -0.6430,  0.1418,  0.8010,  0.2739,\n",
            "        -0.7738, -0.5270,  1.9033,  0.5498, -0.3963,  0.6426,  0.8542,  0.0354,\n",
            "        -0.3541,  0.3158,  0.4854, -0.2336, -1.2817,  0.7399,  0.5460, -0.2504,\n",
            "         0.2735, -0.7408, -0.0053,  0.4972, -0.3581, -0.0766,  0.4869, -1.0859,\n",
            "        -0.4938,  0.3340, -0.8735, -0.1397, -0.2237, -0.6048,  0.5032,  0.5098,\n",
            "        -0.0983,  0.1438, -0.6391,  0.9852, -0.6349, -0.2678, -0.2689,  0.0492,\n",
            "        -0.5587, -0.4776, -0.3042, -0.8219, -0.4703,  0.1995, -0.0709,  0.2254,\n",
            "         0.1913,  0.3356,  0.6245,  0.1878, -0.7872,  0.4367, -0.3449,  0.0651,\n",
            "        -0.0736,  0.8951,  1.0567,  0.1404,  0.2860,  0.6293,  0.1366, -0.0865,\n",
            "        -1.1559,  0.2576, -0.4200,  0.5872,  1.0802, -0.1046,  0.4596, -0.0100,\n",
            "        -0.6004,  0.3155,  0.0366,  1.0539, -0.4001,  1.0000,  0.2764,  0.2700,\n",
            "        -0.0759,  0.7381,  0.5822, -0.6599,  0.8533, -0.1880,  0.1161, -0.4370,\n",
            "        -0.7579,  1.0685, -0.8619, -1.6440, -0.5648,  0.5683,  2.1474, -0.2711,\n",
            "         0.2219,  0.2765,  0.7644,  0.5716, -0.0096,  0.2239,  0.2644,  0.6160,\n",
            "         0.0350,  0.2261, -1.7288,  0.2840, -0.8735,  0.1918, -0.2354, -0.1372,\n",
            "         0.2609, -0.0873, -0.3556, -0.4450, -1.1219, -0.2901, -0.4024, -0.9633,\n",
            "         1.0669, -0.0873, -0.0352,  1.1446, -0.9617,  0.4192,  0.8667, -0.2230,\n",
            "         0.2324, -0.1308,  0.0092, -0.2020,  1.0804, -1.6261, -1.4884, -0.4857,\n",
            "        -0.5307, -0.0832, -0.6248, -0.1843,  0.8466, -0.6761, -0.4548,  0.7265,\n",
            "        -0.8474, -0.3506, -0.8423, -0.5426,  0.2521,  0.0860, -0.1303,  0.6647,\n",
            "         0.2227,  0.0939,  0.3984,  0.8764,  0.3214,  0.6380, -0.5884, -1.0114,\n",
            "         0.2425,  0.7768,  0.5197, -0.2065, -0.6004, -0.0644, -0.1089, -0.3496,\n",
            "         0.2319, -0.1888,  0.7135,  0.2495, -0.3693,  1.0185, -0.2602, -0.0408,\n",
            "        -0.1185, -0.9959,  0.0993, -0.9505, -0.4151,  0.6500,  0.4711, -1.4960,\n",
            "         0.1289, -0.2301,  0.5528,  0.0766,  0.4204,  0.4871, -0.2561,  0.7957,\n",
            "        -0.2645, -1.1981, -0.7382, -0.5657])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1716 / 1872\tLoss:\t4.105523\n",
            "pred:\t tensor([-1.6288e-02, -4.7324e-01, -8.8551e-01, -1.7425e+00, -6.8105e-02,\n",
            "         1.1099e+00, -4.8110e-01,  5.2185e-01,  3.0953e-01, -1.7636e+00,\n",
            "        -2.1887e-01,  1.9280e-01, -4.6333e-01,  3.5436e-01,  1.1582e+00,\n",
            "         5.7094e-01,  9.4680e-01, -1.0715e+00,  1.5581e+00,  3.2266e-01,\n",
            "         5.8530e-01,  6.1103e-01,  7.0937e-01,  1.5398e+00,  2.7165e+00,\n",
            "        -1.8286e-01,  7.3104e-02,  3.5229e-01,  1.6838e+00,  3.4864e-01,\n",
            "         3.1895e-01, -1.2401e+00, -1.1140e+00,  1.0065e+00,  3.9628e-01,\n",
            "        -3.4203e+00, -1.0194e+00, -2.1062e+00, -1.1640e-01, -3.5241e-01,\n",
            "         1.3158e+00,  1.3051e+00, -6.5233e-01,  8.7329e-02,  1.4647e+00,\n",
            "         1.2688e-02, -1.3208e+00, -4.9648e-01,  4.1581e-01, -9.8953e-01,\n",
            "         6.9193e-02, -1.9215e-01,  1.4071e+00,  1.4128e+00, -6.4692e-01,\n",
            "        -3.4776e-01, -8.2753e-01, -3.1903e-01, -7.6376e-01,  1.0442e+00,\n",
            "        -6.1406e-01,  2.2424e+00,  1.7684e-02,  1.4553e-01, -7.2012e-01,\n",
            "         4.8644e-01,  6.9539e-01,  1.0682e+00,  4.5394e-01,  4.5859e-01,\n",
            "        -4.8168e-01,  7.7581e-01, -1.1108e+00,  2.7100e-01, -9.1734e-02,\n",
            "        -2.1296e+00,  4.5204e-01, -1.0547e+00, -1.7867e+00, -1.4530e+00,\n",
            "         4.3171e-01, -1.5268e-01,  6.6918e-01, -3.9342e-01, -4.7795e-01,\n",
            "         1.1819e+00,  3.4434e-01,  4.2210e-01,  4.3898e-01,  4.6221e-01,\n",
            "        -5.5837e-01, -5.2101e-01,  1.3972e+00, -3.7609e-01,  7.3224e-01,\n",
            "        -3.8344e-01,  9.3633e-03, -2.1461e-01,  5.4385e-01, -9.2997e-01,\n",
            "        -5.8417e-01, -2.0091e-01, -5.1043e-01, -3.1865e-01,  4.5599e-01,\n",
            "        -2.3847e-01, -2.3947e-02, -9.9738e-01, -1.2089e+00,  1.6610e-01,\n",
            "         6.3479e-01, -3.0197e-01,  4.2634e-01,  1.8013e+00,  1.4117e-01,\n",
            "        -1.0345e+00,  2.0377e-01, -8.8373e-02, -3.3464e-01,  5.9385e-01,\n",
            "         8.0675e-02, -5.9493e-01, -4.2040e-01, -3.5259e-01, -1.3556e-01,\n",
            "        -1.0358e+00,  2.1450e-01, -6.3541e-01, -1.9536e-04,  8.0991e-01,\n",
            "        -2.8048e-01, -4.8366e-01, -1.2721e-01, -5.2061e-01, -7.3650e-01,\n",
            "        -8.4627e-02, -4.7429e-01, -7.1106e-01,  7.9979e-01,  1.8956e-01,\n",
            "         5.4935e-01, -1.6977e+00,  1.6192e-01,  5.4967e-01, -1.1201e+00,\n",
            "        -3.5091e-01, -3.1130e-01, -7.8941e-01,  1.3893e+00,  7.0888e-01,\n",
            "         2.1641e-01, -2.2919e-01,  1.0902e-01, -7.0193e-01,  1.1019e+00,\n",
            "        -3.3322e-01, -1.0859e+00,  1.5270e-01, -3.9617e-02,  4.5843e-01,\n",
            "         7.3862e-01, -4.2281e-01, -1.2359e-01, -9.7693e-01, -1.4467e-01,\n",
            "        -1.3843e+00, -8.6793e-01, -2.5679e-01,  8.5104e-01, -6.8361e-01,\n",
            "         1.6228e+00,  1.1207e+00, -2.6320e-01,  1.0428e+00,  7.6621e-01,\n",
            "        -8.1172e-02,  5.0701e-01, -1.1539e+00, -4.5988e-01,  4.9313e-01,\n",
            "         2.1689e-01, -3.8965e-01,  7.9034e-01,  5.5668e-01,  1.7241e-02,\n",
            "         2.8202e-01, -1.2452e-01, -1.4796e+00, -1.5964e-01,  8.6289e-01,\n",
            "         3.0101e-02, -1.3799e+00, -3.3585e-01, -1.3030e+00,  1.1024e+00,\n",
            "         6.6978e-01,  2.5632e-01,  1.4734e-01, -1.0525e-01,  9.0223e-01,\n",
            "         1.2952e-01,  5.8520e-01, -1.4163e-01,  9.1191e-01, -5.6907e-01,\n",
            "         7.5994e-01, -2.4735e-01, -1.3702e+00,  1.8980e+00, -6.4148e-01,\n",
            "         2.4888e-01, -3.0201e-01,  8.9683e-01, -1.3238e+00,  2.2997e-01,\n",
            "        -5.0898e-01, -5.3580e-01,  1.3205e-01,  6.9613e-03,  8.7305e-01,\n",
            "         2.6974e-01, -2.4513e-01,  6.6533e-01,  9.7222e-02, -3.5120e-01,\n",
            "         1.0549e+00,  2.5184e+00,  1.3614e-01, -1.6210e-01,  4.2962e-01,\n",
            "         1.8248e+00,  3.2720e-01, -7.3921e-01,  2.6895e-02, -7.9533e-01,\n",
            "        -1.1995e+00, -6.2677e-01,  8.0777e-01,  2.6422e-01, -1.7005e-01,\n",
            "         1.9557e-01, -7.9659e-01, -2.2931e-01,  9.5706e-01,  1.5114e+00,\n",
            "         1.3551e-01,  3.3399e-01,  2.7599e-02, -8.4667e-01,  8.7814e-01,\n",
            "        -1.4921e-01, -4.2596e-01,  2.7515e-01, -8.0923e-01,  8.2298e-02,\n",
            "         4.0608e-01,  6.3412e-01,  4.1545e-01,  1.8639e-01, -5.4121e-01,\n",
            "        -9.1288e-01, -1.4104e+00, -5.4786e-01, -1.9360e-01, -4.8321e-01,\n",
            "         1.1264e-01,  1.2616e+00,  6.2320e-01, -3.5814e-01, -1.3494e-01,\n",
            "        -4.5638e-01,  8.7222e-01,  4.7358e-01,  5.4235e-01,  7.6809e-02,\n",
            "         1.0598e-01,  9.0210e-01, -7.6312e-03, -1.0084e+00, -1.0036e+00,\n",
            "         6.7015e-01,  7.9477e-02,  1.0635e+00,  8.9909e-01,  4.9651e-02,\n",
            "        -1.1451e+00, -2.9337e-03,  5.1867e-01, -3.0007e-01,  3.0680e-01,\n",
            "        -1.8505e-01, -2.0272e-01,  9.4808e-01, -1.2505e+00, -1.2655e-01,\n",
            "         7.8812e-01,  3.5428e-01,  9.6893e-01,  6.3994e-01,  3.0590e-01,\n",
            "         9.1543e-02,  6.9330e-01,  9.7081e-01,  4.8631e-01, -3.5288e-01,\n",
            "         5.2487e-01,  2.8883e-01, -8.0982e-01, -1.2941e-02,  6.5201e-01,\n",
            "         5.8930e-01,  8.8377e-02,  3.1143e-02,  1.5462e+00,  5.6172e-01,\n",
            "        -2.0270e+00, -7.3463e-01,  1.3921e+00,  1.2372e+00, -1.2744e-01,\n",
            "        -8.7636e-01,  1.0569e-01, -8.0333e-01,  1.3315e+00, -6.0975e-01,\n",
            "         1.6256e-01,  2.1852e-01,  4.9592e-02,  7.0642e-01,  6.1165e-01,\n",
            "         8.1917e-02,  5.5005e-02,  9.8491e-02, -3.8697e-01,  1.0004e-01,\n",
            "         7.1634e-01,  2.2044e-01,  3.7443e-01,  1.9183e-01, -1.2429e-01,\n",
            "         2.9796e-01, -7.0661e-01, -2.5371e-02, -7.4700e-01,  1.1537e+00,\n",
            "         5.9277e-01,  7.3369e-01, -3.4120e-02, -9.8223e-01,  3.7292e-01,\n",
            "        -9.1304e-02,  6.5850e-01, -2.3202e-01,  2.5118e-01,  5.2507e-01,\n",
            "         3.2635e-01,  3.2658e-01, -1.9917e+00, -1.1459e-01, -5.3511e-01,\n",
            "        -5.5155e-02,  3.0495e-01, -1.8403e-01, -1.3760e-02, -1.0461e+00,\n",
            "        -1.6494e-01,  1.5734e-01,  1.6470e+00, -4.5842e-01,  6.2913e-01,\n",
            "        -8.0181e-01, -1.8952e-01, -4.6454e-01, -5.8466e-01, -2.3254e-01,\n",
            "         2.1894e-01, -2.5969e-01,  4.6922e-02, -5.8714e-01, -2.4149e-01,\n",
            "         1.0443e-01,  8.5163e-01,  2.8212e-01, -5.9588e-01, -2.5255e-01,\n",
            "         3.6919e-02, -7.0368e-01,  9.2417e-02,  1.0037e+00,  3.8486e-01,\n",
            "         2.1577e-01,  2.5121e-01, -3.1986e-01,  9.3387e-02,  4.8524e-01,\n",
            "        -7.2946e-03,  2.6796e-02,  6.5175e-02, -3.9629e-01,  1.5325e-01,\n",
            "         1.5367e-01, -8.3459e-01, -2.4350e-01, -2.2733e-01,  1.0319e-03,\n",
            "        -2.7900e-02, -6.1478e-01, -1.3296e+00,  1.8242e-01,  2.8476e-01,\n",
            "         1.3851e+00, -6.7500e-01, -7.5564e-02, -3.8283e-01, -1.6736e-01,\n",
            "         9.5100e-01, -1.1335e+00,  2.1612e-01, -1.1866e+00, -2.8893e-01])\n",
            "\n",
            "tgt:\t tensor([266, 221,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64_-j-dWurKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "8c7463ee-e832-44c7-be19-78613354d3f6"
      },
      "source": [
        "plt.plot(np.arange(len(transformer_loss_list)), transformer_loss_list)\n",
        "plt.title('Loss Curve of Transformer')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Transformer')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM3qWbQ2urKX"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 75% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaUsIyiyurKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "818579bc-f7fd-4912-fe3d-b34390ba63ee"
      },
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_transformer(net, train_iter, device):\n",
        "  net.eval()\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  \n",
        "torch.manual_seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_transformer(transformer_net, train_iter, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-faf12e6a9c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mvocab_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_fra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_nmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mevaluate_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-faf12e6a9c9e>\u001b[0m in \u001b[0;36mevaluate_transformer\u001b[0;34m(net, train_iter, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpred_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-05acdd12f022>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, src_array, src_valid_len, tgt_array, tgt_valid_len)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mdenseX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_valid_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenseX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-725d537d8657>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_valid_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_valid_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# END OF YOUR CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-eeddb8d92d50>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0menc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enc_outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#print(\"got to the decoder block\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0matten1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_valid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0maddnorm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0matten2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddnorm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_valid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-0a346df3f0de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, valid_length)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mv_reshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalider_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0matt_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-311b0132d633>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, valid_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-046fc10494b6>\u001b[0m in \u001b[0;36mmasked_softmax\u001b[0;34m(X, valid_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [256, 1] at index 0 does not match the shape of the indexed tensor [512, 1] at index 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKY9nbM8MrGf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}